[MUSIC PLAYING] We now begin unit two. So far, we have discussed neural architectures for different problems. We have discussed techniques to train neural networks effectively. We now enter the more mathematical parts of the course. 

Our goal in this unit will be to understand why things work, seeing the details of how different methods to train neural networks, such as gradient descent, stochastic gradient descent, are implemented. This will improve your understanding of why these methods work, and you will get an appreciation for what happens when they do not work. You will become better at making things work well and training neural networks for different problems of your choice. Our goal in this unit will be to use mathematics to write down the nuts and bolts of what these methods are. 

We will begin this module with an introduction to convex functions. Finding the minimum of a convex function is easy. It is like finding the bottom of a parabola. The curvature of the parabola, how far away you are from the bottom when you begin are the two things that determine how quickly you find the bottom. 

Gradient descent corresponds to moving the weights in the direction of the bottom of the parabola. For a one-dimensional quadratic. if you are to the right of the minimum, then you move to the left. If you are to the left of the minimum, you, similarly, move to the right. At all points, you move in the direction that is opposite to the tangent [? of ?] the function, opposite to the gradient, we call it. This is why we say, gradient descent. You descend down the gradient. We will begin the study of gradient descent on convex functions in this module. 