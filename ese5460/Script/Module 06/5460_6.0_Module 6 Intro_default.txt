[MUSIC PLAYING] If the neural network is too small, then it has too few weights, it has too few neurons, and it cannot identify the correct patterns to classify the input images. In this case, we say that it is biased. It is a bit different from the more complicated neural network that you should really be using. That is what bias means. We want bias to be small. We want our network to be close to the good neural network. 

On the other hand, if the network is too big, then it can latch on to spurious correlations in the data to make its predictions. For example, if most images in the training data set have a cow sitting on grass, then the network might simply use the presence of grass to detect a cow. Next time it sees a dog on grass, it might call it a cow instead. If the test data consists of a giraffe on a beach, it wouldn't be able to classify this image as being a good image of a giraffe. 

This concept is called variance. We want variance to also be small. We will discuss in this lesson a very fundamental result that states that both bias and variance cannot be small. There is a fundamental trade-off that the network has to make. It has to choose between the two. But we'll also discuss how neural networks flout this fundamental trade-off. This will be our first insight into avant garde research, a question that many researchers, including me, are trying to figure out an answer to. 

We will encounter many such incongruencies in deep learning stuff, where classical statistics and classical machine learning theory say something, but neural networks happily flout things that we see in textbooks. 

Neural networks are very large models. Modern ones can consist of billions of weights. To ensure that the network does not overfit to the training data, that it makes accurate predictions on the training data but this accuracy also translates to good predictions on the test data, we use regularization. There are many different kinds of regularization. We will look at the three most prominent ones. In some sense, this is the most important topic in all of deep learning. 

The first one that we look at is called weight decay. It restricts the magnitude of the weights in a neural network. This is a classical technique that prevents the network from learning complicated functions. Think of a polynomial whose higher-order coefficients have very small magnitudes. Such a polynomial effectively has a small degree, and therefore it is a simple function. 