[AUDIO LOGO] Objects in the real world can transform in many complicated ways. For example, the image of a cat looks quite different if the cat is in dark. To address variations in data beyond translations and rotations, we will use data augmentation. We will expand the training set to have many versions of each image using image processing routines-- some photos that are zoomed in, some that are brighter, some that are darker, some that are mirror images of the original images, et cetera. And then we'll do machine learning with this expanded data set. 

We will next look at loss functions. This refers to the ways in which we penalize a network for making mistakes on the training data. In a sense, backpropagation is an algorithm that takes the derivative of the loss function, the objective of your training process with respect to weights of the network. Depending on what problem we are solving-- regression, for instance, or classification would be another example-- we need to use different kinds of loss functions. And we'll look at many different ones for this reason. 