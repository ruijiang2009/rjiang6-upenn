Learning the feature vector Transcript (00:00)
Kernels are easy to come up with. And that is why they are nice, much nicer than feature vectors. So if I gave you two strings like this, and you-- how many feel that they are similar? I mean, I tried to make them similar. So I guess they are somewhat similar. So this is a legitimate string from English. And this is something scrambled. So people might have heard of how if you take the same word and then keep the first and the last letter the same, you can basically guess what the word is. So this is what I tried to do when I created this string. 

So you and I would agree that they are somewhat similar to each other than, let's say, some other bunch of characters like this. Can you give me one example of how you will write a kernel for this? Right. So given this string, you can think of some method that changes this string into this other string. People might have heard of something called as edit distance. Edit distance is the shortest number of characters I need to change string A to string B. This is an algorithm. You can code it up using dynamic programming. And you can write down a Python function that takes in two strings and then calculates their edit distance, how many things should I change? 

It doesn't look like any function that we can write down mathematically. But you can code it up in your Python code. You can check whether the grammar matrix has non-negative eigenvalues. And you can verify whether this is a legitimate comment and sort. It is because it is a metric. OK? So kernels can be very complicated things depending on how you come up with them. They may not look like this nice analytical functions that you write down on paper. 

Going from kernels to deep learning is pretty easy. You simply learn the feature vector field. Until now, we've been talking about how to come up with new feature vectors. In deep learning, we'll be interested in learning the feature vectors automatically. So in one sentence, this is the definition of deep learning. 

Random features model Transcript (02:21)
Before we learn the feature vectors, let us look at a simple case where we come up with the feature vectors in some very simple way. We don't use complicated functions. And this is a very nice paper called "The Random Features Model." It is also the reading assignment for this particular chapter. And here is how it goes. So phi i of x is our feature. 

Let us imagine that instead of-- so I have simply expanded out the w transpose phi of x as a sum. wi is my i-th width. And phi i is the i-th coordinate of my feature, phi of x. I can pick a simple set of features, which is given by some matrix S times my input x times some function, sigma. This is just my choice. Maybe it is a good choice. Maybe it is a bad choice. We don't know yet. 

You could have picked any, which feature. Let us pick this particular one. Some non-function sigma times S times x. We will also imagine that this function sigma-- so S is a matrix. x is a vector. S transpose x is another vector. So we'll also say that this function sigma always hits all the entries of the vector independently. So think of sigma as something like sigma of x equal to x squared if x belongs to real numbers. 

But sigma of some vector v is v1 squared, v2 squared so on to vd squared if v belongs to, let's say, Rd. So sigma is just some function that takes whatever you give as its argument and then applies to all the elements of that argument one by one. Just another way of picking one special kind of feature. We are also going to use a very special matrix S. I will not even create this matrix in a clever way. I'll simply fill in this matrix with random numbers. 

So this is a matrix, let us say. And every entry of this matrix, you sample from Gaussian random variable and call it your feature matrix. So S is a matrix full of random entries, nothing special here. This paper showed that for shift-invariant kernels-- shift-invariant kernels simply means that k of x comma x prime equals k of x minus x prime. So the x and x prime never show up separately inside the formula for the kernel. They always show up as terms of x minus x prime. 

This is a shift-invariant kernel. Because I never see x without x primary. I'll always see x minus x prime kind of terms in my kernel. Can you tell me why it is called shift-invariant? So if I add the same constant to both x and x prime, then the kernel doesn't change. So whatever kernel I compute at this location is the same as the kernel I compute at some other location. That is the literal meaning of shift-invariance. 

For these kinds of kernels, this paper showed that you can pick a random matrix. You can pick the nonlinear function, sigma. Just like we chose the nonlinear function to be a quadratic, they chose the nonlinear function to be cosine of its argument, just another choice. They were using cosines because then you can do some nice Fourier transform of the output to prove some things. But you could have chosen something else. 

For a shift-invariant kernel, these two choices give you essentially a nice feature vector. The feature vector is not too big as compared to x. And it still matches the output of the kernel of these two things. This is what the point of the paper is. The paper says, look, you can spend a lot of grief in coming up with complicated kernels. But you can also choose the simple kernel that you can also represent not just as a kernel, but also as a simple feature vector. And that works pretty nicely for a number of problems. 

It is just one particular choice. So using a random matrix for creating our features is a very cheap trick. It doesn't cost essentially any computation because you did not even have to think about it in your head. And it creates a lot of features. So depending on the size of S, you can create however many dimensional feature vectors that you want, right? Let us look at an example. So let us imagine that this particular entry is xi, and this particular entry is xj. So I took all my images, and I'm creating a matrix of size n cross n, where every entry of this matrix is k of xi comma xj. 

And as I said, k of xi comma xj is equal to sigma of x transpose xi transpose sigma of S transpose xj. This is the feature. I apply the feature and take its inner product with the feature for element xj. And this is the entry of this particular matrix. So if you knew the kernel, you could compute this matrix. What this picture is trying to show is that this kernel can be decomposed as a bunch of simple-looking kernels, where all these elements are similar to each other, all these elements are similar to each other. 

According to this particular kernel, all these elements are similar to each other. All these images are similar to each other. It can be written down as a linear combination of a bunch of simple kernels. Is that easy to appreciate? You don't know what the kernel is in this case. It is effectively the Fourier transform of this feature vector. But the point that the picture is making is that, look, this particular kernel, I can write it down as a sum of a bunch of kernels that look simple. You could have come up with the simple kernels yourself, but it is much easier to come up with kernel. 

And if you sum them up, you get something that is similar to our original kernel. So this is just a way to motivate the fact that this simple kernel is not so simple. It can be written down as a bunch of other kernels. And in the paper, they also showed that it works well for a few data sets. So the moral of the story for us here is that there may be instances where you don't need to think too hard about the kind of features you use. 

Such kernels-- it will be useful for you to know-- are very powerful for speech. In speech, if you want to recognize the phonemes that I'm using or recognize the words that I'm using, these kernels have always worked well for the last 20 years or so. Before deep networks came up, this is what was running on most phones and most telephones. When NSA uses it to spy on your telephone, this is kind of the stuff that they use. 

These are our features now. Just like before, we would like to use a vector w to write down a classifier. The w transpose phi of x is our classifier. Once you write down your model minimizing the model, you can use your favorite surrogate loss and minimize the model using build intercept or stochastic intercept. So if you think about this, w is what we are fitting. We are finding the w. S is what we have fixed, right? 

Now the difference between deep learning and random features is that we don't fix these. We also optimize S itself. You can think of this as a function of only w if S is fixed. But if S is not fixed, if I want you to find your own S, not a random S, then you can just as easily think of this function as a function of two variables, w and S. Instead of w star, you will also have w star and s star, where you minimize this objective. 

So this is intuitively a neural network, simplest neural network, a two-layer neural network. And we'll get to this in a bit. But for now, understand that there is no big difference between a random features model and a neural network. The neural network simply learns one extra object on top of the random features model. The random features model chooses a special kind of feature vector. The neural network also learns that particular feature vector. 

Learning the feature matrix Transcript (11:50)
It may look like we did a very basic thing, but we have completely changed the problem. So in this problem, this is a linear function of w. Is that easy to see for everyone? If you use the hinge loss, you get minus y times w transverse x. Instead of this entire business, now you get minus y times w transpose sigma of S prime x. This entire business is still a linear function of w. So you can take the derivative pretty trivial. S is fixed. If S is not fixed, then this is not a linear function of w and S. Is that obvious to everyone? 

w and S multiply each other, right? So the derivative of w depends on S. The derivative with respect to the S depends on w. So this is a nonlinear function. If you have S fixed, it is a linear function. So just because we chose to learn S, we moved from the world of a linear function to a nonlinear function. And these are much, much more complicated objects than linear functions. So we have made our life harder. We've introduced some nonlinearity that comes from sigma. So it is clearly not a linear function even due to sigma. But it is also now nonlinear function because it is some function of both w and sigma multiplying with each other. So it's a nonlinear function, not a linear function. 

If you had p weights, if w was a p dimensional vector before if you had p dimensional features, then S is a matrix of size d cross p. Because we always want to do S transpose x. And x is a vector of d dimensions. So S is a matrix of d times p entries. So the total number of parameters that we are supposed to fit now is what? Is p for w, and d times p for S. Depending on the value of d, this can be much more than p. So we made our model more complicated by choosing to learn S. 

As we said, if you have many parameters to fit for your function, you need more data to fit the function because of the cursor dimensionality. So now because you've chosen to learn NS, you should procure more data to fit your model. So this is a high dimensional problem or at least a larger dimensional problem. If you have an S that is very large, then it is a high dimensional problem. It's a nonlinear problem. What is not so obvious to you right now is that it is also a non-convex problem. 

Convex functions look basically like parabolas. Parabolas are nice because there is only one bottom. You cannot fall anywhere else, but the bottom when you do gradient descend on a parabola. Non-convex functions look like this, for example. If you do gradient descent from this location, if you initialize your weights here, and you do gradient descent from this location, you will get stuck here at some largest value of the training error. If you start here, then you will descend to this place. So depending on where you initialize, you may get different answers. This is what non-convex functions look like. They are more complicated than convex functions. 

Convex functions, it is only a matter of how fast you get to the bottom. There is no mystery about whether or not you get to the bottom. For a non-convex function it also matters of where you come from. So such problems are much more complicated than minimizing a parabola. And so non-convex optimization is a much more complicated beast than convex optimization, which is what we are doing when we fit the perceptron. We have made the problem much more complicated by choosing to learn S also. And as he said, I can think of this as a two-layer neural network. Layer is simply a name that is given to these objects. There is nothing meaningful about what a layer is. S is the first set of parameters. w is the second set of parameters. 

Deep fully connected networks Transcript (16:31)
In this chapter, I will choose to write the last letter as not w, but v. And you will see why. Just a renaming of the phrase. Instead of having two parameters like this, you can have many parameters like this. You can create complicated features that are functions of other features. So imagine that S1x with sigma applied to it is one kind of feature. When you apply another matrix, sigma 2 transposed to it and a nonlinearity on the outside, you get a more complicated feature. So you create a simple feature using S transpose x, you make it more complicated by multiplying it again by S2 transpose x. And you can keep doing this many, many times. 

In this case, I have written it down for L different times. Whatever you come up with is your feature vector phi of x. As soon as you have a feature vector, you use a vector v to classify, to create a linear model out of that feature vector. So we've done nothing other than taking this two-layer neural network and then writing down an L layer neural network, where instead of one S, we have many S's. This is a deep neural network. So nothing very deep about it other than just stacking up multiple layers using the same business. 

So every time you see this complicated expression, there is nothing to worry about it. You simply replace everything that is written here by phi of x, and then imagine it as our same perceptron. But the important thing to realize is that a neural network, or a deep network, creates new features by composing older features. The old features were S1x of the first layer. The second layer takes these features and then makes them more complicated. And this keeps on happening for a few times. At each instance, you take the old features, multiply them by some matrix S. Multiply them and apply some nonlinear function sigma to it to make things more complicated. And then pass it off to the next layer. 

So this entire object, sigma applied to S times whatever the argument is, is what people call a layer. They call it a layer because in your brain, there is this kind of operations that happen. And people have noticed that there is a hierarchy of such operations. And so each such higher level of the hierarchy is called a layer. For us, it is just a mathematical expression. Cool. So this is a neural network. And if the problem in the previous chapter was complicated because we had to wait, the problem in this chapter is even more complicated because there are really many, many, many ways, right? Every S matrix gives you lots of parameters. And they all add up in your network to create your weight vector. 

What features do neural networks learn? Transcript (19:51)
Here is how features look. So we said that phi is something like S transpose x. So I can take a trained neural network, and we will see how to do this later. But I can take a trained neural network and check what kind of features are being learned after the first layer. And turns out you see features that look like this. So these are edges, where there is some bright stuff in the middle. And then it is surrounded by two dark lines. This is a feature that is relevant to green color. So this feature will always have a large magnitude. Every time there is a region in your image that is green, this is the gradient of green and pink, et cetera. So this is how the features look of a trained neural network after the first one or two layers. 

Now, this is interesting because the way visual processing works in your eye is that there is a retina. Retina receives the light. It processes this light in some way. And then it passes it off to some kinds of neurons. So if you imagine, this is your eye. It passes it on to something called as a v1 cortex. It is the first hierarchy of the visual processing pipeline in your brain. Turns out the features that people have measured, you can put some electrodes on the neurons of v1 and see when the electrode registers a high voltage, depending on what kind of input you show to the eye, right? 

So if you show green input, and this particular neuron registers a high voltage, then that this neuron responds to green. So now people have developed many sophisticated ways of measuring such neurons. And it is known since a long time ago, actually 50 years or so ago, that the features of v1, they look very similar to this. They respond they respond to edges. They respond to colors. They respond to simple functions of simple gradients. You know these features from your homework. They are Gabor-like features. Gabor-like features look exactly like these edges. 

So these are the features of the first few layers, let's say the second layer of the neural network. As you go higher up in the layers, these features become more complicated. So you will see that these features are functions of these more fundamental features, the edges and the colors and everything. So if you see an edge that is horizontal as one feature, if you see an edge that is vertical in another feature, then a neuron that combines these features can learn to recognize crosses, right? This is how these features are combined. 

They're combined in linearly with the matrix S with some nonlinearity applied to it to make it more complicated. And this is how those features look. People have spent a lot of time trying to look at these features and then guess what they do, et cetera. You can also do it. So for instance, this feature probably corresponds to what? An eye or a football, one of them anyway. Right? So it's very difficult to guess what these features really are. But for sure, you will agree with me that these are more complicated than these features, right? 

As you go higher up in the network, you get even more complicated features. So these are the eyes, for instance. This particular picture has a lot of eyes, or this might be a golf ball or something because the data set that this was created from, it is called ImageNet. And ImageNet has lots and lots of dogs and lots and lots of cats. So people like their pets, so they click a nice photograph of the eyes and the face of a pet. So this is how the kind of features that will show up in the neural network. 

So all this is to convince you that the features really do get more and more complicated as you go higher up in the layers. Inputs are natural images. And these are features of these natural images. I wanted to tell you a little bit about the features of the eye. So it turns out a neural network learns when you train it. But your visual processing pipeline, it consists of your eye over here and then a bunch of stuff that-- let us call it the brain. The brain learns after you are born, but the eye doesn't really learn. 

The eye has been created across many millennia of evolution. And basically, you have perfect processing of the neurons in your eye as soon as you're born. Nothing is changed after that. There is some small things that change. For instance, when you go from a dark room to bright sunlight outside, there is some molecule called rhodopsin that decreases how many molecules it is secreting. So there is some small kinds of adaptation that happens when you use your eyes, but the neurons fundamentally don't do anything different. 

Mammalian retina Transcript (25:27)
So there was a neuroscientist called Santiago Ram√≥n y Cajal in Spain. In the early 1900s, he basically looked at lots and lots of small neuronal circuits under a microscope. And he drew these very beautiful drawings of everything that he saw under the microscope. And this is the drawing of a mammalian retina. And it is very interesting to see how it works. So this is the backside of the retina, the stuff that is the farthest away from your cornea. Light enters into your eye from your cornea. And it turns out light enters a little bit like this. God knows why. Actually, maybe God actually knows why. 

So it enters from this side. And these kinds of cells, they are called rod cells. Rod cells are more sensitive in dark regions, when there is not too much light in your environment. These kinds of cells are called cone cells. Cone cells are active. They process information when there is lots of light. So you have two different kinds of sensitivities-- sensitivity in dark regions, sensitivity in light environments. And if you see, there is a lot more rod cells than cone cells. This is because your body has evolved or your retina has evolved to respond more to dark environments because that is really where things are harder than by daylight. 

So this is the first step of processing of light. Photons come in. Photons interact with the chemicals or molecules in here. Chemical reactions happen, and electrical impulses are given down. These things are called bipolar cells. So bipolar cells take a bunch of electrical impulses that come down from every one of these rods and cone cells. And they basically average them out. So the output of these bipolar cells are usually spikes like this. So if lots of photons come in in between this period, then there is a spike. 

If few photons come in for a long period, then there is no spike. So the output of these bipolar cells are these spikes even if photons essentially are incident continuously on your eye. So this is the first step, where you convert a continuous signal into a discrete signal. And it is very beautiful to study how and when it occurs. So there's lots of people studying these kinds of inputs calculated by bipolar cells. These cells are very cool stuff. So you don't see this in typical neural architectures. It is a very interesting course project if you want to build one like this. 

These are called a marked instance. They are in charge of killing the signal. So your retina is like a large camera, right? If you see something coming at you from the center of the camera, then you really don't care about what is happening in some other part of the input. So you want to kill the signals that come in this part if you see that this stuff is very dominant, if there are lots of spikes coming in here. So these inhibitory cells, they connect across different parts of your retina, different parts of your sensor, essentially, and kill stuff that is redundant. 

If the same move thing comes in, then watching it in one place is good enough. You don't need to track it everywhere in your field of view. So this is a very nice course project, where you can take a neural network and create these kinds of inhibitions and see whether you can convince yourself that this is important or irrelevant. After you go across this kind of processing, you're reducing the amount of information that comes in by a lot. So the tiniest speck of light that you can detect-- so if you're in a perfectly dark room, and you see this small tiny speck of light, it is about a billion photons that are incident on your eye. 

Those 1 billion photons roughly are converted by these bipolar cells into one spike. That is how much redundancy it is killing from your input signal. So just like our input images are very large images, and these are few features after all. Your input image has 1 million pixels and three colors. And these may be some 100 features or 200 features. The same kind of redundancy is being killed by your retina when it processes. And these are what are called ganglion cells. So the output of the ganglion cells looks very much like this. 

These cells take in those spikes, and then they respond to-- some cells respond to like movements that are at 120-degree angles. Some other cells respond to colors. Some of their cells respond to gradients, et cetera, et cetera. All of this is given to you for free at birth. You never have to learn it. And you would be pretty bad at living if you had to learn all this entire thing. So vision, eyes are some of the most complicated things that have been created by evolution. And that is why nature doesn't trust you to learn them on your own. 

Why are neural nets useful? Transcript (30:53)
Cool. So after a little bit of interlude of inspiration, let us come back to neural networks. Fitting a neural network is just calculating all these weights. Nothing changes. You still minimize the hinge loss if you have two classes. Just now, in addition to taking the derivative with respect to-- of one of the weights, you have to take the derivative to every weight and update all of them one by one. 

So a few things before we proceed to backpropagation. Not having to pick features is very powerful. And this is the biggest reason why everyone is doing deep learning right now instead of doing other kinds of machine learning. 

You know this, but you may not have appreciated this. But let's say when I was your age, let's say finishing my undergraduate or beginning my grad school, for a vision person to be able to sit in a talk given by someone who works on text was basically impossible. You have no idea of what is going on in the natural language processing community. A person who works on NLP, if he or she sits down in a talk given by a computer vision person, these are just two completely alien worlds, totally different techniques. Everything is different. The problems are different. 

Today this is not the case because neural networks are such nice machine learning models that basically the same concepts that you use to solve a problem in computer vision, you can translate some of these concepts to solve a different problem in NLP. So they have brought together these fields in a surprisingly nice way because people have-- people don't need to cook up their own features anymore. 10 years ago, in a vision conference or in a vision paper, you would cook up your own features, solve your problem. In an NLP conference, you would cook up different kinds of features. 

And 50 years of feature engineering is essentially replaced now by simply learning those features. It is not clear whether we learn the same features that we used to create before. In fact, I would argue that we don't learn the same features that we used to create before. Otherwise, we would not work any better. So we seem to learn different kinds of features. But it has just completely killed this entire divide between intellectual fields. 

There is also a little bit of rush that is unwarranted in the sense. There are many fields, for instance, medical imaging, where people study images of brain or images of lungs and et cetera to make some predictions or understand them. These fields have developed very sophisticated ways of creating features. When people began to apply deep networks to these fields, they noticed that these networks do not really work well even though presumably you are learning the features. So just like all these other fields, you should be doing better. 

So you will still see remnants of this kind of thinking, where people are applying neural networks to problems where the original features are actually very good. And they see performance that is much worse than the old methods. But because there is so much temptation to apply these things and so much hope that they will work better, people still keep trying. 

But moral of the story is that you don't need to pick features anymore. I can go to a talk given by an NLP professor. I will understand it somewhat. 

Deep networks are universal approximators. And this is something that we have said many times in words. We will again say it only in words, not in mathematics. Any function that you want to fit to the data, if the network is large enough, and the network has enough layers and enough width, you can fit this function. Doesn't mean that it reduces the amount of data you need to fit the function because the larger the network, the more the amount of data you need to fit because of the curse of dimensionality. But you can fit it given enough data. 

So this is what it means to be a universal approximator. Any kind of complicated predictions you can have, you can make them. 

Deep learning jargon Transcript (35:11)
So some jargon, this is really not important. These are just names given to objects by people to reduce the bit rate of what we talk about, I guess. In the first lecture, we saw the McCulloch and Pitts neuron. It predicts a sign of its inputs, w transpose x. And this is the sine function. A related variant is a threshold operator. So these are all replacements for the nonlinear function that is inside our network. 

You can think of it as a threshold function. It returns a 1 if its argument is greater than 0, or it returns a 0 otherwise. So let us do this same exercise. We will take an x as the argument. And x is a real number because we know that our nonlinearity applies to vectors the same way it applies to real numbers, right? So a threshold function is what? This is 1 if x is positive. And it is 0 otherwise. It is not a linear function. That much you should appreciate. It is a nonlinear function. 

What does that mean a function mean? Can you define? Yeah, so f of x plus by is a times f of x plus b times f of y for all a, b, x, y. This is a linear function. A sigmoid or a logistic looks like this. So if x is very large and positive, what is it? 1. If x is very large and negative, what is it? Minus 1. If x is 0, what is it? Half. So we have a sigmoid that is this. So you see that the sigmoid is kind of a smoother version of a threshold nonlinear function. 

Hyperbolic tangent is a similar function. Again, if x is very large and positive, what is it? 1. If x is very large and negative, what is it? Negative. What is it at 0? So this is tanh of x. A rectified linear unit is a maximum of 0 and x. So how does it look? If it is larger than 0, then you have simply x. If it is less than 0, then you have 0. So value is another linear function. It is very, very close to a nonlinear function. It is very, very close to a linear function because to the right of the origin, it is a linear function. To the left of the origin, it is also a linear function. Just that there is this little kink in the middle. 

Leaky ReLu is another thing that you might see sometimes. Let us invent a color. Leaky ReLu is a function that is like the yellow on the right-hand side and kind of like a ReLu but with a slope that is something slightly positive on the left-hand side of the origin. It is very close to a ReLu. Now, these are all names given to different functions that people have used historically in place of sigma. Some of them work better than others. Some of them work worse than others. There are some legitimate reasons why things work better than others. And we will see soon why. 

The most important thing to remember from this little section is ReLu is a very, very good replacement-- very, very good nonlinearity to use. So unless something very special about your problem that tells you that you should not be using a ReLu, you should use a ReLu. In general, never use Leaky ReLu because Leaky is pretty damn similar to ReLu. Also, you will never see sigmoids being used today. You will sometimes see tanh is being used. And you will sometimes see ReLus being used. And in a bit, we'll see why. 

Another nomenclature, when we thought of the perceptron, we said that we know how to fit a binary perceptron. How do you fit a multiclass perceptron? Either by doing a one versus all, or doing some pairs of classes, et cetera, and fitting many perceptrons. In a neural network, you can do things in a much more elegant way. The final vector v, we have always thought of it as a vector, right? Because we always thought of y hat or a sine function being applied to z happening to return plus 1's and minus 1's. But the v doesn't have to be a vector, though it can be a matrix. 

In that case, it will-- so let us say that v is a matrix of size p cross 5. And so this entire thing is a vector of p dimensions. So v times our feature vector is now a vector of five dimensions. Is that clear? Or v transpose times our feature vector is a vector of five dimensions. Each of these five dimensions, we can interpret as cat or no cat, dog or no dog, elephant or no elephant, giraffe or no giraffe. So we don't have to train multiple perceptrons. We can get output of five different classes in the same model. 

It is conceptually not that different from training multiple perceptron because I mean, these are perceptrons whose feature vectors are shared. And you can just code them up together in a nice way. Is this clear to people? We'll see different ways of interpreting this output, but remember for now that the output doesn't need to be a vector. It doesn't need to be one number. It can be a vector. And the last layer by extension doesn't need to be a vector. It can be a matrix. 

Cool. So another name-- so we have been calling the intermediate outputs as our features, and we'll continue to do so. So we'll just make a distinction between S applied to whatever comes before it as h. And this is called pre-activation. It is called pre-activation because it is before you apply the nonlinear function sigma. Just like we think of the McCulloch and Pitts neuron as taking everything in its input, then applying some function to it and taking its output, this sum function is our nonlinearity. And we will talk about what happens before the nonlinear is applied as the pre-activation, and what comes after the nonlinearity as the post-activation, as the feature itself. 

So sigma of hl is our feature, but hl is-- we have just given it a name. It is called pre-activation. Sometimes in papers, we will also see features being called activations, one and the same thing. The reason to think about these two is that it will be useful to code up backpropagation, to think about them slightly differently. So we said how this entire object is a layer. But you can also think of S transpose x as one layer, and then sigma as another layer. 

The stigma is a layer. It just does not have any parameters. It is an operation that you perform on some input. So S transpose x is one operation that you apply to x. This is like one layer. Sigma is another operation that you apply to S transpose x. So sigma can be thought of as another layer. So it will be useful when you think of backpropagation to think of sigma as one layer and the linear operation as another layer. That is why we separate them out. 

Everything except the last activations are called hidden layers. So sometimes you will see people saying, I have a neural network with one hidden layer. What they really mean is that there is x that enters here. There is S transpose that hits it. That is the sigma that applies to it. And then this is sigma of S transpose x as our output. And here, you get a sigma of h1, right? This is our first feature. Let us say that you apply a v to it, and then you get y as an output. 

Now, in this entire network, this is a tooler network because it has parameters S and v. x is something that you plug in. So imagine this entire network as a big box. You feed in x, and you get to watch y as the output. You never get to watch what is happening inside if it is this big box that you are not allowed to go inside. So that is why the activations or the pre-activations are called the hidden layers. So when people say, I have a neural network with one hidden layer, what they really mean is that they have a neural network with two layers-- one, which is acting on the input, and another one which is giving you the output. Just names that you will see people use left, right, and center in the papers. 

If you don't know them, it feels a little jarring, but these are just names. We will use some names, intuitive ones. We'll say that a network is 10 if the dimensionality of S is not very large-- just a name again. Let us give some numbers. So if x is in a vector in rd, and S is a matrix of p times d, then what is the dimension of h1? It is Rp. If p is very large, then we'll say that the network is very fat. If p is very small, then we say that the network is very thin-- just a way to think about stuff. So fat or wide depending on if I remember to use the right word. 

Weights Transcript (46:48)
A few more names, we'll never talk about the individual parameters of the network simply because it is too cumbersome to keep writing that big ugly formula. We will call everything w. So whenever I say w, you should imagine that you take all the parameters and just concatenate them as a big vector. And this is our network. We will also always say that w is a p dimensional vector. Imagine p is very, very large. And then all these concatenations, it lies in this vector. If you are with me so far, then I can write down my neural network as simply a function of two things. x is the input. w are the weights. 

This is a complicated function in the world, not just some linear function with the sine applied to it. It is a bunch of nonlinear functions, many matrices inside it, et cetera, et cetera. But again, write it down like this. And this will simplify our life a lot. This is our function. Again, this is our objective. You can use your favorite objective. We'll talk about more in a bit. But fitting a neural network amounts to you finding the best w. It amounts to you finding the best v's and s1's and s2's and s3's et cetera that is implicit in this w. 

One thing to remember is that even if we write down our expressions in this very simple form, when you go to PyTorch, PyTorch will maintain the different weight of the network as a big list of matrices. So each of weight will be a matrix or a vector depending on the particular layer. And when you say model dot parameters, you will get this big list as the output. In the second homework, you will learn how to take this list and modify this list. Conceptually, there is nothing different happening. It is just how PyTorch chooses to maintain its weights because we don't care about how the code looks like here when writing the expressions. We think of them as simply vectors. 