{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uk9e20eYBts"
      },
      "source": [
        "# Homework 6: Neural Language Models\n",
        "\n",
        "## Total Points: 48+5\n",
        "- **Overview**: In this assginment, we’ll be moving on from traditional n-gram based language models to more advanced forms of language modeling using *neural networks*. Additionally, we will learn how to build a character-level recurrent neural network, known as a char-RNN for short, with a famous Python package - PyTorch.\n",
        "\n",
        "  This assignment is divided into 2 sections:\n",
        "  1. Classify simple and hard words\n",
        "  2. Classify city names to country\n",
        "  \n",
        "  Following our instructions,  you will implement your own char-RNN, and learn how it can deal with classification problems.  \n",
        "  \n",
        "\n",
        "- **FAQs:** Please look at the FAQ section before you start working:\n",
        "  - **How do I save a PyTorch model?**\n",
        "\n",
        "    After you trained your PyTorch model, the command below will save your model to the local directory. Please ensure that your model can be used for classification.\n",
        "```\n",
        "torch.save(model.state_dict(), PATH)\n",
        "```\n",
        "\n",
        "  - **How do I load a PyTorch model?**\n",
        "\n",
        "    Use the command below.\n",
        "```\n",
        "model = RNN()\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval() #To predict\n",
        "```\n",
        "\n",
        "  - **How do I get started with PyTorch?**  \n",
        "\n",
        "    This assignment will give you a basic insight to PyTorch. For more details, please have a look at [this tutorial](https://hackernoon.com/linear-regression-in-x-minutes-using-pytorch-8eec49f6a0e2).  \n",
        "\n",
        "  - **How do I speed up training?**\n",
        "\n",
        "    Although you have access to GPU with free account, you will have a better GPU if you use Google Colab Pro. To use GPU, send the model, input, and output tensors to the GPU using `.to(device)`. Refer [the PyTorch docs](https://pytorch.org/docs/stable/notes/cuda.html) for further information.\n",
        "\n",
        "  - **Why are some of the words mislabeled in the training and development datasets?**\n",
        "\n",
        "    Noisy data is common when data is harvested automatically like [the cities dataset](https://www.maxmind.com/en/geoip-demo). The onus is on the data scientist to ensure that their data is clean. However, for this assignment, you are not required to clean the dataset.\n",
        "  - **What if I disconnected from the session due to inactivity or maximum usage, and can't connect back again?**\n",
        "    \n",
        "    If you subscribe to Google Colab Pro you will have longer session time and better access to GPU.\n",
        "\n",
        "- **Grading**: We will use the auto-grading system called `PennGrader`. To complete the homework assignment, you should implement anything marked with `#TODO` and run the cell with `#PennGrader` note. **There will be no hidden tests in this assignment.** In other words, you will know your score once you finish all the `#TODO` and run all the `#PennGrader` tests!\n",
        "\n",
        "\n",
        "## Recommended Readings\n",
        "- [Neural Nets and Neural Language Models](https://web.stanford.edu/~jurafsky/slp3/8.pdf). Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd edition draft) .\n",
        "- [PyTorch Tutorial](https://hackernoon.com/linear-regression-in-x-minutes-using-pytorch-8eec49f6a0e2) by Sanyam Bhutani.\n",
        "- [Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning](https://www.amazon.com/Natural-Language-Processing-PyTorch-Applications/dp/1491978236) by Delip Rao, Brian McMahan\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Andrej Karpathy. Blog post. 2015.\n",
        "- [A Neural Probabilistic Language Model (longer JMLR version)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). Yoshua Bengio, Réjean Ducharme, Pascal Vincent and Christian Jauvin. Journal of Machine Learning Research 2003.\n",
        "- Andrej Karpathy, previously a researcher at OpenAI, has written an excellent blog post about using RNNs for language models, which you should read before beginning this assignment. The title of his blog post is [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
        "\n",
        "  Karpathy shows how char-rnns can be used to generate texts for several fun domains:\n",
        "\n",
        "   - Shakespeare plays\n",
        "   - Essays about economics\n",
        "   - LaTeX documents\n",
        "   - Linux source code\n",
        "   - Baby names\n",
        "   \n",
        "## To get started, **make a copy** of this colab notebook into your google drive!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDv9qN5c9357"
      },
      "source": [
        "## Setup 1: PennGrader Setup [4 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpSjR2N19kk4"
      },
      "outputs": [],
      "source": [
        "## DO NOT CHANGE ANYTHING, JUST RUN\n",
        "%%capture\n",
        "!pip install penngrader-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM_qbOLIXmZX",
        "outputId": "4fc09b15-b8f8-455e-9b19-1f103b645856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvPA8Z2D9ki_",
        "outputId": "778783a2-c987-4fc3-ca17-3a7ca2c4fdac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing notebook-config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbeXT4Oj9kg_",
        "outputId": "52c76bce-7d74-4e8b-e123-e5704b4109c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
            "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'\n"
          ]
        }
      ],
      "source": [
        "!cat notebook-config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OceP0Hr-9kfC",
        "outputId": "3954778b-c30b-4838-a54e-f92d3aa485da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PennGrader initialized with Student ID: 62502470\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ]
        }
      ],
      "source": [
        "from penngrader.grader import *\n",
        "\n",
        "## TODO - Start\n",
        "STUDENT_ID = 62502470 # YOUR PENN-ID GOES HERE AS AN INTEGER#\n",
        "## TODO - End\n",
        "\n",
        "SECRET = STUDENT_ID\n",
        "grader = PennGrader('notebook-config.yaml', 'CIS5300_OL_23Su_HW6', STUDENT_ID, SECRET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5lAgKaYa-uN",
        "outputId": "3f31337f-9f47-4b12-9efe-df45e9dce5fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# check if the PennGrader is set up correctly\n",
        "# do not chance this cell, see if you get 4/4!\n",
        "name_str = 'Rui Jiang'\n",
        "grader.grade(test_case_id = 'name_test', answer = name_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOSjvZ8CcEtj"
      },
      "source": [
        "## Setup 2: Dataset / Packages\n",
        "- **Run the following cells without changing anything!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XZ7ps9kfZxI"
      },
      "outputs": [],
      "source": [
        "#Import Packages\n",
        "from os.path import exists\n",
        "from sklearn.metrics import accuracy_score\n",
        "import codecs\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA, is a parallel computing platform and programming model developed by NVIDIA.\n",
        "# It allows developers to use NVIDIA GPUs for general purpose processing.\n",
        "# CUDA will help us to run PyTorch model on GPU.\n",
        "# Verify CUDA acceleration, should print cuda:0\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "assert str(device) == \"cuda:0\""
      ],
      "metadata": {
        "id": "FVp_42OzsFwr",
        "outputId": "06bbbb93-c0a2-461f-8be7-c6c63b0a1dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3548169993.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4S-GJitVcGpP",
        "outputId": "ca02dee0-8da3-4a3c-910c-81e2487a36e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16KoidumvFEoI9hliqgrPiWhvhFHkqMEJ\n",
            "To: /content/complex_words_test_unlabeled.txt\n",
            "100% 181k/181k [00:00<00:00, 4.94MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17xaJNRt3DY2zhEgBE2zj0JEuMqf8CCdk\n",
            "To: /content/complex_words_training.txt\n",
            "100% 798k/798k [00:00<00:00, 9.03MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JX-G-olW84eckkGW-1OC-5XeVJ-Yx3RK\n",
            "To: /content/complex_words_development.txt\n",
            "100% 198k/198k [00:00<00:00, 3.66MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ppyM-7kFyabNG8zOudsTuhWl-2j-zy5Z\n",
            "To: /content/complex_words_test_mini.txt\n",
            "100% 396/396 [00:00<00:00, 2.86MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kKfh2oaxinBPVwWaux6FA9TerqAH5nHw\n",
            "To: /content/shakespeare_input.txt\n",
            "100% 4.57M/4.57M [00:00<00:00, 29.7MB/s]\n",
            "--2025-06-25 01:22:23--  http://computational-linguistics-class.org/homework/nn-lms/cities_test.txt\n",
            "Resolving computational-linguistics-class.org (computational-linguistics-class.org)... 185.199.110.153\n",
            "Connecting to computational-linguistics-class.org (computational-linguistics-class.org)|185.199.110.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10973 (11K) [text/plain]\n",
            "Saving to: ‘cities_test.txt.1’\n",
            "\n",
            "cities_test.txt.1   100%[===================>]  10.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-25 01:22:23 (200 MB/s) - ‘cities_test.txt.1’ saved [10973/10973]\n",
            "\n",
            "--2025-06-25 01:22:23--  http://computational-linguistics-class.org/homework/nn-lms/cities_val.zip\n",
            "Resolving computational-linguistics-class.org (computational-linguistics-class.org)... 185.199.110.153\n",
            "Connecting to computational-linguistics-class.org (computational-linguistics-class.org)|185.199.110.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7558 (7.4K) [application/x-zip-compressed]\n",
            "Saving to: ‘cities_val.zip.1’\n",
            "\n",
            "cities_val.zip.1    100%[===================>]   7.38K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-25 01:22:23 (1.05 GB/s) - ‘cities_val.zip.1’ saved [7558/7558]\n",
            "\n",
            "--2025-06-25 01:22:23--  http://computational-linguistics-class.org/homework/nn-lms/cities_train.zip\n",
            "Resolving computational-linguistics-class.org (computational-linguistics-class.org)... 185.199.110.153\n",
            "Connecting to computational-linguistics-class.org (computational-linguistics-class.org)|185.199.110.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 159676 (156K) [application/x-zip-compressed]\n",
            "Saving to: ‘cities_train.zip.1’\n",
            "\n",
            "cities_train.zip.1  100%[===================>] 155.93K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-06-25 01:22:23 (7.35 MB/s) - ‘cities_train.zip.1’ saved [159676/159676]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Archive:  cities_val.zip\n",
            "replace val/af.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: val/af.txt              \n",
            "replace val/cn.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: val/cn.txt              \n",
            "replace val/de.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: val/de.txt              \n",
            "replace val/fi.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: val/fi.txt              \n",
            "replace val/fr.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: val/fr.txt              \n",
            "replace val/ir.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: val/ir.txt              \n",
            "replace val/za.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: val/za.txt              \n",
            "replace val/pk.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: val/pk.txt              \n",
            "replace val/in.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: val/in.txt              \n",
            "Archive:  cities_train.zip\n",
            "replace train/af.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train/af.txt            \n",
            "replace train/de.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train/de.txt            \n",
            "replace train/fi.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train/fi.txt            \n",
            "replace train/fr.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train/fr.txt            \n",
            "replace train/in.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train/in.txt            \n",
            "replace train/ir.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train/ir.txt            \n",
            "replace train/cn.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train/cn.txt            \n",
            "replace train/za.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train/za.txt            \n",
            "replace train/pk.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train/pk.txt            \n"
          ]
        }
      ],
      "source": [
        "#Download and unzip files\n",
        "!gdown 16KoidumvFEoI9hliqgrPiWhvhFHkqMEJ # https://drive.google.com/file/d/16KoidumvFEoI9hliqgrPiWhvhFHkqMEJ/view?usp=sharing\n",
        "!gdown 17xaJNRt3DY2zhEgBE2zj0JEuMqf8CCdk # https://drive.google.com/file/d/17xaJNRt3DY2zhEgBE2zj0JEuMqf8CCdk/view?usp=sharing\n",
        "!gdown 1JX-G-olW84eckkGW-1OC-5XeVJ-Yx3RK # https://drive.google.com/file/d/1JX-G-olW84eckkGW-1OC-5XeVJ-Yx3RK/view?usp=sharing\n",
        "!gdown 1ppyM-7kFyabNG8zOudsTuhWl-2j-zy5Z # https://drive.google.com/file/d/1ppyM-7kFyabNG8zOudsTuhWl-2j-zy5Z/view?usp=sharing\n",
        "!gdown 1kKfh2oaxinBPVwWaux6FA9TerqAH5nHw # https://drive.google.com/file/d/1kKfh2oaxinBPVwWaux6FA9TerqAH5nHw/view?usp=sharing\n",
        "\n",
        "!wget http://computational-linguistics-class.org/homework/nn-lms/cities_test.txt\n",
        "!wget http://computational-linguistics-class.org/homework/nn-lms/cities_val.zip\n",
        "!wget http://computational-linguistics-class.org/homework/nn-lms/cities_train.zip\n",
        "!sudo apt-get install unzip\n",
        "!unzip cities_val.zip\n",
        "!unzip cities_train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4HWPnBLrZc8"
      },
      "outputs": [],
      "source": [
        "########## DO NOT CHANGE ##########\n",
        "## Loads in the words and labels of one of the datasets\n",
        "def load_labeled_file(data_file):\n",
        "    words = []\n",
        "    labels = []\n",
        "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
        "        i = 0\n",
        "        for line in f:\n",
        "            if i > 0:\n",
        "                line_split = line[:-1].split(\"\\t\")\n",
        "                words.append(line_split[0].lower())\n",
        "                labels.append(int(line_split[1]))\n",
        "            i += 1\n",
        "    X = np.array(words)\n",
        "    y = np.array(labels)\n",
        "    return X, y\n",
        "\n",
        "def getWords(baseDir, lang, train = True):\n",
        "    suff = \"train/\" if train else \"val/\"\n",
        "    arr = []\n",
        "    with codecs.open(baseDir+suff+lang+\".txt\", \"r\",encoding='utf-8', errors='ignore') as fp:\n",
        "        for line in fp:\n",
        "            arr.append(line.rstrip(\"\\n\"))\n",
        "    return np.array(arr)\n",
        "\n",
        "def readData(baseDir, train=True):\n",
        "    X, y = np.array([]), np.array([])\n",
        "    for lang in languages:\n",
        "        tempX = getWords(baseDir, lang, train)\n",
        "        X = np.append(X, tempX)\n",
        "        y = np.append(y, np.array([lang]*tempX.shape[0]))\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNl3MkHqcIQD"
      },
      "source": [
        "##**Introduction to PyTorch**\n",
        "\n",
        "\n",
        "PyTorch is one of the most popular deep learning frameworks in both industry and academia, and learning its use will be invaluable should you choose a career in deep learning. You will be using PyTorch for this assignment, we ask you to build off a couple PyTorch tutorials.\n",
        "\n",
        "PyTorch abstracts the back-propogation process from us, allowing us to define neural network structures and use a generic `.backward()` function to compute the gradients that are later used in gradient descent (PyTorch also implements such optimization algorithms for us).\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=15pO4cDL_F_AKaJhTTAexAny--faAFp5q'>\n",
        "\n",
        "PyTorch does all of this for us by maintaining a computational graph, which allows differentiation to happen automatically! Don’t worry if you don’t remember your chain rules from MATH 114. Another nice thing about PyTorch is that it makes strong use of both object-oriented and functional programming paradigms, which makes reading and writing PyTorch code very accessible to previous programmers.\n",
        "\n",
        "Before you start, make sure you have run the previous cells to set up the environment. Remember to set hardware accelerator to **GPU** under **Change Runtime Type** in the **Runtime** menu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFbSo2hZehCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "5ceb0b21-2333-43e4-9bef-c44c82bd07bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3548169993.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# CUDA, is a parallel computing platform and programming model developed by NVIDIA.\n",
        "# It allows developers to use NVIDIA GPUs for general purpose processing.\n",
        "# CUDA will help us to run PyTorch model on GPU.\n",
        "# Verify CUDA acceleration, should print cuda:0\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "assert str(device) == \"cuda:0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W1LsPuKorQt"
      },
      "source": [
        "# Section 1: Revisiting an old friend: classify simple and hard words [22 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPI7pESlt9_z"
      },
      "source": [
        "Let's start with a simple example we've seen before - text classifier: distinguish between words that are simple and words that are complex. We've used Naive Bayes and Logistic Regression to solve this problem in HW2, and now we are using RNN!   \n",
        "\n",
        "This homework is developed from [a PyTorch Tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ViutxgjGevo"
      },
      "source": [
        "### **Dataset description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuCz4cY-GkLr"
      },
      "source": [
        "Let's take a look at the dataset first (although you might be already familiar with it). The dataset contains english words with a label 0: easy, and 1: hard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmcIGSrXTtj0"
      },
      "outputs": [],
      "source": [
        "word_train_data = load_labeled_file(\"complex_words_training.txt\")\n",
        "word_val_data = load_labeled_file(\"complex_words_development.txt\")\n",
        "\n",
        "X_word, y_word = word_train_data\n",
        "print(X_word[:5])\n",
        "print(y_word[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAaYrh8jIANq"
      },
      "outputs": [],
      "source": [
        "word_train_data[1][66]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUx-SayMJYwz"
      },
      "outputs": [],
      "source": [
        "string.ascii_letters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvT67MXO27On"
      },
      "source": [
        "For convenience, let's define 'all_letters' and 'group'.  \n",
        "**'all_letters'** is a long string that contains all character we use.  \n",
        "**'n_letters'** is the length of 'all_letters', also the input dimention of our PyTorch model.  \n",
        "**'groups'** is a list that has all possible y values (labels) a dataset can have. In this example, there is only two: 0 and 1. We name it as 'complexity'.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0pEcRW43CDc"
      },
      "outputs": [],
      "source": [
        "# We will use \"_\" to represent our out-of-vocabulary character, that is, any character we are not handling in our model\n",
        "all_letters = string.ascii_letters + \" .,'\" + \"_\"\n",
        "n_letters = len(all_letters)\n",
        "complexity = [0,1]\n",
        "\n",
        "print(all_letters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjpgRr3hUOdf"
      },
      "source": [
        "### **Transform to tensor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oefdc4BVUTyU"
      },
      "source": [
        "In the PyTorch word, models only take 'tensor' as inputs, so we need to transform everything from Numpy array to PyTorch tensor format.\n",
        "[little more intro, tensor is matrix]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rjtzai94fD1"
      },
      "source": [
        "Let's start with a very simple function: letterToIndex()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NNz2rXP4m-Y"
      },
      "outputs": [],
      "source": [
        "def letterToIndex(letter):\n",
        "    '''\n",
        "    Find letter index from all_letters, e.g. \"a\" = 0\n",
        "    hint: use .find() function and return our out-of-vocabulary character if we encounter a character not in all_letters.\n",
        "        This should be a one line function!\n",
        "\n",
        "    Inputs:\n",
        "        letter: a character. Ex) 'a', 'r', 'T'\n",
        "\n",
        "    Returns:\n",
        "        index integer. Ex) 0, 17,\n",
        "    '''\n",
        "    # TODO\n",
        "    torch_tensor = torch.from_numpy(np.array([all_letters.find(letter) if letter in all_letters else all_letters.find('_')])).to(device)\n",
        "    return torch_tensor.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b4eoAkAKCjX"
      },
      "outputs": [],
      "source": [
        "letterToIndex('a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTCR4mAc4qcO"
      },
      "outputs": [],
      "source": [
        "assert letterToIndex('a') == 0\n",
        "assert letterToIndex('r') == 17\n",
        "assert letterToIndex('T') == 45\n",
        "assert letterToIndex('!') == letterToIndex('_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40QNzzzKjppK"
      },
      "outputs": [],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "letters = ['b','f','G',' ']\n",
        "letterIndices = [letterToIndex(x) for x in letters]\n",
        "grader.grade(test_case_id = 'testLetterToIndex', answer = letterIndices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pwTxw_j6mlt"
      },
      "source": [
        "Using this letterToIndex() function, let's define letterToTensor() function.  \n",
        "This function should take a character(letter) as input, and transforms it into a tensor.  But what is a tensor?  What does it look like?\n",
        "\n",
        "PyTorch tensor is similar to NumPy arrays, they both are tools to handle multi-dimentional data. The key difference between those two is GPU support - tensor works with GPU, which makes computation a lot faster.\n",
        "\n",
        "In NLP, we can use tensor to represent text:   \n",
        "\n",
        "* To represent a single character, we use a 'one-hot vector' of size <1 x n_letters>. Since we have 57 letters, it will be a <1 x 57> Tensor. One-hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. Please see more introduction at this [wiki](https://en.wikipedia.org/wiki/One-hot).    \n",
        "\n",
        "* To represent a line of text, we join multiple 'one-hot vector' to form  <line_length x 1 x n_letters>. Line length is the length of your input text. For example, line_length of \"I like apple\" is 12.\n",
        "\n",
        "Why there is always a size 1 dimension? That is a batch dimension. In machine learning, a \"batch\" refers to a subset of the dataset. When training a model, instead of using the entire dataset at once, the dataset is divided into several batches. Each batch is then used to update the model's weights and improve its performance. PyTorch assumes there should be a batch dimension in all tensors, and we use 1 in our examples. (which means we don't use batch!)\n",
        "\n",
        "**[IMPORTANT]** Whenever you create a tensor, make sure to send them to GPU using ```.to(device)```!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bf9Vn_N_p4V"
      },
      "source": [
        "Before we start, let's see what an empty tensor looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yayvFlq57eGV"
      },
      "outputs": [],
      "source": [
        "# character tensor\n",
        "torch.zeros(1, n_letters).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eizVCGn1_8-r"
      },
      "outputs": [],
      "source": [
        "# line tensor\n",
        "line_length = 3\n",
        "torch.zeros(line_length, 1, n_letters).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkBYcl4W8Z_z"
      },
      "source": [
        "Now let build letterToTensor() function using letterToIndex().  \n",
        "**[IMPORTANT]** Whenever you create a tensor, make sure to send them to GPU using ```.to(device)```!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28aOEZjV5iQH"
      },
      "outputs": [],
      "source": [
        "def letterToTensor(letter):\n",
        "    '''\n",
        "    Transform a character to tensor\n",
        "    input: a character\n",
        "    output: a one-hot encoded <1 x n_letters> tensor\n",
        "\n",
        "    Hint: please use torch.zeros() and letterToIndex() here!\n",
        "\n",
        "    Inputs:\n",
        "        letter: a character\n",
        "\n",
        "    Returns:\n",
        "        a tensor\n",
        "    '''\n",
        "    # TODO\n",
        "    tensor = torch.zeros(1, n_letters)  # <1 x n_letters>\n",
        "    index = letterToIndex(letter)\n",
        "    tensor[0][index] = 1\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v5rCY9iBKA6"
      },
      "outputs": [],
      "source": [
        "assert np.array_equal(letterToTensor('J').cpu().numpy(),np.array([[0]*35+[1]+[0]*21]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB5DPGB6oQ5D"
      },
      "outputs": [],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "letters = ['b','G']\n",
        "letterTensors = [letterToTensor(x).cpu().numpy() for x in letters]\n",
        "devices = [str(letterToTensor(x).device) for x in letters]\n",
        "grader.grade(test_case_id = 'testLetterToTensor', answer = letterTensors + devices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI5_ObQDBMfQ"
      },
      "source": [
        "Finally, let's build lineToTensor also using `letterToIndex()`.  \n",
        "For the word 'cat' the tensor should look like this:\n",
        "```\n",
        "tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "          0., 0., 0., 0., 0., 0.]],\n",
        "\n",
        "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "          0., 0., 0., 0., 0., 0.]],\n",
        "\n",
        "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "          0., 0., 0., 0., 0., 0.]]], device='cuda:0')\n",
        "```\n",
        "\n",
        "Remember to use `.to(device)`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XxDUt5NUjP_"
      },
      "outputs": [],
      "source": [
        "# Turn a line into a <line_length x 1 x n_letters>\n",
        "# input: a line of text\n",
        "# output:  a <line_length x 1 x n_letters> tensor\n",
        "def lineToTensor(line):\n",
        "    '''\n",
        "    Turn a line into a <line_length x 1 x n_letters>\n",
        "    input: a line of text\n",
        "    output:  a <line_length x 1 x n_letters> tensor\n",
        "\n",
        "    Inputs:\n",
        "        line: string signifying a line of text\n",
        "\n",
        "    Returns:\n",
        "        a tensor\n",
        "    '''\n",
        "    # TODO\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        index = letterToIndex(letter)\n",
        "        if index == -1:\n",
        "            raise ValueError(f\"Character '{letter}' not found in all_letters.\")\n",
        "        tensor[li][0][index] = 1\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V57fD4hwM-Zx"
      },
      "outputs": [],
      "source": [
        "lineToTensor('Jones')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AJUpi3lVXOI"
      },
      "outputs": [],
      "source": [
        "assert list(lineToTensor('Jones').size()) == [5, 1, 57]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5xblrhCNEyg"
      },
      "outputs": [],
      "source": [
        "test_tensor = lineToTensor(\"test_word\").cpu().numpy()\n",
        "test_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUGbc2EiZFeS"
      },
      "outputs": [],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "test_tensor = lineToTensor(\"test_word\").cpu().numpy()\n",
        "grader.grade(test_case_id = 'testLineToTensor', answer = test_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_-Ep5r8u3_9"
      },
      "source": [
        "### **PyTorch RNN model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUoxiMKfCXzD"
      },
      "source": [
        "RNN, or Recurrent Neural Network, is a class of artificial neural networks designed to recognize patterns in sequences of data. Unlike traditional neural networks which process inputs independently, RNNs have loops to allow information persistence.\n",
        "\n",
        "**Key Points**:\n",
        "- **Memory**: RNNs remember past information and use it to influence future output.\n",
        "- **Sequential Data**: They are especially good for sequences like time series, speech, text, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19SyI4iVCtNF"
      },
      "source": [
        "Using PyTorch, you can build, train and implement your own model very easily. A cool thing about PyTorch is you don't need to worry about backpropagation, PyTorch will do it for you! You only need to fill up the forward function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGTbdy_Hu-UG"
      },
      "source": [
        "Since we know that you are new to PyTorch, we are providing a **complete** example RNN PyTorch model for this task. For this model we are not asking you to build anything, but please read the explanation line by line to understand what each line is about. Later in this homework assignment we will ask you to build your own model by yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk5uEe-JvjJf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        print(\"__init__\")\n",
        "        super(RNN, self).__init__()  # Calling the parent class (nn.Module) initializer\n",
        "\n",
        "        self.hidden_size = hidden_size  # Define the size of the hidden state\n",
        "\n",
        "        # Linear layer taking concatenated input and hidden state to the next hidden state\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # Linear layer to map hidden state to output\n",
        "        # A hidden layer in a neural network is between the input and output layers and captures patterns in the data by applying weights and activation functions.\n",
        "        self.h2o = nn.Linear(hidden_size, 2)\n",
        "        # LogSoftmax activation for output (useful for classification tasks)\n",
        "        # The softmax function converts a vector of values into a probability distribution, often used in multi-class classification to assign probabilities to different classes.\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Concatenate the input and hidden tensors along dimension 1\n",
        "        # print(\"input is: \", input)\n",
        "        # print(\"hidden is: \", hidden)\n",
        "        # print(\"input is on:\", input.device)\n",
        "        # print(\"hidden is on:\", hidden.device)\n",
        "        if input.device != hidden.device: # added by ruijiang\n",
        "            # print(\"input is on:\", input.device)\n",
        "            # print(\"hidden is on:\", hidden.device)\n",
        "            input = input.to(hidden.device)\n",
        "\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        # Pass the concatenated tensor through the i2h layer to get the next hidden state\n",
        "        hidden = self.i2h(combined)\n",
        "        # Pass the hidden state through the h2o layer to get the raw output\n",
        "        output = self.h2o(hidden)\n",
        "        # Apply softmax to the raw output\n",
        "        output = self.softmax(output)\n",
        "        # Return the final output and the new hidden state\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        # Initializes hidden state with zeros\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-kvxr5Nz9Mg"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szjg_NHzwmzL"
      },
      "source": [
        "Now we have the RNN network. However we still need a few more helper functions to prepare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5vpw3BI0Pp6"
      },
      "source": [
        "Let's start with random_training_pair function. In each epoch, we will using a randomly choosen instance to train the model. This will help us to increase the efficiency of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk5NrRAWwmbF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_training_pair(X, y, seed = None): # seed is required for penngrader only.\n",
        "    '''\n",
        "    Pseudocode:\n",
        "        1. Initialize a random generator with given seed.\n",
        "        2. Generate a random index 'ind' between 0 and (number of rows in X) - 1.\n",
        "        3. Fetch 'category' from y and 'line' from X using the random index 'ind'.\n",
        "        4. Convert 'category' to a tensor and move it to the specified device.\n",
        "        5. Convert 'line' to a tensor by calling the function lineToTensor.\n",
        "        6. Return 'category', 'line', 'category_tensor', and 'line_tensor'.\n",
        "\n",
        "    Input:\n",
        "        training data:\n",
        "            X: features\n",
        "            y: labels\n",
        "            seed: needed for randomness\n",
        "\n",
        "    Returns:\n",
        "        A tuple of 4 items:\n",
        "            category: output label(category) as an integer,\n",
        "            line: input line (here by word) as a string,\n",
        "            category_tensor: the category as a tensor. Ex) category = 1 => category_tensor = tensor([1]),\n",
        "                            Tip: make sure to send your tensor to GPU!\n",
        "            line_tensor: line as a tensor. Tip: use lineToTensor()!\n",
        "    '''\n",
        "    # 1. Initialize a random generator with given seed.\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    # 2. Generate a random index 'ind' between 0 and (number of rows in X) - 1.\n",
        "    ind = random.randint(0, len(X)-1)\n",
        "\n",
        "    # 3. Fetch 'category' from y and 'line' from X using the random index 'ind'.\n",
        "    category = int(y[ind])\n",
        "    line = str(X[ind])\n",
        "\n",
        "    # 4. Convert 'category' to a tensor and move it to the specified device.\n",
        "    category_tensor = torch.tensor([category], device=device)\n",
        "\n",
        "    # 5. Convert 'line' to a tensor by calling the function lineToTensor.\n",
        "    line_tensor = lineToTensor(line).to(device)\n",
        "\n",
        "    # 6. Return 'category', 'line', 'category_tensor', and 'line_tensor'.\n",
        "    return category, line, category_tensor, line_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkU50ymxhkKJ"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator()\n",
        "g.manual_seed(26)\n",
        "ind = torch.randint(0, 4000, (1,), generator=g).item()\n",
        "X_word[ind]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdEHX54pVWxb"
      },
      "source": [
        "An example of the output will look like this:  \n",
        "```\n",
        "RUN:\n",
        "random_training_pair(X_word, y_word, seed = 694)\n",
        "\n",
        "RESULT:\n",
        "(0,\n",
        " 'owl',\n",
        " tensor([0], device='cuda:0'),\n",
        " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
        "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "           0., 0., 0., 0., 0., 0.]],\n",
        "\n",
        "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "           0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "           0., 0., 0., 0., 0., 0.]],\n",
        "\n",
        "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
        "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "           0., 0., 0., 0., 0., 0.]]], device='cuda:0'))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7ywu2aMR6rv"
      },
      "outputs": [],
      "source": [
        "random_training_pair(X_word, y_word, seed = 694)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rix1xnSzJvO"
      },
      "outputs": [],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "randomPair = list(random_training_pair(X_word, y_word, seed = 26))\n",
        "devices_RTP = [str(randomPair[2].device), str(randomPair[3].device)]\n",
        "randomPair[2] = randomPair[2].cpu().numpy()\n",
        "randomPair[3] = randomPair[3].cpu().numpy()\n",
        "grader.grade(test_case_id = 'testRandomTrainingPair', answer = randomPair + devices_RTP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpNGrpacUFMq"
      },
      "outputs": [],
      "source": [
        "randomPair + devices_RTP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pWPXJh2ZZiW"
      },
      "source": [
        "Let's use this `random_training_pair()` function for training. Here we implement `trainOneEpoch()` function, which train the model with one word per one epoch.\n",
        "\n",
        "As we learned from the lecture, the way we train a neural net model is:\n",
        "1. Pick an X,y pair\n",
        "2. Zeroing the gradients and initializing the hidden state for the model\n",
        "3. Forward X through the model to get a prediction(output)\n",
        "4. Compare output with y to get loss\n",
        "5. Use loss to do backpropagation.\n",
        "6. Repeat from 1.\n",
        "\n",
        "We learned how to do Step 1. Step 2 is like the following with PyTorch:\n",
        "```\n",
        "model.zero_grad()\n",
        "hidden = model.initHidden().to(device)\n",
        "```\n",
        "This is how we do Step 3 on RNN:\n",
        "```\n",
        "output, hidden = model(input, hidden)\n",
        "```\n",
        "Step 4 and 5 is relatively easy on PyTorch:\n",
        "```\n",
        "loss = criterion(output, answer_as_tensor)\n",
        "loss.backward()\n",
        "```\n",
        "Criterion (loss function) varies across tasks. In this example we will use `nn.NLLLoss()` (Negative Log Likelihood Loss)  \n",
        "Now, let's implement `trainOneEpoch()` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXqyzQvCzfVj"
      },
      "outputs": [],
      "source": [
        "def trainOneEpoch(model, criterion, optimizer, X, y):\n",
        "    '''\n",
        "    Define a function to train the model for one epoch called trainOneEpoch.\n",
        "\n",
        "    Do the following steps:\n",
        "\n",
        "    1. Reset any accumulated gradients in the model to zero.\n",
        "    2. Initialize a hidden state for the model using its initHidden method.\n",
        "    3. Randomly select a training pair (a category and a line, along with their tensor representations) using the random_training_pair function on X and y.\n",
        "    4. Loop over each tensor (character) in the line_tensor:\n",
        "    a. For each tensor, pass it and the current hidden state into the model to get the predicted output and the next hidden state.\n",
        "    5. Once the entire line_tensor is processed, compute the loss by comparing the model's final output to the true category_tensor using the provided criterion.\n",
        "    6. Propagate the error backward through the model to compute the gradients.\n",
        "    7. Update the model's parameters using the optimizer's step method.\n",
        "    8. Return the model's output, the computed loss as a single value, and the original line and category from the random training pair.\n",
        "\n",
        "    Inputs:\n",
        "        - model: the neural network model we want to train\n",
        "        - criterion: the loss function to calculate the training error\n",
        "        - optimizer: the optimization algorithm to adjust model parameters\n",
        "        - X: the input data\n",
        "        - y: the corresponding labels\n",
        "\n",
        "    Returns:\n",
        "        - output: the model's final output (prediction)\n",
        "        - output_loss: the computed loss as a single value\n",
        "        - line: the randomly choosen line from random_training_pair()\n",
        "        - category: the randomly choosen category from random_training_pair()\n",
        "    '''\n",
        "    # Zeroing the gradients to clear up the accumulated history\n",
        "    model.zero_grad()\n",
        "    # Initializing the hidden state for the model\n",
        "    hidden = model.initHidden().to(device)\n",
        "    # TODO: implement step 3, 4\n",
        "    category, line, category_tensor, line_tensor = random_training_pair(X, y)\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = model(line_tensor[i], hidden)\n",
        "\n",
        "\n",
        "    # Calculating the loss between the model's output and the actual target (category_tensor)\n",
        "    loss = criterion(output, category_tensor)\n",
        "    # Backward pass: compute the gradient of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "    # Updating the model parameters based on the calculated gradients\n",
        "    optimizer.step()\n",
        "    # Extracting the value of the loss as a Python number\n",
        "    output_loss = loss.data.item()\n",
        "    return output, output_loss, line, category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj3J_Rz8ZAB0"
      },
      "outputs": [],
      "source": [
        "# TEST: DO NOT CHANGE\n",
        "test_model = RNN(input_size=len(all_letters), hidden_size=10).to(device)\n",
        "test_model.train()\n",
        "before = list(test_model.parameters())[-1].clone()\n",
        "output, loss, line, category = trainOneEpoch(test_model, nn.NLLLoss(),\n",
        "              torch.optim.SGD(test_model.parameters(), lr=0.2),\n",
        "              word_train_data[0], word_train_data[1])\n",
        "after = list(test_model.parameters())[-1].clone()\n",
        "\n",
        "assert not np.array_equal(before.detach().cpu().numpy(), after.detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pOyAiJYw_GB"
      },
      "outputs": [],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "beforeAndAfter = [before.detach().cpu().numpy(), after.detach().cpu().numpy()]\n",
        "grader.grade(test_case_id = 'testTrainOneEpoch', answer = beforeAndAfter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAP4puMy0JFp"
      },
      "source": [
        "Next, we need predict() function to use a trained model to make predictions. For efficiency, let's design it as a two-way function:  \n",
        "1. If only a trained model and evaluate dataset are given, predict() only returns the predictions.\n",
        "2. If labels and a loss function are additionally given, predict() returns overall loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scYdAObnbGSH"
      },
      "outputs": [],
      "source": [
        "def predict(model, X, y = None, loss_func = None):\n",
        "    '''\n",
        "    Make predictions on the input data X using the given model.\n",
        "    Optionally calculate the average loss using true labels y and loss function loss_func.\n",
        "\n",
        "    Inputs:\n",
        "        model: trained model\n",
        "        X: a list of words\n",
        "        y: a list of categories (optional)\n",
        "        loss_func: a loss function (optional)\n",
        "    Returns:\n",
        "        predictions: as a NumPy array if y and loss_func are None, else the average loss.\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        # Set the model to evaluation mode\n",
        "        model.eval()\n",
        "        # Initialize lists to store predictions and individual losses\n",
        "        pred = []\n",
        "        val_loss = []\n",
        "        # Loop over each sample in the input data X\n",
        "        for ind in range(X.shape[0]):\n",
        "            # Initialize hidden state\n",
        "            hidden = model.initHidden().to(device)\n",
        "            # Convert the current input sample to a tensor\n",
        "            val = lineToTensor(X[ind])\n",
        "            # Loop over each element in the input tensor and get the model's output\n",
        "            for i in range(val.size()[0]):\n",
        "                output, hidden = model(val[i], hidden)\n",
        "            # Move the output tensor back to CPU and extract data (log probabilities)\n",
        "            log_probabilities = output.cpu().data\n",
        "            # Calculate the prediction by comparing the log probabilities\n",
        "            log_prob0, log_prob1 = log_probabilities[0]\n",
        "            pred.append(int(log_prob0 < log_prob1))\n",
        "            # If true labels and a loss function are provided, calculate the loss for the current sample\n",
        "            if y is not None and loss_func is not None:\n",
        "                category_tensor = torch.tensor([int(y[ind])]).to(device)\n",
        "                val_loss.append(loss_func(output, category_tensor).data.item())\n",
        "\n",
        "    # If true labels and a loss function were provided, return the average loss\n",
        "    if y is not None and loss_func is not None:\n",
        "        return sum(val_loss) / len(val_loss)\n",
        "\n",
        "    # Otherwise, return the predictions as a NumPy array\n",
        "    return np.array(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wsyMJkZ3X4W"
      },
      "source": [
        "For convenience, we need calculateAccuracy() to calculate accuracy given a model, a dataset and the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4Kz1DehbNId"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def calculateAccuracy(model, X, y):\n",
        "    '''\n",
        "    HINT: you can use accuracy_score function.\n",
        "\n",
        "    Pseudocode:\n",
        "    1. Calculate prediction of X using predict()\n",
        "    2. Calculate accuracy using accuracy_score() fuction\n",
        "\n",
        "    Inputs:\n",
        "        model: trained model,\n",
        "        X: a list of words,\n",
        "        y: a list of class labels as integers\n",
        "    Returns:\n",
        "        accuracy score of the given model on the given input X and target y\n",
        "    '''\n",
        "    # TODO\n",
        "    y_pred = predict(model, X, y, loss_func = None)\n",
        "    return accuracy_score(y, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_irDex7eo2_"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzzEqp-v4nu3"
      },
      "source": [
        "Now everything is ready. Let's train our model!\n",
        "\n",
        "Function run() will train and save your classification model. It also records training loss and validation loss for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtOVSO-jZ1ly"
      },
      "outputs": [],
      "source": [
        "def run(train_data, val_data, hidden_size, n_epochs, learning_rate, loss_func, print_every, plot_every, model_name):\n",
        "    X, y = train_data\n",
        "    X_val, y_val = val_data\n",
        "    model = RNN(input_size=len(all_letters), hidden_size=hidden_size)\n",
        "    model = model.to(device)\n",
        "    current_loss = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(0, n_epochs):\n",
        "        output, loss, line, category = trainOneEpoch(model,\n",
        "                    criterion = loss_func,\n",
        "                    optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate),\n",
        "                    X=X,\n",
        "                    y=y)\n",
        "        current_loss += loss\n",
        "\n",
        "        # print intermediate reports\n",
        "        if epoch % print_every == 0:\n",
        "            log_probabilities = output.cpu().data\n",
        "            log_prob0, log_prob1 = log_probabilities[0]\n",
        "            prediction = int(log_prob0 < log_prob1)\n",
        "            correct = 'correct' if prediction == category else 'incorrect (True:%s)' % category\n",
        "            print('Epoch %d (%d%%)  Loss: %.4f, Word: %s, Prediction: %s | %s' % (epoch, epoch / n_epochs * 100, loss, line, prediction, correct))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            # Training Loss\n",
        "            train_losses.append(current_loss/plot_every)\n",
        "            current_loss= 0\n",
        "\n",
        "            # Validation Loss\n",
        "            val_losses.append(predict(model, X_val, y_val, loss_func))\n",
        "\n",
        "    torch.save(model.state_dict(), model_name)\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEFxPlrjQUBs"
      },
      "outputs": [],
      "source": [
        "# Let's do simple vs complex word classification!\n",
        "# 1 is complex, 0 is simple\n",
        "# Don't worry about the hyperparameters, we will take a look at them later.\n",
        "# training will take around 5 minutes\n",
        "word_train_losses, word_val_losses = run(train_data = word_train_data,\n",
        "                              val_data = word_val_data,\n",
        "                              hidden_size = 50,\n",
        "                              n_epochs = 50000,\n",
        "                              learning_rate = 0.005,\n",
        "                              loss_func = nn.NLLLoss(),\n",
        "                              print_every = 5000,\n",
        "                              plot_every = 250,\n",
        "                              model_name = \"./word_RNN\"\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_train_losses)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PUHe_i6laKDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troaOqmAwqq3"
      },
      "source": [
        "**[Important]** Colab sometimes disconnects and erases all your memory. It is painful! We want to help you to minimize the complexity - When you finished running run() function, your model will be saved to the colab directory as the name you put in the *model_name* variable (we are using 'word_RNN' here). Make sure to download it to your PC! When you are disconnected, you don't need to run the model again. You only need to upload the model back to the cobal directory, and reload the model using functions below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rx3K9oHdc-O"
      },
      "outputs": [],
      "source": [
        "# This is how you load your trained model from the directory\n",
        "test_model = RNN(input_size=len(all_letters), hidden_size=50).to(device)\n",
        "test_model.load_state_dict(torch.load(\"word_RNN\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('./')"
      ],
      "metadata": {
        "id": "IQXm15CVIk81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp ./word_RNN \"/content/drive/MyDrive/CIS5300/HW6/2025-06-23/\""
      ],
      "metadata": {
        "id": "zipsqAdLXiWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_train_losses"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jqwb7NYGYFlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_val_losses)"
      ],
      "metadata": {
        "id": "A4qrxv12YIEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz_KfA9JezFR"
      },
      "source": [
        "## **Experiment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsK-sHHIytpS"
      },
      "source": [
        "Let's see how our model performs by calculating validation accuracy! Please use calculateAccuracy() function with word_val_data to evaluate your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv0VgHb53RE6"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode. This changes the behavior of certain layers like dropout layers\n",
        "# that have different behavior during training vs testing (i.e. they are turned off during testing).\n",
        "# In PyTorch, model.eval() is used to set the model to evaluation mode.\n",
        "# Specifically, it changes the behavior of certain layers like dropout layers and batch normalization layers to be fixed during inference.\n",
        "test_model.eval()\n",
        "\n",
        "\n",
        "# Todo: Compute Validation accuracy.\n",
        "# Tip: `word_val_data` is the validation dataset! Please use calculateAccuracy()\n",
        "X_val, y_val = word_val_data\n",
        "\n",
        "val_acc = calculateAccuracy(test_model, X_val, y_val)\n",
        "val_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word_train_data = load_labeled_file(\"complex_words_training.txt\")\n",
        "# word_val_data = load_labeled_file(\"complex_words_development.txt\")\n",
        "\n",
        "# X_word, y_word = word_train_data\n",
        "# print(X_word[:5])\n",
        "# print(y_word[:5])\n",
        "X_val, y_val = word_val_data\n",
        "print(X_val[:5])\n",
        "print(y_val[:5])"
      ],
      "metadata": {
        "id": "2pfnXaevan-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZiVji7YAjLw"
      },
      "outputs": [],
      "source": [
        "assert val_acc > 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80OB4gVQy8wL"
      },
      "source": [
        "In neural networks, \"loss\" is a scalar value representing the difference between the predicted output and the true label. It quantifies how well the model's predictions match the actual data. The objective during training is to minimize this loss value by adjusting the model's parameters.\n",
        "\n",
        "A good training loss is typically a low value, indicating that the model is accurately predicting the training data. However, the training loss alone is not sufficient to evaluate the model's performance, as a very low training loss can indicate overfitting, where the model becomes too specialized to the training data and performs poorly on unseen data.\n",
        "\n",
        "Validation loss is calculated on a separate dataset that the model hasn't seen during training. A good validation loss is also low, but more importantly, it should be comparable to the training loss. If the validation loss is significantly higher than the training loss, it may indicate overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6h4exyWG0h_"
      },
      "source": [
        "To further insights, we should plot training and validation loss. Our run() function records word_val_losses and word_all_losses, which can be used for generating plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWE3_D6SfWqG"
      },
      "outputs": [],
      "source": [
        "# TODO: plot training and validation loss\n",
        "# Your loss is stored at word_val_losses and word_train_losses\n",
        "import matplotlib.pyplot as plt\n",
        "n = len(word_val_losses)\n",
        "x = list(range(n))\n",
        "plt.plot(x, word_train_losses, label='Training Loss', marker='o')\n",
        "plt.plot(x, word_val_losses, label='Validation Loss', marker='s')\n",
        "\n",
        "\n",
        "plt.title('Training Loss and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9PxnWgjHnbL"
      },
      "source": [
        "This is TA's example of training and validation loss curve (your curve might be slightly different) :\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1n7tzqoOne_7tc3avmd2UWIQ9uq1s_jTp'>\n",
        "\n",
        "In the plot above, both the training loss and validation loss exhibit similar trends, which is a positive sign, suggesting that the model is generalizing well to the unseen validation data.\n",
        "\n",
        "From epoch 0 to 12500(x\\*plot_every = 50\\*250), the validation loss decreases slowly from 0.7 to 0.67, indicating a gradual learning process. At epoch 12500, there is a significant drop in validation loss to 0.55, suggesting that the model might have learned some crucial features or representations that considerably improved its performance on the validation set.\n",
        "\n",
        "Post epoch 12500, the validation loss stabilizes and fluctuates around 0.55 till epoch 50000. This stabilization implies that the model has reached a plateau, and further training is not leading to significant improvements in loss reduction on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nof4UnqGft_-"
      },
      "source": [
        "# Section 2: Another classification: city names [22 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj8Ve6lhgBES"
      },
      "source": [
        "Now let's work with another classification example! We will use another dataset we are familiar with - Names of the Cities. The task is to predict the country a city is in.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4wvL8r6nT72"
      },
      "source": [
        "### **Dataset Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxyhglvnYf0"
      },
      "source": [
        "Like we have used in the previous homework before, this dataset has a list of city names and their countries as label. The following countries are included in the dataset.\n",
        "\n",
        "| Index | id | Country |\n",
        "|----------|----------|----------|\n",
        "| 0 | af | Afghanistan |\n",
        "| 1 | cn | China |\n",
        "| 2 | de | Germany |\n",
        "| 3 | fi | Finland |\n",
        "| 4 | fr | France |\n",
        "| 5 | in | India |\n",
        "| 6 | ir | Iran |\n",
        "| 7 | pk | Pakistan |\n",
        "| 8 | za | South Africa |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiIfVtfyH_Aj"
      },
      "source": [
        "We need to re-define our 'group' first, since there are now 9 categories. Let's name it as 'languages'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7smDVHAHsQV"
      },
      "outputs": [],
      "source": [
        "languages = [\"af\", \"cn\", \"de\", \"fi\", \"fr\", \"in\", \"ir\", \"pk\", \"za\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw6stKXCKWYR"
      },
      "source": [
        "Let's also take a look at the dataset. We have finished some preprocessing for you, but each country label is in a string form, while tensor only accepts integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_INIhxiCJABE"
      },
      "outputs": [],
      "source": [
        "city_train_data_raw = readData(\"/content/\", train=True)\n",
        "city_val_data_raw = readData(\"/content/\", train=False)\n",
        "\n",
        "X_city, y_city_str = city_train_data_raw\n",
        "X_val_city, y_val_city_str = city_val_data_raw\n",
        "\n",
        "seed = 12\n",
        "local_random = np.random.RandomState(seed)\n",
        "print(X_city[local_random.choice(len(X_city), 5, replace=False)])\n",
        "local_random = np.random.RandomState(seed)\n",
        "print(y_city_str[local_random.choice(len(y_city_str), 5, replace=False)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGMB7SWlJ7za"
      },
      "source": [
        "In order to do multiclass classification with PyTorch, please pre-process `y_city_str`, using the index-id mapping table provided above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdCDPiiTJ7Dw"
      },
      "outputs": [],
      "source": [
        "# TODO: Please create y_city and y_val_city, where y is a NumPy array of corresponding integer index.\n",
        "# Hint: use .index() function to find index of a value in a list\n",
        "country_dict = {\"af\": 0,\n",
        "\"cn\": 1,\n",
        "\"de\": 2,\n",
        "\"fi\": 3,\n",
        "\"fr\": 4,\n",
        "\"in\": 5,\n",
        "\"ir\": 6,\n",
        "\"pk\": 7,\n",
        "\"za\": 8}\n",
        "\n",
        "y_city = [country_dict[city_str] for city_str in y_city_str]\n",
        "y_val_city = [country_dict[city_str] for city_str in y_val_city_str]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_city_str[0]"
      ],
      "metadata": {
        "id": "6OgZHjC3gz0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G99nKtewLaWe"
      },
      "outputs": [],
      "source": [
        "assert y_city[10] == 0\n",
        "assert y_city[10000] == 3\n",
        "assert y_city[25000] == 8\n",
        "assert y_val_city[10] == 0\n",
        "assert y_val_city[100] == 1\n",
        "assert y_val_city[500] == 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x51SlqcNW9bG"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE\n",
        "city_train_data = X_city, y_city\n",
        "city_val_data = X_val_city, y_val_city\n",
        "\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "city_tests = [15, 12500, 19000]\n",
        "city_val_tests = [5, 120, 480]\n",
        "city_indices = [y_city[x] for x in city_tests]\n",
        "city_val_indices = [y_val_city[x] for x in city_val_tests]\n",
        "grader.grade(test_case_id = 'testIndexedY', answer = (city_indices,city_val_indices))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hmoApzAJ9qZ"
      },
      "source": [
        "### **Remake the model and functions: from binary to multiclass**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y77IKupcLyJQ"
      },
      "source": [
        "The model and functions we used in the previous example is made for binary classification, where output classes are only 0 and 1. In this problem, we have 8 different categories. What should we change? We need to make our model accepts multiple outputs dimension, which includes fixing `RNN()`, `predict()`, `calculateAccuracy()`, and `run()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMIfYy7XMIsw"
      },
      "source": [
        "Let's start with the RNN model first. We assumed there were only 2 categories in RNN(), but now we have more than that. Please build RNN_multi() for multiclass classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ9IefbfMHUX"
      },
      "outputs": [],
      "source": [
        "class RNN_multi(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        '''\n",
        "        TODO: Remake the __init__ function to adapt multiclass classification\n",
        "        The function should accept various range of output size.\n",
        "\n",
        "        Inputs:\n",
        "            self: points to initialized object\n",
        "            input_size: dimensions of input tensor\n",
        "            hidden_size: dimensions of the hidden layer\n",
        "            output_size: dimensions of the expected output tensor\n",
        "        Returns:\n",
        "            nothing, it initializes the RNN object\n",
        "\n",
        "        '''\n",
        "        super(RNN_multi, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        if input.device != hidden.device:\n",
        "            input = input.to(hidden.device)\n",
        "\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.h2o(hidden)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgUBQON9OCfE"
      },
      "outputs": [],
      "source": [
        "# TEST - it should accept different output size! (Should be fine if there is no error)\n",
        "RNN_multi(1,1,1)\n",
        "RNN_multi(10,10,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTqpFHEOzwFN"
      },
      "outputs": [],
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "test_models = [str(RNN_multi(100,100,100)), str(RNN_multi(20,20,20))]\n",
        "grader.grade(test_case_id = 'testRNN_multi', answer = test_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guy42-VOSh5F"
      },
      "source": [
        "The `predict()` function we used was designed for binary classification only too. We need to fix how prediction is derived from `output` of the model.\n",
        "\n",
        "HINT: We are using `nn.NLLLoss()` (negative log lokelihood), model's last layer is a logarithm of probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zGFui925Web"
      },
      "outputs": [],
      "source": [
        "def predict_multi(model, X, y=None, loss_func=None):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pred = []\n",
        "        val_loss = []\n",
        "        for ind in range(X.shape[0]):\n",
        "            hidden = model.initHidden().to(device)\n",
        "            val = lineToTensor(X[ind])\n",
        "            for i in range(val.size()[0]):\n",
        "                output, hidden = model(val[i], hidden)\n",
        "            # TODO: fill this part to get prediction from output\n",
        "            # pseudocode:\n",
        "            #   1. Get the index of the maximum value of the output tensor\n",
        "            #   2. Append it to the pred list\n",
        "            if y is not None and loss_func is not None:\n",
        "                category_tensor = torch.tensor([int(y[ind])]).to(device)\n",
        "                val_loss.append(loss_func(output, category_tensor).data.item())\n",
        "    if y is not None and loss_func is not None:\n",
        "        return sum(val_loss) / len(val_loss)\n",
        "    return np.array(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_3DWwtNbb9o"
      },
      "source": [
        "Let's also update `calculateAccuracy()` function, where `predict_multi()` shuould be updated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aZn9SNQbdXF"
      },
      "outputs": [],
      "source": [
        "def calculateAccuracy_multi(model, X, y):\n",
        "    preds = predict_multi(model, X)\n",
        "    return accuracy_score(y, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwuvXV4iT7sR"
      },
      "source": [
        "Finally, let's update `run()` function to address the changes we have made above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPedF0IlUCpV"
      },
      "outputs": [],
      "source": [
        "def run_multi(train_data, val_data, hidden_size, n_epochs, learning_rate, loss_func, print_every, plot_every, model_name):\n",
        "    X, y = train_data\n",
        "    X_val, y_val = val_data\n",
        "    # TODO: add RNN_multi() here\n",
        "\n",
        "    model = model.to(device)\n",
        "    current_loss = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(0, n_epochs):\n",
        "        output, loss, line, category = trainOneEpoch(model,\n",
        "                    criterion = loss_func,\n",
        "                    optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate),\n",
        "                    X=X,\n",
        "                    y=y)\n",
        "        current_loss += loss\n",
        "\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            # TODO: design your own report to print (freestyle!)\n",
        "            # What you can do:\n",
        "            #   1. make prediction\n",
        "            #   2. compare with gold label to see right or wrong\n",
        "            #   3. report the number of epoch, the percentage of completion, loss,...\n",
        "            pass\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            train_losses.append(current_loss/plot_every)\n",
        "            current_loss= 0\n",
        "\n",
        "            # Validation Loss\n",
        "            # TODO: update with predict_multi() function\n",
        "\n",
        "    torch.save(model.state_dict(), model_name)\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTBGaEbCy5We"
      },
      "source": [
        "### **Test the new model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPv2i05R5K-T"
      },
      "source": [
        "Let's test our model with these hyperparameters first. They are not great, but we will tune them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_sLy4LefhqR"
      },
      "outputs": [],
      "source": [
        "# This will take around 7 mins\n",
        "city_all_losses, city_val_losses = run_multi(train_data = city_train_data,\n",
        "                              val_data = city_val_data,\n",
        "                              hidden_size = 10,\n",
        "                              n_epochs = 50000,\n",
        "                              learning_rate = 0.01,\n",
        "                              loss_func = nn.NLLLoss(),\n",
        "                              print_every = 5000,\n",
        "                              plot_every = 250,\n",
        "                              model_name = \"./city_RNN\"\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uOzePr3uDDK"
      },
      "outputs": [],
      "source": [
        "# Load saved model\n",
        "test_model_multi = RNN_multi(input_size=len(all_letters), hidden_size=10, output_size=len(languages)).to(device)\n",
        "test_model_multi.load_state_dict(torch.load(\"./city_RNN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhctnbcxYK9c"
      },
      "source": [
        "As we have done before, please calculate validation accuracy and draw a training and validation loss plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZRzqKk3uKXG"
      },
      "outputs": [],
      "source": [
        "test_model_multi.eval()\n",
        "# TODO: Compute Validation accuracy. Tip: `city_val_data` is the validation dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3JYpw8tp6tV"
      },
      "outputs": [],
      "source": [
        "# TODO: Draw training and validation loss plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2OK-DmHzCD4"
      },
      "source": [
        "### **Hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uadgnDNRnwHN"
      },
      "source": [
        "The current set of hyperparameters will very likely to make your model generate `Nan` loss, and it often does not learn the data well. What is the problem? The main reason is the model we used in word classification is not suitable for this task. Your main goal in this section is improving the model with \"hyperparameter tuning\".\n",
        "\n",
        "We now have 3 hyperparameters to tune:   \n",
        "1. **learning_rate** represents how fast the model learn for each backpropagation. The higher a value, the faster a model converges. However if a learning rate is too high, it will cause a lot of problems like overshooting (can't converge, bouncing around the minimum) or divergence (loss goes to infinity).\n",
        "2. **hidden_size** defines the size of the hidden layer in your RNN model. The larger the size, the more complex model can be (and slower).\n",
        "3. **n_epochs** controls the number of epoch the model use for training. The more we train, the more it learns from training dataset (and takes longer time). Becareful for overfitting!\n",
        "\n",
        "\n",
        "Please do the experiments below, and write your finding in another file to uploaded as a PDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SviwU4ACvKCI"
      },
      "source": [
        "### **Task 2.1:** learning rate tunning [6 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3EghVmjstZ_"
      },
      "source": [
        "From the base hyperparameters we gave, adjust the learning_rate = {0.0002, 0.002 0.02} and compare the result, what have you found?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAd_PowOp6SG"
      },
      "outputs": [],
      "source": [
        "# TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5MWnYIPudui"
      },
      "source": [
        "### **Task 2.2:** hidden layer size tunning [6 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEt7fSIEsyJB"
      },
      "source": [
        "With the best learning rate you have, build models with hidden_size = {10, 50, 100} and compare the result, what have you found?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO3QLqZ0udSG"
      },
      "outputs": [],
      "source": [
        "# TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msm0HtfYut5P"
      },
      "source": [
        "### **Task 2.3:** epoch tuning [6 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BJ3FsTMs1HN"
      },
      "source": [
        "With the best learnign rate and hidden_size you got, adjust the model with epoch = {1000, 50000, 100000} and compare the result, what have you found?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDjfGGoPvJh-"
      },
      "outputs": [],
      "source": [
        "# TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqlMNGJA1rd_"
      },
      "source": [
        "## **Task 2.4(optional):** Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm7_1s681m4R"
      },
      "source": [
        "There are still a lot of other items you can adjust:\n",
        "1. Model structure (number of layers, type of layers)\n",
        "2. Optimizer: SGD, Adagrad, Adam, ...\n",
        "3. Criterion(loss function): MSE, Cross-Entropy Loss, ...\n",
        "4. And even more...\n",
        "\n",
        "Try to make your best model and compete with others! Write code to make predictions on the provided test set `cities_test.txt`. The test set has one unlabeled city name per line. Your code should output a file `labels.txt` with one two-letter country code per line. Make sure your test accuracy can pass the auto-grader and then experiment to achieve higher values. Extra credit (5 points) will be given to the top 5 leaderboard submissions. Here are some ideas for improving your leaderboard performance:\n",
        "\n",
        " - Try dropout if your model is overfitting\n",
        " - Experiment with different loss functions, optimizers\n",
        " - Compare the different types of RNNs - RNN, LSTM, GRU units.\n",
        " - Use a different initalization for the weights, for example, small random values instead of 0s\n",
        "\n",
        "In your report, describe your final model and training parameters.\n",
        "\n",
        "Another tip for experimenting with neural network hyperparameters is to maintain notes (e.g. a spreadsheet or text-file) with different parameters and their resulting accuray. As you can imagine, there is a combinatorial explosion of the possible hyperparameter space so navigating it efficiently is best done by remembering your past experiments. Feel free to include this in your report as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr_cVMk01mYz"
      },
      "outputs": [],
      "source": [
        "# TODO(optional):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0yHNsBJpobw"
      },
      "source": [
        "# Submission\n",
        "### Congratulation on finishing your homework! Here are the deliverables you need to submit to GradeScope:\n",
        "\n",
        "  - `writeup.pdf`: including Task 2.1, 2.2, and 2.3\n",
        "    - 2.4(optional)\n",
        "  - `homework6.ipynb`: This .ipynb notebook\n",
        "  - `homework6.py`: .py file of this notebook\n",
        "  - `labels.txt`: predictions for leaderboard (optional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeLuefQAsGjq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}