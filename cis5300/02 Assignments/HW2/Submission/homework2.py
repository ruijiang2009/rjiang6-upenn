# -*- coding: utf-8 -*-
"""CIS5300-OL-hw2-v0.5_rjiang6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XX3UnXy3goSoqcRUd-waPAZHo-kktz3a

# Homework 2: Text Classification
## Due Date: (TBD)
## Total Points: (TBD)
- **Overview**: For this assignment, we’ll be building a text classifier. The goal of our text classifer will be to distinguish between words that are simple and words that are complex. Example simple words are `heard`, `sat`, and `town`, and example complex words are `abdicate`, `detained`, and `vintners`. Distinguishing between simple and complex words is the first step in a larger NLP task called text simplification, which aims to replace complex words with simpler synonyms. Text simplification is potentially useful for re-writing texts so that they can be more easily understood by younger readers, people learning English as a second language, or people with learning disabilities.

- **Learning goals**:
    - Understand an important class of NLP evaluation methods (precision, recall and F1), and implement them yourself.
    - Employ common experimental design practices in NLP. Split the annotated data into training/development/test sets, implement simple baselines to determine how difficult the task is, and experiment with a range of features and models.
    - Get an introduction to `sklearn`, an excellent machine learning Python package.

- **Data**: We will provide you with training and development data that has been manually labeled. We will also give you a test set without labels. You will build a classifier to predict the labels on our test set. You can upload your classifier’s predictions to Gradescope. We will score its predictions and maintain a leaderboard showing whose classifier has the best performance.

---
- **Delieverables:** This assignment has several deliverables:
  - Code (this notebook) *(Automatic Graded)*
    - rename to `homework2.ipynb` and `homework2.py`.
  - Write Up (in a separate **writeup.pdf**) *(Manually Graded)*
    - Answers to all questions labeled as `Answer #.#` in a file named `writeup.pdf`
  - Leaderboard: Section 4 *(Automatic Graded on GradeScope)*
    - Your best model’s output for the test set using only the provided training and development data (`test_labels.txt`)
    - (Optional) your best model’s output for the test set, using any data that you want. (`test_labels.txt`)

- **Grading**: We will use the auto-grading system called `PennGrader`. To complete the homework assignment, you should implement anything marked with `#TODO` or `##YOUR CODE HERE` and run the cell with `#PennGrader` note.

## Recommended Readings
- [Naive Bayes Classification and Sentiment](https://web.stanford.edu/~jurafsky/slp3/4.pdf) Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd edition draft).
- [Logistic Regression](https://web.stanford.edu/~jurafsky/slp3/5.pdf) Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd edition draft) .
- [Problems in Current Text Simplification Research: New Data Can Help](http://www.cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf) Wei Xu, Chris Callison-Burch, and Courtney Napoles. TACL 2015.
- [Comparison of Techniques to Automatically Identify Complex Words](http://aclweb.org/anthology/P/P13/P13-3015.pdf) Matthew Shardlow. ACL 2013.
- [SemEval 2016 Task 11: Complex Word Identification](https://www.researchgate.net/profile/Gustavo_Paetzold/publication/305334627_SemEval_2016_Task_11_Complex_Word_Identification/links/57bab70a08ae14f440bd9722/SemEval-2016-Task-11-Complex-Word-Identification.pdf) Gustavo Paetzold and Lucia Specia. ACL 2016.

## Setup 1: PennGrader Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# ## DO NOT CHANGE ANYTHING, JUST RUN
# %%capture
# !pip install penngrader-client dill

# Commented out IPython magic to ensure Python compatibility.
# %%writefile notebook-config.yaml
# 
# grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'
# grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'

!cat notebook-config.yaml

from penngrader.grader import *

## TODO - Start
STUDENT_ID = 62502470 # YOUR PENN-ID GOES HERE AS AN INTEGER#
## TODO - End

SECRET = STUDENT_ID
grader = PennGrader('notebook-config.yaml', 'CIS5300_OL_23Su_HW2', STUDENT_ID, SECRET)

def reload_grader():
    grader = PennGrader('notebook-config.yaml', 'CIS5300_OL_23Su_HW2', STUDENT_ID, SECRET)
    return grader

# check if the PennGrader is set up correctly
# do not chance this cell, see if you get 4/4!
name_str = 'Rui Jiang'
grader.grade(test_case_id = 'name_test', answer = name_str)

"""## Setup 2: Dataset / Packages
- **Run the following cells without changing anything!**
- [Loading dataset from huggingface](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html#from-local-files)
"""

from collections import defaultdict
import gzip

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
import numpy as np
from tqdm import tqdm
from dill.source import getsource

#%%capture
# data
!gdown 16KoidumvFEoI9hliqgrPiWhvhFHkqMEJ # https://drive.google.com/file/d/16KoidumvFEoI9hliqgrPiWhvhFHkqMEJ/view?usp=sharing
!gdown 17xaJNRt3DY2zhEgBE2zj0JEuMqf8CCdk # https://drive.google.com/file/d/17xaJNRt3DY2zhEgBE2zj0JEuMqf8CCdk/view?usp=sharing
!gdown 1JX-G-olW84eckkGW-1OC-5XeVJ-Yx3RK # https://drive.google.com/file/d/1JX-G-olW84eckkGW-1OC-5XeVJ-Yx3RK/view?usp=sharing
!gdown 1ppyM-7kFyabNG8zOudsTuhWl-2j-zy5Z # https://drive.google.com/file/d/1ppyM-7kFyabNG8zOudsTuhWl-2j-zy5Z/view?usp=sharing
!curl -L -o ngram_counts.txt.gz http://www.cis.upenn.edu/~cis5300/18sp/data/ngram_counts.txt.gz

!gdown 1rkYaFae_qJk1AlHORDLH3aPH47f9DA8s



train_file = "complex_words_training.txt"
dev_file = "complex_words_development.txt"
test_file = "complex_words_test_unlabeled.txt"
mini_test_file = 'complex_words_test_mini.txt'

########## DO NOT CHANGE ##########
## Loads in the words and labels of one of the datasets
def load_labeled_file(data_file):
    words = []
    labels = []
    with open(data_file, 'rt', encoding="utf8") as f:
        i = 0
        for line in f:
            if i > 0:
                line_split = line[:-1].split("\t")
                words.append(line_split[0].lower())
                labels.append(int(line_split[1]))
            i += 1
    return words, labels

def load_unlabeled_file(data_file):
    words = []
    # labels = []
    with open(data_file, 'rt', encoding="utf8") as f:
        words = [line.strip() for line in f.readlines() if len(line.strip()) > 0]
    return words

def load_test_file(data_file):
    words = []
    with open(data_file, 'rt', encoding="utf8") as f:
        next(f) # skip first line (header)
        words = [line.strip().split('\t')[0] for line in f.readlines() if len(line.strip()) > 0]
    return words

## Loads Google NGram counts
def load_ngram_counts(ngram_counts_file = 'ngram_counts.txt.gz'):
    counts = defaultdict(int)
    with gzip.open(ngram_counts_file, 'rt') as f:
        for line in f:
            token, count = line.strip().split('\t')
            if token[0].islower():
                counts[token] = int(count)
    return counts
ngram_counts = load_ngram_counts()

"""### About the data
- `train_data`, `dev_data`: these are words with labels, provided for you to train and evaluate your models.
- `mini_test_words`: this is a subset (50) of the unseen test dataset, we will provide test the **basic functionality** of your models based on the performance on this mini testset (PennGrader tests). **This would only be a basic sanity check of your implementation. The final grade would be based on the PennGrader Grades and manual grading on your implementation**
- `test_words`: this is the full, unlabelled test set. You are expected to submit the prediction of these words at the end of the assignment.
"""

########## DO NOT CHANGE ##########
train_data = load_labeled_file(train_file)
dev_data = load_labeled_file(dev_file)
mini_test_words = load_unlabeled_file(mini_test_file)
test_words = load_test_file(test_file)

# you can take a look at this mini-dev data by uncommenting this line
dev_words, dev_labels = dev_data
dev_words[:5] # some examples of this dev dataset

mini_test_words[:5]

"""# Section 1. Implement the Evaluation Metrics
- You might find this [Wikipedia Page](https://en.wikipedia.org/wiki/Precision_and_recall) useful.
- **Problem 1.1** Predicion, Recall, F1 Score
"""

## Input: y_pred, a list of length n with the predicted labels,
## y_true, a list of length n with the true labels

## Calculates the precision of the predicted labels
def get_precision(y_true, y_pred):
    ## YOUR CODE HERE...
    n = len(y_pred)
    num_true_positive = 0
    num_false_positive = 0
    for i in range(n):
      if y_pred[i]:
        if y_true[i]:
          num_true_positive += 1
        else:
          num_false_positive += 1
    if num_true_positive + num_false_positive == 0:
        return 0.0  # Avoid division by zero
    return num_true_positive/ (num_true_positive + num_false_positive)

## Calculates the recall of the predicted labels
def get_recall(y_true, y_pred):
    ## YOUR CODE HERE...
    n = len(y_pred)
    num_true_positive = 0
    num_false_negative = 0
    for i in range(n):
      if y_pred[i]:
        if y_true[i]:
          num_true_positive += 1
      else: # y_pred[i] == False
        if y_true[i]: # y_true[i] == True
          num_false_negative += 1
    if num_true_positive + num_false_negative == 0:
        return 0.0  # Avoid division by zero
    recall = num_true_positive/ (num_true_positive + num_false_negative)
    return recall

## Calculates the f-score of the predicted labels
def get_fscore(y_true, y_pred):
    ## YOUR CODE HERE...
    precision = get_precision(y_true, y_pred)
    recall = get_recall(y_true, y_pred)

    if (precision + recall) == 0.0:
      return 0.0
    fscore = (2 * precision * recall) / (precision + recall)
    return fscore

# PennGrader - DO NOT CHANGE
# grader.grade(test_case_id = 'test_q11_eval_funcs', answer = (get_precision, get_recall, get_fscore))
grader.grade(test_case_id = 'test_q11_eval_funcs', answer = [getsource(get_precision), getsource(get_recall), getsource(get_fscore)])

"""# Section 2. Baselines

## 2.1 Implement a majority class baseline
You should start by implementing simple baselines as classifiers. Your first baseline is a **majority class baseline** which is one of the most simple classifier. You should complete the function `all_complex(words)`, which takes in a list of words, and returns out the predictions.

- **Problem 2.1:** Implement `all_complex()` that always predicts the majority class of the data (i.e. predicting every word as complex)
"""

## Labels every word complex
def all_complex(words):
    ## YOUR CODE HERE...
    y_pred = []
    for word in words:
      y_pred.append(True)
    return y_pred

dev_words, dev_labels = dev_data

"""- **Answer 2.1:** Please report the precision, recall, and f-score on both the training data and the development data.
    - Training **[writeup.pdf]**
    - Development **[writeup.pdf]**
"""

dev_words, dev_labels = dev_data
y_pred = all_complex(dev_words)
precision = get_precision(dev_labels, y_pred)
recall = get_recall(dev_labels, y_pred)
fscore = get_fscore(dev_labels, y_pred)
fscore
print("precision is {}".format(precision))
print("recall is {}".format(recall))
print("fscore is {}".format(fscore))

# training data
training_words, training_labels = train_data
y_pred = all_complex(training_words)
tprecision = get_precision(training_labels, y_pred)
trecall = get_recall(training_labels, y_pred)
tfscore = get_fscore(training_labels, y_pred)
print("tprecision is {}".format(tprecision))
print("trecall is {}".format(trecall))
print("tfscore is {}".format(tfscore))

# PennGrader - DO NOT CHANGE
all_complex_pred = all_complex(mini_test_words)
grader.grade(test_case_id = 'test_baseline_q21', answer = all_complex_pred)

"""## 2.2 Word length baseline
For our next baseline, we will use a slightly complex baseline, the length of each word to predict its complexity.

For the word length baseline, you should try setting various thresholds for word length to classify them as simple or otherwise. For example, you might set a threshold of 9, meaning that any words with less than 9 characters will be labeled simple, and any words with 9 characters or more will be labeled complex. Once you find the best threshold using the training data, use this same threshold for the development data as well.

You will be filling in the function `word_length_threshold(train_data, dev_data)`. This function takes in both the training and development datasets, finds the best threshold on word length and returns out the predictions on development data for your best threshold.

Usually, Precision and Recall are inversely related and while building binary-classification systems we try to find a good balance between them (by maximizing f-score, for example). It is often useful to plot the Precision-Recall curve for various settings of the classifier to gauge its performance and compare it to other classifiers. For example, for this baseline, a Precision-Recall curve can be plotted by plotting the Precision (on the y-axis) and Recall (on the X-axis) for different values of word-length threshold.

- **Problem 2.2:** Implement `word_length_threshold()` that finds the best word length threshold and makes predictions on the development (or testing) data.
"""

### 2.2: Word length thresholding
## Makes feature matrix for word_length_threshold

global_word_length_threshold = 7

def length_threshold_feature(words, threshold):
    # return predictions based on the threshold
    if len(words) < threshold:
      return False # simple words
    return True # complex words

## Finds the best length threshold by f-score, and uses this threshold to
## classify the training and development set
def word_length_threshold(train_data, dev_words):
    twords, tlabels = train_data

    ## YOUR CODE HERE
    dev_pred = []
    for word in dev_words:
      dev_pred.append(length_threshold_feature(word, global_word_length_threshold))
    return dev_pred

"""- **Answer 2.2:** Please report the precision, recall, and f-score on both the training data and the development data.
    - Training **[writeup.pdf]**
    - Development **[writeup.pdf]**
    - Range of thresholds tested using the training data **[writeup.pdf]**
    - Best threshold **[writeup.pdf]**
    - Precision-recall Curve for training data **[writeup.pdf]**
        - For plotting, [matplotlib](https://matplotlib.org/) is a useful python library
"""

dev_words, dev_labels = dev_data
y_pred = word_length_threshold(train_data, dev_words)
dprecision = get_precision(dev_labels, y_pred)
drecall = get_recall(dev_labels, y_pred)
dfscore = get_fscore(dev_labels, y_pred)
print("dprecision is {}".format(dprecision))
print("drecall is {}".format(drecall))
print("dfscore is {}".format(dfscore))

def word_length_threshold_with_length(train_data, dev_words, length_threshold):
    twords, tlabels = train_data

    ## YOUR CODE HERE
    dev_pred = []
    for word in dev_words:
      dev_pred.append(length_threshold_feature(word, length_threshold))
    return dev_pred

precision_list = []
recall_list = []
fscore_list = []
max_fscore = 0
max_threshold = 0
for threshold in range (1, 13):
  dev_words, dev_labels = dev_data
  y_pred = word_length_threshold_with_length(train_data, dev_words, threshold)
  dprecision = get_precision(dev_labels, y_pred)
  drecall = get_recall(dev_labels, y_pred)
  dfscore = get_fscore(dev_labels, y_pred)
  precision_list.append(dprecision)
  recall_list.append(drecall)
  fscore_list.append(dfscore)
  if dfscore > max_fscore:
    max_fscore = dfscore
    max_threshold = threshold

print("precision_list {}".format(precision_list))
print("recall_list {}".format(recall_list ))
print("fscore_list {}".format(fscore_list))
print("max_fscore {}, max_threshold {}".format(max_fscore, max_threshold))

import matplotlib.pyplot as plt
# precision = [0.35555555555555557, 0.35555555555555557, 0.35555555555555557, 0.35555555555555557, 0.35555555555555557, 0.35555555555555557, 0.35555555555555557, 0.34782608695652173, 0.34782608695652173, 0.34782608695652173, 0.34782608695652173, 0.34782608695652173, 0.34782608695652173, 0.34782608695652173, 0.34782608695652173]
# recall = [0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249, 0.03827751196172249]
# labels = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"]
# for i in range(12):
#     # plt.text(precision[i], recall[i], labels[i], fontsize=8)
#     plt.annotate(labels[i], (precision[i], recall[i]))
plt.scatter(precision_list, recall_list)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.show()

# training data
training_words, training_labels = train_data
y_pred = []
for word in training_words:
  y_pred.append(length_threshold_feature(word, 7))
tprecision = get_precision(training_labels, y_pred)
trecall = get_recall(training_labels, y_pred)
tfscore = get_fscore(training_labels, y_pred)
print("tprecision is {}".format(tprecision))
print("trecall is {}".format(trecall))
print("tfscore is {}".format(tfscore))



# PennGrader - DO NOT CHANGE
word_length_pred = word_length_threshold(train_data, mini_test_words)
grader.grade(test_case_id = 'test_baseline_q22', answer = word_length_pred)

"""## 2.3 Word frequency baseline

Our final baseline is a classifier similar to the last one, but thresholds on word frequency instead of length. We have provided Google NGram frequencies in the text file ngram_counts.txt, along with the helper function `load_ngram_counts(ngram_counts_file)` to load them into Python as a dictionary.

You will be filling in the function `word_frequency_threshold(train_data, dev_data, ngram_counts)`, where `ngram_counts` is the dictionary of word frequencies. This function again finds the best threshold and returns predictions on the development data.

Please again report the precision, recall, and f-score on the training and development data individually, along with the range of thresholds you tried, and the best threshold to be graded. Similar to the previous baseline, plot the Precision-Recall curve for range of thresholds you tried. Compared with word length baseline, which classifier looks better on average?

**Note: Due to its size, loading the ngram counts into Python takes around 20 seconds, and finding the correct threshold may take a few minutes to run.**

- **Problem 2.3:** Implement `word_frequency_threshold()` that finds the best word frequency threshold and makes predictions on the development (or testing) data.
"""

## Make feature matrix for word_frequency_threshold
def frequency_threshold_feature(words, threshold, ngram_counts):
    # return predictions based on the threshold
    ## YOUR CODE HERE
    # hint: check the content of `ngram_counts` variable
    # if words not in ngram_counts:
    #   return True
    if ngram_counts[words] > threshold:
      return False # simple words
    return True # complex words

def word_frequency_threshold(train_data, dev_words, ngram_counts):
    twords, tlabels = train_data

    ## YOUR CODE HERE

    dev_pred = []
    for word in dev_words:
      dev_pred.append(frequency_threshold_feature(word, 15100000, ngram_counts))
    return dev_pred

dev_words, dev_labels = dev_data
y_pred = word_frequency_threshold(train_data, dev_words, ngram_counts)

dprecision = get_precision(dev_labels, y_pred)
drecall = get_recall(dev_labels, y_pred)
dfscore = get_fscore(dev_labels, y_pred)
# dfscore
print("dprecision is {}".format(dprecision))
print("drecall is {}".format(drecall))
print("dfscore is {}".format(dfscore))

# training data
training_words, training_labels = train_data
y_pred = []
for word in training_words:
  y_pred.append(frequency_threshold_feature(word, 15100000, ngram_counts))
tprecision = get_precision(training_labels, y_pred)
trecall = get_recall(training_labels, y_pred)
tfscore = get_fscore(training_labels, y_pred)
print("tprecision is {}".format(tprecision))
print("trecall is {}".format(trecall))
print("tfscore is {}".format(tfscore))

ngram_counts
max_count = 0
min_count = 1000000
for word, counts in ngram_counts.items():
  if max_count < counts:
    max_count = counts
  if min_count > counts:
    min_count = counts

print("max_count {} min_count {}".format(max_count, min_count))

def word_frequency_threshold_frequency(train_data, dev_words, ngram_counts, frequency):
    twords, tlabels = train_data

    ## YOUR CODE HERE

    dev_pred = []
    for word in dev_words:
      dev_pred.append(frequency_threshold_feature(word, frequency, ngram_counts))
    return dev_pred

def run_multiple_round(train_data, dev_words, ngram_counts):
  precision_list = []
  recall_list = []
  fscore_list = []
  frequency_list = []
  max_fscore = 0
  max_threshold = 0
  for i in range(100000, 47376829651, 500000):
    dev_words, dev_labels = dev_data
    y_pred = word_frequency_threshold_frequency(train_data, dev_words, ngram_counts, i)
    dprecision = get_precision(dev_labels, y_pred)
    drecall = get_recall(dev_labels, y_pred)
    dfscore = get_fscore(dev_labels, y_pred)
    precision_list.append(dprecision)
    recall_list.append(drecall)
    fscore_list.append(dfscore)
    frequency_list.append(i)
    if dfscore > max_fscore:
      max_fscore = dfscore
      max_threshold = i

  return precision_list, recall_list, fscore_list, frequency_list, max_fscore, max_threshold

dev_words, dev_labels = dev_data
freq_precision_list, freq_recall_list, freq_fscore_list, frequency_list, max_fscore, max_threshold = run_multiple_round(train_data, dev_words, ngram_counts)
print("precision_list is {}".format(freq_precision_list))
print("recall_list is {}".format(freq_recall_list))
print("fscore_list is {}".format(freq_fscore_list))
print("max_fscore {} max_threshod {}".format(max_fscore, max_threshold))

import matplotlib.pyplot as plt
plt.scatter(freq_recall_list, freq_precision_list)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.show()

print("max_fscore {} max_threshod {}".format(max_fscore, max_threshold))

index = 0
for threshold in frequency_list:
  index += 1
  if threshold == 15100000:
    print("index is {}".format(index))

freq_recall_list[30]

freq_precision_list[30]

"""- **Answer 2.3:** Please report the precision, recall, and f-score on both the training data and the development data.
    - Training **[writeup.pdf]**
    - Development **[writeup.pdf]**
    - Range of thresholds tested using the training data **[writeup.pdf]**
    - Best threshold **[writeup.pdf]**
    - Precision-recall Curve for training data **[writeup.pdf]**
        - For plotting, [matplotlib](https://matplotlib.org/) is a useful python library
"""

dev_words, dev_labels = dev_data
y_pred = word_frequency_threshold(train_data, dev_words, ngram_counts)

dprecision = get_precision(dev_labels, y_pred)
drecall = get_recall(dev_labels, y_pred)
dfscore = get_fscore(dev_labels, y_pred)
# dfscore
print("dprecision is {}".format(dprecision))
print("drecall is {}".format(drecall))
print("dfscore is {}".format(dfscore))



# PennGrader - DO NOT CHANGE
word_freq_pred = word_frequency_threshold(train_data, mini_test_words, ngram_counts)
grader.grade(test_case_id = 'test_baseline_q23', answer = word_freq_pred)

"""# Section 3. Classifiers

## 3.1 Naive Bayes classification

Now, let’s move on to actual machine learning classifiers! For our first classifier, you will use the built-in [Naive Bayes model from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), to train a classifier. You should refer to the online sklearn documentation when you are building your classifier.

The first thing to note is that sklearn classifiers take in `numpy` arrays, rather than regular lists. You may use the [online numpy documentation](https://numpy.org/doc/stable/). To create a `numpy` list of length 5, you can use the following Python commands:

```
import numpy as np
X = np.array([1,2,3,4,5])
```


To train a classifier, you need two numpy arrays: `X_train`, an `m` by `n` array, where `m` is the number of words in the dataset, and `n` is the number of features for each word; and `Y`, an array of length `m` for the labels of each of the words.

**Before we start training models, we need to convert our texts/words into numpy arrays, i.e. making training/testing feature vectors.**

- **Problem 3.0**: Implementing `get_training_features()` and `get_test_features()` that convert train/test dataset to numpy arrays
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # run the following cell if you want to use count of syllabus as a feature
# !pip install syllables
# import syllables

# True means complex word
# False means: simple word
def syllables_threshold_feature(word, threshold):
  return syllables.estimate(word) < threshold

def syllables_threshold(train_data, dev_words):
    twords, tlabels = train_data

    ## YOUR CODE HERE

    dev_pred = []
    for word in dev_words:
      dev_pred.append(syllables_threshold_feature(word, 7))
    return dev_pred

dev_words, dev_labels = dev_data
y_pred = syllables_threshold(train_data, dev_words)

dprecision = get_precision(dev_labels, y_pred)
drecall = get_recall(dev_labels, y_pred)
dfscore = get_fscore(dev_labels, y_pred)

print("dprecision is {}".format(dprecision))
print("drecall is {}".format(drecall))
print("dfscore is {}".format(dfscore))

def get_training_features(train_data, ngram_counts):
    words, y_true = train_data
    features = []
    for word in words:
        ## YOUR CODE HERE
        # 1. word length feature
        # 2. unigram frequency feature
        # 3. (optional) syllables count feature
        length_feature = length_threshold_feature(word, 9)
        unigram_feature = frequency_threshold_feature(word, 200, ngram_counts)
        syllables_feature = syllables_threshold_feature(word, 7)
        features.append([length_feature, unigram_feature, syllables_feature])

    X = np.asarray(features)
    ## YOUR CODE HERE
    # TODO: calculate the mean and std of the vectorized data (X)
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)

    # TODO: calculate the scaled data, with the mean and std you just calculated
    X_scaled = (X - means) / stds

    Y = np.asarray(y_true)
    return X_scaled, Y, means, stds

def get_test_features(dev_words, ngram_counts, means, stds):

    ## YOUR CODE HERE
    # TODO: do the similar thing, except using means and stds as given (from training data)
    features = []
    for word in dev_words:
        ## YOUR CODE HERE
        # 1. word length feature
        # 2. unigram frequency feature
        # 3. (optional) syllables count feature
        length_feature = length_threshold_feature(word, 9)
        unigram_feature = frequency_threshold_feature(word, 200, ngram_counts)
        syllables_feature = syllables_threshold_feature(word, 7)
        features.append([length_feature, unigram_feature, syllables_feature])

    X = np.asarray(features)
    X_scaled = (X - means) / stds

    return X_scaled

# PennGrader - DO NOT CHANGE
dev_words, dev_labels = dev_data
X_train, Y_train, means, stds = get_training_features(train_data, ngram_counts)
X_dev = get_test_features(dev_words, ngram_counts, means, stds)

train_feats = (X_train, Y_train, means, stds)
test_feats = (X_dev)

grader.grade(test_case_id = 'test_q30_test_feature_shapes', answer = (train_feats, test_feats))

"""Once we have these feature arrays, we can fit a Naive Bayes classifier using the following commands:
```
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train, Y)
```
Finally, to use your model to predict the labels for a set of words, you only need one numpy array: `X_test`, an `m` by `n` array, where `m` is the number of words in the test set, and `n` is the number of features for each word. Note that the `n` used here is the same as the `n` in `X_train`. Then, we can use our classifier to predict labels using the following command:

```
Y_pred = clf.predict(X_test)
```

- **Problem 3.1**: Fill in the function `naive_bayes(train_data, dev_data, ngram_counts)`. This function will train a Naive Bayes classifier on the training data using **word length** and **word frequency** as features, and returns your model’s predictions on the training data and the development data individually.
    - **NOTE**: Before training and testing a classifier, it is generally important to normalize your features. This means that you need to find the mean and standard deviation (sd) of a feature. Then, for each row, perform the following transformation: `X_scaled = (X_original - mean)/sd`. **Be sure to always use the means and standard deviations from the training data**.
    - **Optional**: You can include more features if you want to, e.g. [the count of syllabus](https://github.com/prosegrinder/python-syllables)
"""

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB

def get_naive_bayes_threshold_feature_test(train_data, ngram_counts):
    words, y_true = train_data
    features = []
    for word in words:
        ## YOUR CODE HERE
        # 1. word length feature
        # 2. unigram frequency feature
        # 3. (optional) syllables count feature
        length_feature = length_threshold_feature(word, 7)
        unigram_feature = frequency_threshold_feature(word, 15100000, ngram_counts)
        # syllables_feature = syllables_threshold_feature(word, 2)
        # features.append([length_feature, unigram_feature, syllables_feature])
        features.append([length_feature, unigram_feature])

    X = np.asarray(features)
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)

    # TODO: calculate the scaled data, with the mean and std you just calculated
    X_scaled = (X - means) / stds

    from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    clf.fit(X_scaled, y_true)

    train_pred = clf.predict(X_scaled)
    return y_true, train_pred


y_true, train_pred = get_naive_bayes_threshold_feature_test(train_data, ngram_counts)

def get_naive_bayes_threshold_feature(train_data, ngram_counts):
    words, y_true = train_data
    features = []
    for word in words:
        ## YOUR CODE HERE
        # 1. word length feature
        # 2. unigram frequency feature
        # 3. (optional) syllables count feature
        length_feature = length_threshold_feature(word, 7)
        unigram_feature = frequency_threshold_feature(word, 15100000, ngram_counts)
        # syllables_feature = syllables_threshold_feature(word, 7)
        # if syllables_feature is None:
        #   print(word)
        # features.append([length_feature, unigram_feature, syllables_feature])
        features.append([length_feature, unigram_feature])

    X = np.asarray(features)
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)

    # TODO: calculate the scaled data, with the mean and std you just calculated
    X_scaled = (X - means) / stds

    from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    clf.fit(X_scaled, y_true)

    train_pred = clf.predict(X_scaled)
    return train_pred, clf

## Trains a Naive Bayes classifier using length and frequency features
def naive_bayes(train_data, dev_words, ngram_counts):
    # TODO
    # 1. Gets the features from the training data, and trains the model
    # 2. Train model & inference on dev_words
    ## YOUR CODE HERE
    # train_pred, dev_pred = None, None
    train_pred, clf = get_naive_bayes_threshold_feature(train_data, ngram_counts)

    features = []
    for word in dev_words:
        ## YOUR CODE HERE
        # 1. word length feature
        # 2. unigram frequency feature
        # 3. (optional) syllables count feature
        length_feature = length_threshold_feature(word, 9)
        unigram_feature = frequency_threshold_feature(word, 200, ngram_counts)
        # syllables_feature = syllables_threshold_feature(word, 7)
        # features.append([length_feature, unigram_feature, syllables_feature])
        features.append([length_feature, unigram_feature])

    X = np.asarray(features)
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)
    X_scaled = (X - means) / stds

    dev_pred = clf.predict(X_scaled)

    return train_pred, dev_pred

"""- **Answer 3.1:** Please report the precision, recall, and f-score on both the training data and the development data.
    - Training **[writeup.pdf]**
    - Development **[writeup.pdf]**
"""

# report train/development metrics!
dev_words, dev_labels = dev_data
train_pred, dev_pred = naive_bayes(train_data, dev_words, ngram_counts)
dev_precision = get_precision(dev_labels, dev_pred)
dev_recall = get_recall(dev_labels, dev_pred)
dev_f = get_fscore(dev_labels, dev_pred)
dev_f

# added by me to report train set
tmp, train_labels = train_data
train_pred, model = get_naive_bayes_threshold_feature(train_data, ngram_counts)
train_precision = get_precision(train_labels, train_pred)
train_recall = get_recall(train_labels, train_pred)
train_fscore = get_fscore(train_labels, train_pred)

print("train_precision is {}".format(train_precision))
print("train_recall is {}".format(train_recall))
print("train_fscore is {}".format(train_fscore))

# added by me
dev_words, dev_labels = dev_data
nb_train_pred, nb_dev_pred = naive_bayes(train_data, dev_words, ngram_counts)
dev_precision = get_precision(dev_labels, nb_dev_pred)
dev_recall = get_recall(dev_labels, nb_dev_pred)
dev_f = get_fscore(dev_labels, nb_dev_pred)
dev_f

print("dev_precision is {}".format(dev_precision))
print("dev_recall is {}".format(dev_recall))
print("dev_fscore is {}".format(dev_f))

dev_precision

dev_recall

# PennGrader - DO NOT CHANGE
train_pred, mini_test_pred = naive_bayes(train_data, mini_test_words, ngram_counts)
grader.grade(test_case_id = 'test_naive_bayes', answer = mini_test_pred)

"""## 3.2 Logistic Regression
Next, you will use sklearn’s built-in Logistic Regression classifier. Again, we will use word length and word frequency as your two features. You should refer to [the online sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) when you are building your classifier. To import and use this model, use the following command:

```
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()
```

- **Problem 3.2**: For this problem, you will be filling in the function `logistic_regression(train_data, dev_data, ngram_counts)`. This function will train a `Logistic Regression` classifier on the training data, and returns your model’s predictions on the training data and the development data individually.

"""

from sklearn.linear_model import LogisticRegression

def get_logistic_threshold_feature(train_data, ngram_counts):
    words, y_true = train_data
    features = []
    for word in words:
        ## YOUR CODE HERE
        # 1. word length feature
        # 2. unigram frequency feature
        # 3. (optional) syllables count feature
        length_feature = length_threshold_feature(word, 7)
        unigram_feature = frequency_threshold_feature(word, 15100, ngram_counts)
        # syllables_feature = syllables_threshold_feature(word, 7)
        # features.append([length_feature, unigram_feature, syllables_feature])
        features.append([length_feature, unigram_feature])

    X = np.asarray(features)
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)

    # TODO: calculate the scaled data, with the mean and std you just calculated
    X_scaled = (X - means) / stds

    classifier = LogisticRegression()
    classifier.fit(X_scaled, y_true)

    train_pred = classifier.predict(X_scaled)

    return train_pred, classifier

## Trains a logistic regression classifier using length and frequency features
def logistic_regression(train_data, dev_words, ngram_counts):
    ## TODO
    # train_pred, dev_pred = None, None
    train_pred, classifier = get_logistic_threshold_feature(train_data, ngram_counts)

    features = []
    for word in dev_words:
        ## YOUR CODE HERE
        # 1. word length feature
        # 2. unigram frequency feature
        # 3. (optional) syllables count feature
        length_feature = length_threshold_feature(word, 7)
        unigram_feature = frequency_threshold_feature(word, 15100, ngram_counts)
        # syllables_feature = syllables_threshold_feature(word, 7)
        # features.append([length_feature, unigram_feature, syllables_feature])
        features.append([length_feature, unigram_feature])

    X = np.asarray(features)
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)
    X_scaled = (X - means) / stds

    dev_pred = classifier.predict(X_scaled)

    return train_pred, dev_pred

# added by me to report train set
tmp, train_labels = train_data
train_pred, model = get_logistic_threshold_feature(train_data, ngram_counts)
train_precision = get_precision(train_labels, train_pred)
train_recall = get_recall(train_labels, train_pred)
train_fscore = get_fscore(train_labels, train_pred)

print("train_precision is {}".format(train_precision))
print("train_recall is {}".format(train_recall))
print("train_fscore is {}".format(train_fscore))

"""- **Answer 3.2:** Please report the precision, recall, and f-score on both the training data and the development data.
    - Training **[writeup.pdf]**
    - Development **[writeup.pdf]**
"""

dev_words, dev_labels = dev_data
train_pred, dev_pred = logistic_regression(train_data, dev_words, ngram_counts)
dev_precision = get_precision(dev_labels, dev_pred)
dev_recall = get_recall(dev_labels, dev_pred)
dev_f = get_fscore(dev_labels, dev_pred)
dev_f

dev_words, dev_labels = dev_data
lr_train_pred, lr_dev_pred = logistic_regression(train_data, dev_words, ngram_counts)
dev_precision = get_precision(dev_labels, lr_dev_pred)
dev_recall = get_recall(dev_labels, lr_dev_pred)
dev_f = get_fscore(dev_labels, lr_dev_pred)

print("dev_precision is {}".format(dev_precision))
print("dev_recall is {}".format(dev_recall))
print("dev_fscore is {}".format(dev_f))

print(nb_dev_pred == lr_dev_pred)

dev_precision

dev_recall

# PennGrader - DO NOT CHANGE
train_pred, mini_test_pred = logistic_regression(train_data, mini_test_words, ngram_counts)
grader.grade(test_case_id = 'test_logistic', answer = mini_test_pred)

"""## 3.3 Comparing Naive Bayes and Logistic Regression
- **Answer 3.3**: After implementing Naive Bayes and Logistic Regression classifiers, you will notice that their performance is not identical, even though they are given the same data. **Write a paragraph below that discusses which model performed better on this task and what are the potential reasons.**
    - [TODO: discussion of the differences in your writeup.pdf]
"""

## Answer in your writeup.pdf

"""# Section 4. Build your own model
Finally, the fun part! In this section, you will build your own classifier for the complex word identification task, and compare your results to that of your classmates. You will also perform an error analysis for your best performing model.

You can choose any other types of classifier, and any additional features you can think of!
- For classifiers, beyond `Naive Bayes` and `Logistic Regression`, you might consider trying `SVM`, `Decision Trees`, and `Random Forests`, among others.
- Additional word features that you might consider include number of syllables, number of `WordNet` synonyms, and number of `WordNet` senses. To use WordNet in Python, refer to [this documentation](http://www.nltk.org/howto/wordnet.html).
- You could also include sentence-based complexity features, such as length of the sentence, average word length, and average word frequency.

When trying different classifiers, we recommend that you train on training data, and test on the development data, like the previous sections.

In the following cell, please include a description of **all of the models** and **features** that you tried. To receive full credit, you MUST try **at least 1 type of classifier** (not including `Naive Bayes` and `Logistic Regression`), and **at least two features** (not including length and frequency).

**Note**: You can also tune the parameters of your model, e.g. what type of kernel to use. This is NOT required, as some of you may not be that familiar with this.

## 4.1 Your Own Model

An important part of text classification tasks is to determine what your model is getting correct, and what your model is getting wrong. For this problem, you must train your best model on the training data, and report the precision, recall, and f-score on the development data. In order to receive full credit, your model must be able to outperform all of the baselines.
"""

## YOUR CODE HERE
vowel_set = set(['a', 'e', 'o', 'i', 'u'])

def count_vowel_feature(word, threshold):
    count = 0
    for c in word:
      if c in vowel_set:
        count += 1
    return count > threshold  # if count > threshold, the word is complex word

import syllables
def syllables_threshold_feature(word, threshold):
  return syllables.estimate(word) > threshold # if the number of syllables is more less than threshold, treated as complex word

from sklearn.svm import SVC

def get_my_own_feature_set(train_data, vowel_threshold, length_threshold, syllables_threshold):
  words, y_true = train_data
  features = []
  for word in words:
    vowel_feature = count_vowel_feature(word, vowel_threshold)
    # length_feature = length_threshold_feature(word, length_threshold)
    syllables_feature = syllables_threshold_feature(word, syllables_threshold)
    features.append([vowel_feature, syllables_feature])

  X = np.asarray(features)

  means = np.mean(X, axis=0)
  stds = np.std(X, axis=0)

  X_scaled = (X - means) / stds

  Y = np.asarray(y_true)

  svc = SVC()
  svc.fit(X_scaled, y_true)

  train_pred = svc.predict(X_scaled)

  return train_pred, svc


def my_svm_model(train_data, dev_words, vowel_threshold, length_threshold, syllables_threshold):
    ## YOUR CODE HERE
    # TODO: do the similar thing, except using means and stds as given (from training data)
    features = []
    train_pred, svc = get_my_own_feature_set(train_data, vowel_threshold, length_threshold, syllables_threshold)
    for word in dev_words:
      vowel_feature = count_vowel_feature(word, vowel_threshold)
      # length_feature = length_threshold_feature(word, length_threshold)
      syllables_feature = syllables_threshold_feature(word, syllables_threshold)
      features.append([vowel_feature, syllables_feature])

    X = np.asarray(features)
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)
    # print("stds is {}".format(stds))
    X_scaled = (X - means) / stds
    # return X, X_scaled
    dev_pred = svc.predict(X_scaled)
    return train_pred, dev_pred

# test vowel count's performance
vowel_set = set(['a', 'e', 'o', 'i', 'u'])

def count_vowel_feature(word, threshold):
    count = 0
    for c in word:
      if c in vowel_set:
        count += 1
    return count > threshold  # if count > threshold, the word is complex word

## Finds the best length threshold by f-score, and uses this threshold to
## classify the training and development set
def vowel_length_threshold(train_data, dev_words, vowel_length):
    twords, tlabels = train_data

    ## YOUR CODE HERE
    dev_pred = []
    for word in dev_words:
      dev_pred.append(count_vowel_feature(word, vowel_length)) # 2 is the best
    return dev_pred

dev_words, dev_labels = dev_data
y_pred = vowel_length_threshold(train_data, dev_words, vowel_length=2)
dprecision = get_precision(dev_labels, y_pred)
drecall = get_recall(dev_labels, y_pred)
dfscore = get_fscore(dev_labels, y_pred)
print("dprecision is {}".format(dprecision))
print("drecall is {}".format(drecall))
print("dfscore is {}".format(dfscore))

precision_list = []
recall_list= []
fscore_list = []
for i in range(1, 8):
  dev_words, dev_labels = dev_data
  y_pred = vowel_length_threshold(train_data, dev_words, i)
  dprecision = get_precision(dev_labels, y_pred)
  drecall = get_recall(dev_labels, y_pred)
  dfscore = get_fscore(dev_labels, y_pred)
  precision_list.append(dprecision)
  recall_list.append(drecall)
  fscore_list.append(dfscore)

print("fscore_list is {}".format(fscore_list))
import matplotlib.pyplot as plt
plt.scatter(recall_list, precision_list)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Number of Vowel Precision-Recall Curve")
plt.show()
# threshold is 2

# Test syllables feature
# True means complex word
# False means: simple word
def syllables_threshold_feature(word, threshold):
  return syllables.estimate(word) > threshold

def syllables_threshold(train_data, dev_words, syllables_threshold=7):
    twords, tlabels = train_data
    dev_pred = []
    for word in dev_words:
      dev_pred.append(syllables_threshold_feature(word, syllables_threshold))
    return dev_pred


dev_words, dev_labels = dev_data
precision_list = []
recall_list= []
fscore_list = []
for i in range(1, 9):
  y_pred = syllables_threshold(train_data, dev_words, i)

  dprecision = get_precision(dev_labels, y_pred)
  drecall = get_recall(dev_labels, y_pred)
  dfscore = get_fscore(dev_labels, y_pred)
  precision_list.append(dprecision)
  recall_list.append(drecall)
  fscore_list.append(dfscore)

print("fscore_list is {}".format(fscore_list))
import matplotlib.pyplot as plt
plt.scatter(recall_list, precision_list)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Syllables Feature Precision-Recall Curve")
plt.show()
# threshold should be 2

tmp, train_labels = train_data
train_pred, model = get_my_own_feature_set(train_data, vowel_threshold=3, length_threshold=7, syllables_threshold=2)
train_precision = get_precision(train_labels, train_pred)
train_recall = get_recall(train_labels, train_pred)
train_fscore = get_fscore(train_labels, train_pred)
print("train_precision is {}".format(train_precision))
print("train_recall is {}".format(train_recall))
print("train_fscore is {}".format(train_fscore))

dev_words, dev_labels = dev_data
lr_train_pred, lr_dev_pred = my_svm_model(train_data, dev_words, vowel_threshold=2, length_threshold=7, syllables_threshold=2)
dev_precision = get_precision(dev_labels, lr_dev_pred)
dev_recall = get_recall(dev_labels, lr_dev_pred)
dev_f = get_fscore(dev_labels, lr_dev_pred)

print("dev_precision is {}".format(dev_precision))
print("dev_recall is {}".format(dev_recall))
print("dev_fscore is {}".format(dev_f))

# True Positive labled as complex words prediction is complex words
for i in range(0, 100):
  if dev_labels[i] == lr_dev_pred[i] and lr_dev_pred[i] == 1:
    print(dev_words[i], lr_dev_pred[i], dev_labels[i])

# True Negative labled as simple words prediction is simple words
for i in range(0, 100):
  if dev_labels[i] == lr_dev_pred[i] and lr_dev_pred[i] == 0:
    print(dev_words[i], lr_dev_pred[i], dev_labels[i])

# False Simple labled as complex words prediction is simple words
for i in range(0, 100):
  if dev_labels[i] != lr_dev_pred[i] and lr_dev_pred[i] == 0:
    print("word: {} label: {} prediction: {}".format(dev_words[i], dev_labels[i], lr_dev_pred[i]))
    print("number of vowels {} number of syllables {}".format(count_vowel_feature(dev_words[i], 2), syllables.estimate(dev_words[i])))

# False Complex labled as complex words prediction is simple words
for i in range(0, 100):
  if dev_labels[i] != lr_dev_pred[i] and lr_dev_pred[i] == 1:
    print("word: {} label: {} prediction: {}".format(dev_words[i], dev_labels[i], lr_dev_pred[i]))
    print("number of vowels {} number of syllables {}".format(count_vowel_feature(dev_words[i], 2), syllables.estimate(dev_words[i])))

syllables.estimate("coup")

for i in range(0, 500):
  if dev_labels[i] != lr_dev_pred[i] and lr_dev_pred[i] == 1:
    print(dev_words[i], lr_dev_pred[i], dev_labels[i])

"""- **Answer 4.1**: Train your best model on the training data, and report the precision, recall, and f-score on the development data"""

syllables.estimate("patterns")

## Answer in your writeup.pdf
dev_words, dev_labels = dev_data
lr_train_pred, lr_dev_pred = my_logistic_regression(train_data, dev_words, vowel_threshold=2, length_threshold=7, syllables_threshold=3)
dev_precision = get_precision(dev_labels, lr_dev_pred)
dev_recall = get_recall(dev_labels, lr_dev_pred)
dev_f = get_fscore(dev_labels, lr_dev_pred)

print("dev_precision is {}".format(dev_precision))
print("dev_recall is {}".format(dev_recall))
print("dev_fscore is {}".format(dev_f))

"""## 4.2 Analyze your model


Describe the model and features you choose, and perform a detailed error analysis of your models. Give several examples of words on which your best model performs well. Also give examples of words which your best model performs poorly on, and identify at least TWO categories of words on which your model is making errors.

- **Answer 4.2**: Write a detailed description of your model and features used. Also include error analysis of your model.
"""

## Answer in your writeup.pdf
"""
I used logistic regression model with 3 features.
I select 3 features:
1.  The number of vowels in the word: the best value to use is 2 which offers largest fscore for this feature only.
2.  The length of the word: the best value to use is 7, which offers largest fscore for this feature only
3.  The number of syllables of the word: the best value to use is 2, which offers largest fscore for this feature.
For each feature, I tested separately to find the optimized threshold, and then applied the threshold.
I also tested different value for these threshold, but
"""

"""## 4.3 Leaderboard
Finally, use your classifier to predict labels for the test data, and submit these labels in a text file named `test_labels.txt` (with one label per line) to the leaderboard; be sure NOT to shuffle the order of the test examples. Instructions for how to post to the leaderboard will be posted on Ed soon.
**In addition, the top 3 submissions will receive 5 bonus points! A successful submission to the leaderboard will guarantee 2 points!**
"""

# train_pred, test_pred = `your_model_here`(train_data, test_words, ngram_counts)
tmp, train_labels = train_data
train_pred, model = get_my_own_feature_set(train_data, vowel_threshold=3, length_threshold=7, syllables_threshold=2)
train_precision = get_precision(train_labels, train_pred)
train_recall = get_recall(train_labels, train_pred)
train_fscore = get_fscore(train_labels, train_pred)
print("train_precision is {}".format(train_precision))
print("train_recall is {}".format(train_recall))
print("train_fscore is {}".format(train_fscore))

with open('test_labels.txt', 'w') as f:
    f.write("\n".join(map(str, test_pred)))

from sklearn.linear_model import LogisticRegression

def get_my_logistic_threshold_feature(train_data, ngram_counts):
    words, y_true = train_data
    features = []
    for word in words:
        ## YOUR CODE HERE
        # 1. word length feature
        # 2. unigram frequency feature
        # 3. (optional) syllables count feature
        length_feature = length_threshold_feature(word, 7)
        unigram_feature = frequency_threshold_feature(word, 15100, ngram_counts)
        # syllables_feature = syllables_threshold_feature(word, 7)
        # features.append([length_feature, unigram_feature, syllables_feature])
        features.append([length_feature, unigram_feature])

    X = np.asarray(features)
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)

    # TODO: calculate the scaled data, with the mean and std you just calculated
    X_scaled = (X - means) / stds

    classifier = LogisticRegression()
    classifier.fit(X_scaled, y_true)

    train_pred = classifier.predict(X_scaled)

    return train_pred, classifier

## Trains a logistic regression classifier using length and frequency features
def my_logistic_regression(train_data, dev_words, ngram_counts):
    ## TODO
    # train_pred, dev_pred = None, None
    train_pred, classifier = get_my_logistic_threshold_feature(train_data, ngram_counts)

    features = []
    for word in dev_words:
        ## YOUR CODE HERE
        # 1. word length feature
        # 2. unigram frequency feature
        # 3. (optional) syllables count feature
        length_feature = length_threshold_feature(word, 7)
        unigram_feature = frequency_threshold_feature(word, 15100, ngram_counts)
        # syllables_feature = syllables_threshold_feature(word, 7)
        # features.append([length_feature, unigram_feature, syllables_feature])
        features.append([length_feature, unigram_feature])

    X = np.asarray(features)
    means = np.mean(X, axis=0)
    stds = np.std(X, axis=0)
    X_scaled = (X - means) / stds

    dev_pred = classifier.predict(X_scaled)

    return train_pred, dev_pred

tmp, train_labels = train_data
train_pred, my_logistic_model = get_my_logistic_threshold_feature(train_data, ngram_counts)
train_precision = get_precision(train_labels, train_pred)
train_recall = get_recall(train_labels, train_pred)
train_fscore = get_fscore(train_labels, train_pred)

print("train_precision is {}".format(train_precision))
print("train_recall is {}".format(train_recall))
print("train_fscore is {}".format(train_fscore))

train_pred, test_pred = my_logistic_regression(train_data, test_words, ngram_counts)
with open('test_labels.txt', 'w') as f:
    f.write("\n".join(map(str, test_pred)))

"""- **Results 4.3**: Upload your `test_labels.txt` to the leaderboard in Gradescope.

## 4.4. (Optional) Leaderboard using outside data

While the training data we have provided is sufficient for completing this assignment, it is not the only data for the task of identifying complex words. As an optional addition to this homework, you may look for and use any additional training data, and submit your predicted labels in a text file named `test_labels.txt` to the leaderboard.

As a start, we recommend looking at the [SemEval 2016 dataset](http://alt.qcri.org/semeval2016/task11/), a dataset that was used in a complex words identification competition. In addition, you can try to use data from [Newsela](https://newsela.com/). Newsela’s editors re-write newspaper articles to be appropriate for students at different grade levels. The company has generously shared a dataset with us. The Newsela data **may not** be re-distributed outside of Penn. You can find the data on eniac at `/home1/c/ccb/data/newsela/newsela_article_corpus_with_scripts_2016-01-29.1.zip`.

Good luck, and have fun!
"""







"""# Submission
Here are the deliverables you need to submit to GradeScope:
- Write-up:
    - Answers to all questions labeled as `Answer #.#` in a file named `writeup.pdf`
- Code:
    - This notebook and py file: rename to `homework2.ipynb` and `homework2.py`. You can download the notebook and py file by going to the top-left corner of this webpage, `File -> Download -> Download .ipynb/.py`
- Leaderboard Results:
  - Your model’s output for the test set using only the provided training and development data (`test_labels.txt`)
  - (Optional) your model’s output for the test set, using any data that you want. (`test_labels.txt`)
"""

