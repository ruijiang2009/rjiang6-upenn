{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 5: Advanced Vector Space Models\n",
        "\n",
        "## Due Date: Jun 17\n",
        "## Total Points: 116 points + 8 bonus\n",
        "- **Overview**: In this assignment, we will examine some advanced uses of vector representations of words. We are going to look at three different problems:\n",
        "\n",
        "  - Solving word relation problems like analogies using word embeddings.\n",
        "  - Comparing correlation for human judgments of similarity to the vector similarities\n",
        "  - Discovering the different senses of a ‘polysemous’ word by clustering together its paraphrases.\n",
        "\n",
        "\n",
        "- **Delieverables:** This assignment has several deliverables:\n",
        "  - Code (this notebook) *(Automatic Graded)*\n",
        "    - Part 1: answers to questions\n",
        "    - Part 3: 4 different clustering functions\n",
        "  - Write Up (include in this notebook or a separate **writeup.pdf**) *(Manually Graded)*\n",
        "    - Answers to all questions labeled as `Answer #.#` in a file named `writeup.pdf`\n",
        "      - Part 2: answers to questions **[writeup.pdf]**\n",
        "      - Part 3: F-scores for clustering algorithms & discussions about your models **[writeup.pdf]**\n",
        "  - Leaderboard Without K *(Automatic Graded on GradeScope)*\n",
        "    - `test_nok_output_leaderboard.txt` = Task 3.4 output file\n",
        "  - Leaderboard With K *(Automatic Graded on GradeScope)*\n",
        "    - `test_output_leaderboard.txt` = Task 3.2 or 3.3 output file\n",
        "\n",
        "- **Grading**: We will use the auto-grading system called `PennGrader`. To complete the homework assignment, you should implement anything marked with `#TODO` and run the cell with `#PennGrader` note.\n",
        "\n",
        "\n",
        "## Recommended Readings\n",
        "- [Vector Semantics](https://web.stanford.edu/~jurafsky/slp3/6.pdf). Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd edition draft).\n",
        "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf?). Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. ArXiV 2013.\n",
        "- [Linguistic Regularities in Continuous Space Word Representations](https://www.aclweb.org/anthology/N13-1090). Tomas Mikolov, Wen-tau Yih, Geoffrey Zweig. NAACL 2013.\n",
        "- [Discovering Word Senses from Text](https://cs.uwaterloo.ca/~cdimarco/pdf/cs886/Pantel+Lin02.pdf). Patrick Pangel and Dekang Ling. KDD 2002.\n",
        "- [Linguistic Regularities in Sparse and Explicit Word Representations](https://aclanthology.org/W14-1618.pdf). Patrick Pangel and Dekang Ling. CoNLL 2014.\n",
        "- [Clustering Paraphrases by Word Sense](https://www.cis.upenn.edu/~ccb/publications/clustering-paraphrases-by-word-sense.pdf). Anne Cocos and Chris Callison-Burch. NAACL 2016.\n",
        "\n",
        "## To get started, **make a copy** of this colab notebook into your google drive!"
      ],
      "metadata": {
        "id": "mb4Yr8KhRhbe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDv9qN5c9357"
      },
      "source": [
        "## Setup 1: PennGrader Setup [4 points]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9FG2uNoU4nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT CHANGE ANYTHING, JUST RUN\n",
        "%%capture\n",
        "!pip install penngrader-client"
      ],
      "metadata": {
        "id": "jpSjR2N19kk4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "FvPA8Z2D9ki_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af14bb3b-c474-4152-aba7-bef807bbc329"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing notebook-config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat notebook-config.yaml"
      ],
      "metadata": {
        "id": "xbeXT4Oj9kg_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f87ba8-96de-4545-80cb-7fa4490dbe0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
            "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from penngrader.grader import *\n",
        "\n",
        "## TODO - Start\n",
        "STUDENT_ID = 62502470 # YOUR PENN-ID GOES HERE AS AN INTEGER#\n",
        "## TODO - End\n",
        "\n",
        "SECRET = STUDENT_ID\n",
        "grader = PennGrader('notebook-config.yaml', 'CIS5300_OL_23Su_HW5', STUDENT_ID, SECRET)"
      ],
      "metadata": {
        "id": "OceP0Hr-9kfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a39cb0-0807-4040-c202-efae20624dbc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PennGrader initialized with Student ID: 62502470\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the PennGrader is set up correctly\n",
        "# do not chance this cell, see if you get 4/4!\n",
        "name_str = 'Rui Jiang'\n",
        "grader.grade(test_case_id = 'name_test', answer = name_str)"
      ],
      "metadata": {
        "id": "k5lAgKaYa-uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8652e17a-7cc3-4571-f48f-54a97cbaf9ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 2: Dataset / Packages\n",
        "- **Run the following cells without changing anything!**"
      ],
      "metadata": {
        "id": "CHrdyiQdBt2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### This cell might take 3 min to run ###\n",
        "! echo \"Installing Magnitude.... (please wait, can take a while)\"\n",
        "! (curl https://raw.githubusercontent.com/plasticityai/magnitude/master/install-colab.sh | /bin/bash 1>/dev/null 2>/dev/null)\n",
        "! echo \"Done installing Magnitude.\""
      ],
      "metadata": {
        "id": "lNBeJLVcSusU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "acf9fea0-e783-4356-80dc-518aef556b18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Magnitude.... (please wait, can take a while)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   137  100   137    0     0    521      0 --:--:-- --:--:-- --:--:--   520\n",
            "Done installing Magnitude.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1luyNlDu0GdH_B3D6rjYLKIGGfz_S3yU5 # GoogleNews-vectors-negative300.filter.magnitude\n",
        "!gdown 17a4uC7eNrYdtVlW60wshjyDLcFxTtq0y # SimLex-999.txt\n",
        "!gdown 1h2DHMuubO7OEVxmGQGbvb2Ovj_A6hakC # dev_input.txt\n",
        "!gdown 1I83_VA_i_UB-9cf9GcEe5oGPoc8-ZmLh # dev_output.txt\n",
        "!gdown 1CjK3eYkacyxo3gdLbf9IGdk1DFEfAYvM # test_input.txt\n",
        "!gdown 1sZuq8a2zHJfe6bLjrK3wD2jrZWkQ0-6S # test_nok_input.txt\n",
        "!gdown 1gK13ZVDMA5XYi8sZY8G1gOIZMdxGTuay # coocvec-500mostfreq-window-3.filter.magnitude\n",
        "\n",
        "!gdown 1r0ebRDG-_4ALl3PJ7Vko0DkLcMdLPIoL # glove.6B.50d.magnitude\n",
        "!gdown 1TQ5W7mma_fYKqVL-Dm7_ogwIftyJpXAT # glove.6B.100d.magnitude\n",
        "!gdown 1LiKprfuwD434FGC-bf8OARMIKCtNIL4Z # glove.6B.200d.magnitude\n",
        "!gdown 1_p-9y15JvbobeJ37L5v4kXnWMXsfHsD4 # glove.6B.300d.magnitude\n",
        "!gdown 1zs0Z-m7YbbVbKvqkq-HEIxNYp3e75-7e # glove.840B.300d.magnitude\n",
        "\n",
        "# if the above wget command gives you an error, then uncomment the line below and run this cell\n",
        "!gdown 115ryZ01s_guR1ySc7YLD2kbAm6UpL7VP # GoogleNews-vectors-negative300.magnitude"
      ],
      "metadata": {
        "id": "lVZjUMHvpUBa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "dbd47d11-6c75-4653-badb-afba1aa39ce5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1luyNlDu0GdH_B3D6rjYLKIGGfz_S3yU5\n",
            "To: /content/GoogleNews-vectors-negative300.filter.magnitude\n",
            "100% 3.99M/3.99M [00:00<00:00, 32.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17a4uC7eNrYdtVlW60wshjyDLcFxTtq0y\n",
            "To: /content/SimLex-999.txt\n",
            "100% 43.0k/43.0k [00:00<00:00, 4.02MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h2DHMuubO7OEVxmGQGbvb2Ovj_A6hakC\n",
            "To: /content/dev_input.txt\n",
            "100% 17.4k/17.4k [00:00<00:00, 36.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I83_VA_i_UB-9cf9GcEe5oGPoc8-ZmLh\n",
            "To: /content/dev_output.txt\n",
            "100% 23.1k/23.1k [00:00<00:00, 41.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CjK3eYkacyxo3gdLbf9IGdk1DFEfAYvM\n",
            "To: /content/test_input.txt\n",
            "100% 3.81k/3.81k [00:00<00:00, 16.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sZuq8a2zHJfe6bLjrK3wD2jrZWkQ0-6S\n",
            "To: /content/test_nok_input.txt\n",
            "100% 4.55k/4.55k [00:00<00:00, 16.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gK13ZVDMA5XYi8sZY8G1gOIZMdxGTuay\n",
            "To: /content/coocvec-500mostfreq-window-3.filter.magnitude\n",
            "100% 3.50M/3.50M [00:00<00:00, 21.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1r0ebRDG-_4ALl3PJ7Vko0DkLcMdLPIoL\n",
            "From (redirected): https://drive.google.com/uc?id=1r0ebRDG-_4ALl3PJ7Vko0DkLcMdLPIoL&confirm=t&uuid=7a8a945a-57dc-4e94-9af2-3c4c567bbd1e\n",
            "To: /content/glove.6B.50d.magnitude\n",
            "100% 211M/211M [00:03<00:00, 69.3MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TQ5W7mma_fYKqVL-Dm7_ogwIftyJpXAT\n",
            "From (redirected): https://drive.google.com/uc?id=1TQ5W7mma_fYKqVL-Dm7_ogwIftyJpXAT&confirm=t&uuid=a5b901d3-24d5-44de-bf49-c9c2187b91e6\n",
            "To: /content/glove.6B.100d.magnitude\n",
            "100% 302M/302M [00:04<00:00, 75.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1LiKprfuwD434FGC-bf8OARMIKCtNIL4Z\n",
            "From (redirected): https://drive.google.com/uc?id=1LiKprfuwD434FGC-bf8OARMIKCtNIL4Z&confirm=t&uuid=cda2fa31-82f0-49d5-8ed0-ec2e137706dc\n",
            "To: /content/glove.6B.200d.magnitude\n",
            "100% 507M/507M [00:05<00:00, 89.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_p-9y15JvbobeJ37L5v4kXnWMXsfHsD4\n",
            "From (redirected): https://drive.google.com/uc?id=1_p-9y15JvbobeJ37L5v4kXnWMXsfHsD4&confirm=t&uuid=95351c8d-bd31-4d56-abae-d6a8e747319c\n",
            "To: /content/glove.6B.300d.magnitude\n",
            "100% 667M/667M [00:07<00:00, 84.2MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zs0Z-m7YbbVbKvqkq-HEIxNYp3e75-7e\n",
            "From (redirected): https://drive.google.com/uc?id=1zs0Z-m7YbbVbKvqkq-HEIxNYp3e75-7e&confirm=t&uuid=8ca9671e-eecf-41e3-9e3a-b9dd8ebd7c41\n",
            "To: /content/glove.840B.300d.magnitude\n",
            "100% 3.65G/3.65G [00:45<00:00, 79.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=115ryZ01s_guR1ySc7YLD2kbAm6UpL7VP\n",
            "From (redirected): https://drive.google.com/uc?id=115ryZ01s_guR1ySc7YLD2kbAm6UpL7VP&confirm=t&uuid=eac77071-d98b-4c04-848d-f1fdc3abc74d\n",
            "To: /content/GoogleNews-vectors-negative300.magnitude\n",
            "100% 4.21G/4.21G [00:36<00:00, 114MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en"
      ],
      "metadata": {
        "id": "nMLKUtdbw-qI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "109cc13d-d883-4010-c4c9-1881e40cf28e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/annoy-1.17.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/lz4-4.4.4-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pymagnitude-0.1.143-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/fasteners-0.19-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/annoy-1.17.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/lz4-4.4.4-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pymagnitude-0.1.143-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/fasteners-0.19-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/annoy-1.17.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/lz4-4.4.4-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pymagnitude-0.1.143-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/fasteners-0.19-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y annoy\n",
        "!pip install annoy"
      ],
      "metadata": {
        "id": "LNP6g5CqwGBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8bd25e19-d520-43f6-c473-213dcaec07e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/annoy-1.17.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: annoy 1.17.3\n",
            "Uninstalling annoy-1.17.3:\n",
            "  Successfully uninstalled annoy-1.17.3\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/lz4-4.4.4-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pymagnitude-0.1.143-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/fasteners-0.19-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp311-cp311-linux_x86_64.whl size=553319 sha256=d5930eb82d200b5a50a36e21ed06854ea61e8703954d2f2192763f9ea5f62857\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/e5/58/0a3e34b92bedf09b4c57e37a63ff395ade6f6c1099ba59877c\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U lz4==1.0.0\n",
        "!pip install -U xxhash==1.0.1\n",
        "!pip install -U fasteners==0.14.1"
      ],
      "metadata": {
        "id": "_E3grhSU2tNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a9b453d2-85f1-4a7a-c524-3603b0d3b672"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/lz4-4.4.4-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pymagnitude-0.1.143-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/fasteners-0.19-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting lz4==1.0.0\n",
            "  Downloading lz4-1.0.0.tar.gz (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecation (from lz4==1.0.0)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation->lz4==1.0.0) (24.2)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: lz4\n",
            "  Building wheel for lz4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lz4: filename=lz4-1.0.0-cp311-cp311-linux_x86_64.whl size=843198 sha256=508e66cb47221fbc4d36d2001b5fc98f4ddf87988eae277c209d5127ecf5b856\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/90/51/0e5bd244cfa0118f0ccc4fae0b833c4395ac07a6d6e2b61bc4\n",
            "Successfully built lz4\n",
            "Installing collected packages: deprecation, lz4\n",
            "  Attempting uninstall: lz4\n",
            "    Found existing installation: lz4 4.4.4\n",
            "    Uninstalling lz4-4.4.4:\n",
            "      Successfully uninstalled lz4-4.4.4\n",
            "Successfully installed deprecation-2.1.0 lz4-1.0.0\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pymagnitude-0.1.143-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/fasteners-0.19-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting xxhash==1.0.1\n",
            "  Downloading xxhash-1.0.1.zip (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: xxhash\n",
            "  Building wheel for xxhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xxhash: filename=xxhash-1.0.1-cp311-cp311-linux_x86_64.whl size=38730 sha256=19572259c75a6b75767bbf21ac1b9bafa48f9854eecb3abe9924976fc4e42932\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/6c/0b/3d87fd78c31893e3f7e85187f2163309f8030d0cf130d3b82b\n",
            "Successfully built xxhash\n",
            "Installing collected packages: xxhash\n",
            "  Attempting uninstall: xxhash\n",
            "    Found existing installation: xxhash 3.5.0\n",
            "    Uninstalling xxhash-3.5.0:\n",
            "      Successfully uninstalled xxhash-3.5.0\n",
            "Successfully installed xxhash-1.0.1\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pymagnitude-0.1.143-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/fasteners-0.19-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting fasteners==0.14.1\n",
            "  Downloading fasteners-0.14.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from fasteners==0.14.1) (1.17.0)\n",
            "Collecting monotonic>=0.1 (from fasteners==0.14.1)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading fasteners-0.14.1-py2.py3-none-any.whl (20 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: monotonic, fasteners\n",
            "  Attempting uninstall: fasteners\n",
            "    Found existing installation: fasteners 0.19\n",
            "    Uninstalling fasteners-0.19:\n",
            "      Successfully uninstalled fasteners-0.19\n",
            "Successfully installed fasteners-0.14.1 monotonic-1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "Gn7PUzzIPP3X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your python version\n",
        "!python --version"
      ],
      "metadata": {
        "id": "NzS2Sg3HPjyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e666506c-61e7-41ec-daa9-99aca7f9c14f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your python version is >= 3.9, run the code cell below before importing from pymaginitude:"
      ],
      "metadata": {
        "id": "ZP9pZS2OPhQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.load('en_core_web_sm')\n",
        "import collections\n",
        "collections.Sequence = collections.abc.Sequence\n",
        "collections.Mapping = collections.abc.Mapping\n",
        "collections.MutableMapping = collections.abc.MutableMapping\n",
        "collections.Iterable = collections.abc.Iterable\n",
        "collections.MutableSet = collections.abc.MutableSet\n",
        "collections.Callable = collections.abc.Callable"
      ],
      "metadata": {
        "id": "HAdpIjBxQItc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymagnitude import * # if you encounter an error for this line, try re-running it - I know it's silly but it might work"
      ],
      "metadata": {
        "id": "AVMUXMsTVt4l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this might take ~2min to run\n",
        "!wget http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude"
      ],
      "metadata": {
        "id": "7BtMn0op0ehZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac58308a-7ebe-4a05-c181-1a2bd018f1d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-14 22:25:40--  http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude\n",
            "Resolving magnitude.plasticity.ai (magnitude.plasticity.ai)... 16.182.34.45, 52.217.115.253, 54.231.139.141, ...\n",
            "Connecting to magnitude.plasticity.ai (magnitude.plasticity.ai)|16.182.34.45|:80... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2025-06-14 22:25:40 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if the above wget command gives you an error, then uncomment the line below and run this cell\n",
        "!gdown 115ryZ01s_guR1ySc7YLD2kbAm6UpL7VP"
      ],
      "metadata": {
        "id": "LzimHy7h2tEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99320baf-1e81-4ce2-c9d2-8ba4571beeab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=115ryZ01s_guR1ySc7YLD2kbAm6UpL7VP\n",
            "From (redirected): https://drive.google.com/uc?id=115ryZ01s_guR1ySc7YLD2kbAm6UpL7VP&confirm=t&uuid=5c56bb4c-a7a8-4df2-a953-749b3c294125\n",
            "To: /content/GoogleNews-vectors-negative300.magnitude\n",
            "100% 4.21G/4.21G [00:56<00:00, 74.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !curl -L \"https://drive.usercontent.google.com/download?id=115ryZ01s_guR1ySc7YLD2kbAm6UpL7VP&export=download&confirm\" -o GoogleNews-vectors-negative300.magnitude"
      ],
      "metadata": {
        "id": "oy3BIRN2f5iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('./')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYX147SyEtud",
        "outputId": "05ed24ac-70a2-4b9e-efaa-ff4f3969e4a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'test_input.txt',\n",
              " 'dev_output.txt',\n",
              " 'notebook-config.yaml',\n",
              " 'SimLex-999.txt',\n",
              " 'GoogleNews-vectors-negative300.magnitude',\n",
              " 'glove.6B.100d.magnitude',\n",
              " 'glove.6B.300d.magnitude',\n",
              " 'glove.840B.300d.magnitude',\n",
              " 'glove.6B.200d.magnitude',\n",
              " 'GoogleNews-vectors-negative300.filter.magnitude',\n",
              " 'test_nok_input.txt',\n",
              " 'dev_input.txt',\n",
              " 'coocvec-500mostfreq-window-3.filter.magnitude',\n",
              " 'glove.6B.50d.magnitude',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.cluster import KMeans\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "aiDQlu759Zee"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first time run will take 10 minutes\n",
        "file_path = \"/content/GoogleNews-vectors-negative300.magnitude\"\n",
        "vectors = Magnitude(file_path)\n",
        "\n",
        "sims = vectors.most_similar(\"picnic\")\n"
      ],
      "metadata": {
        "id": "vmgNqpVPp1Gu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('./')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utHVUhUTjM7S",
        "outputId": "3e8dfd32-c8ea-4a27-a9cd-5c5373d1ece9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'test_input.txt',\n",
              " 'dev_output.txt',\n",
              " 'notebook-config.yaml',\n",
              " 'SimLex-999.txt',\n",
              " 'GoogleNews-vectors-negative300.magnitude',\n",
              " 'glove.6B.100d.magnitude',\n",
              " 'glove.6B.300d.magnitude',\n",
              " 'glove.840B.300d.magnitude',\n",
              " 'glove.6B.200d.magnitude',\n",
              " 'GoogleNews-vectors-negative300.filter.magnitude',\n",
              " 'test_nok_input.txt',\n",
              " 'dev_input.txt',\n",
              " 'coocvec-500mostfreq-window-3.filter.magnitude',\n",
              " 'glove.6B.50d.magnitude',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 1luyNlDu0GdH_B3D6rjYLKIGGfz_S3yU5 # GoogleNews-vectors-negative300.filter.magnitude\n",
        "!gdown 17a4uC7eNrYdtVlW60wshjyDLcFxTtq0y # SimLex-999.txt\n",
        "!gdown 1h2DHMuubO7OEVxmGQGbvb2Ovj_A6hakC # dev_input.txt\n",
        "!gdown 1I83_VA_i_UB-9cf9GcEe5oGPoc8-ZmLh # dev_output.txt\n",
        "!gdown 1CjK3eYkacyxo3gdLbf9IGdk1DFEfAYvM # test_input.txt\n",
        "!gdown 1sZuq8a2zHJfe6bLjrK3wD2jrZWkQ0-6S # test_nok_input.txt\n",
        "!gdown 1gK13ZVDMA5XYi8sZY8G1gOIZMdxGTuay # coocvec-500mostfreq-window-3.filter.magnitude"
      ],
      "metadata": {
        "id": "GapecdgJoQhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbce0714-479a-4a75-e42d-875202354e2b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17a4uC7eNrYdtVlW60wshjyDLcFxTtq0y\n",
            "To: /content/SimLex-999.txt\n",
            "100% 43.0k/43.0k [00:00<00:00, 67.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h2DHMuubO7OEVxmGQGbvb2Ovj_A6hakC\n",
            "To: /content/dev_input.txt\n",
            "100% 17.4k/17.4k [00:00<00:00, 38.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I83_VA_i_UB-9cf9GcEe5oGPoc8-ZmLh\n",
            "To: /content/dev_output.txt\n",
            "100% 23.1k/23.1k [00:00<00:00, 42.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CjK3eYkacyxo3gdLbf9IGdk1DFEfAYvM\n",
            "To: /content/test_input.txt\n",
            "100% 3.81k/3.81k [00:00<00:00, 8.44MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sZuq8a2zHJfe6bLjrK3wD2jrZWkQ0-6S\n",
            "To: /content/test_nok_input.txt\n",
            "100% 4.55k/4.55k [00:00<00:00, 13.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gK13ZVDMA5XYi8sZY8G1gOIZMdxGTuay\n",
            "To: /content/coocvec-500mostfreq-window-3.filter.magnitude\n",
            "100% 3.50M/3.50M [00:00<00:00, 24.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ka57dwlDjL9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Exploring Analogies and Other Word Pair Relationships [4 points]\n",
        "**Background:** Word2vec is a very cool word embedding method that was developed by [Thomas Mikolov et al](https://aclanthology.org/N13-1090/). One of the noteworthy things about the method is that it can be used to solve word analogy problems like:\n",
        "\n",
        "***man is to king as woman is to [blank]***\n",
        "\n",
        "The way that it they take the vectors representing king, man and woman and perform some vector arithmetic to produce a vector that is close to the expected answer:\n",
        "\n",
        "***king − man + woman ≈ queen***\n",
        "\n",
        "We can find the nearest vector in the vocabulary by looking for argmax *cos(x, king − man + woman)*. Omar Levy has an explanation of the method in this [Quora post](https://www.quora.com/unanswered/How-does-Mikolovs-word-analogy-for-word-embedding-work-How-can-I-code-such-a-function) and in the [paper](https://aclanthology.org/W14-1618/).\n",
        "\n",
        "In addition to solving this sort of analogy problem, the same sort of vector arithmetic was used with word2vec embeddings to find relationships between pairs of words like the following:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1_ewkcJ6EQMuIK0SrgBulzK7LFi8kD9nD'>\n",
        "\n",
        "In the first part of the assigment, you will play around with the [Magnitude](https://github.com/plasticityai/magnitude) library. You will use Magnitude to load a vector model trained using word2vec, and use it to manipulate and analyze the vectors. In order to proceed further, you need to use the Medium Google-word2vec embedding model trained on Google News by using file `GoogleNews-vectors-negative300.magnitude`. Once the file is downloaded use the following Python commands:"
      ],
      "metadata": {
        "id": "PGlvLsC9B-gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file_path = \"/content/GoogleNews-vectors-negative300.filter.magnitude\"\n",
        "file_path = \"/content/GoogleNews-vectors-negative300.magnitude\"\n",
        "vectors = Magnitude(file_path)"
      ],
      "metadata": {
        "id": "YcILHclcou0L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can use vectors to perform queries. For instance, you can query the distance of cat and dog in the following way:"
      ],
      "metadata": {
        "id": "RJKLr2kjatM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectors.distance(\"cat\", \"dog\")) # should be ~0.69"
      ],
      "metadata": {
        "id": "L-XKt2tpZzi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e33c029-91b0-4b31-a32b-6edda2eaf639"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.69145405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The questions below are designed to familiarize you with the Magnitude word2vec package and get you thinking about what type of semantic information word embeddings can encode. We recommend reading using the [library section](https://github.com/plasticityai/magnitude#using-the-library) to reply to the following set of questions:"
      ],
      "metadata": {
        "id": "Q41x7bVXbLY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 1.1:** What is the dimensionality of these word embeddings? Provide an integer answer. [1 point]"
      ],
      "metadata": {
        "id": "TUnnuRJfbXf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors.dim"
      ],
      "metadata": {
        "id": "d7qnNOEPEtfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3744b329-c7bc-428e-ac3f-8d5b45da7ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "dimensionality = vectors.dim\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q11_dim', answer = dimensionality) # we only check partial data"
      ],
      "metadata": {
        "id": "hVUR92UPbSkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad73f0d-9e91-428c-c133-784f60bf9e37"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Problem 1.2:** What are the top-5 most similar words to `picnic` (not including `picnic` itself)? (Hint: try using `vectors.most_similar`) Please return these as a list of strings named `mostsim`. [1 point]"
      ],
      "metadata": {
        "id": "U-GAJhyvbido"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### The first time you run \"vectors.most_similar\" it will take about 5~10 mins to run\n",
        "sims = vectors.most_similar(\"picnic\")\n",
        "sims_top5 = []\n",
        "for i in range(5):\n",
        "    sims_top5.append(sims[i][0])\n",
        "sims_top5"
      ],
      "metadata": {
        "id": "ZIiW59W-EvjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd5dd52-e19a-4aaa-be18-1d8dd6ff78db"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['picnics', 'picnic_lunch', 'Picnic', 'potluck_picnic', 'picnic_supper']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "mostsim = sims_top5\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "# reload_grader()\n",
        "grader.grade(test_case_id = 'test_q12_picnic', answer = mostsim) # we only check partial data"
      ],
      "metadata": {
        "id": "pTANEQk9bpa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea2d294-11d8-4d53-e40a-a7eb5feb1adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Problem 1.3:** According to the word embeddings, which of these words is not like the others? `['tissue', 'papyrus', 'manila', 'newsprint', 'parchment', 'gazette']` [1 point]"
      ],
      "metadata": {
        "id": "2cuv5HzCbpzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = ['tissue', 'papyrus', 'manila', 'newsprint', 'parchment', 'gazette']\n",
        "for i in range(len(word_list)):\n",
        "    for j in range(i):\n",
        "        print(\"[{}] and [{}] sim scores is [{}]\".format(word_list[i], word_list[j], vectors.similarity(word_list[i], word_list[j])))"
      ],
      "metadata": {
        "id": "I7cAS-RsEwYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54faf2bd-c09f-4ef5-ea80-6ffb76795e42"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[papyrus] and [tissue] sim scores is [0.1519498974084854]\n",
            "[manila] and [tissue] sim scores is [0.13553906977176666]\n",
            "[manila] and [papyrus] sim scores is [0.2507871985435486]\n",
            "[newsprint] and [tissue] sim scores is [0.19044463336467743]\n",
            "[newsprint] and [papyrus] sim scores is [0.2438289374113083]\n",
            "[newsprint] and [manila] sim scores is [0.21090111136436462]\n",
            "[parchment] and [tissue] sim scores is [0.20001494884490967]\n",
            "[parchment] and [papyrus] sim scores is [0.5869774222373962]\n",
            "[parchment] and [manila] sim scores is [0.2911486029624939]\n",
            "[parchment] and [newsprint] sim scores is [0.3022960424423218]\n",
            "[gazette] and [tissue] sim scores is [-0.00041741711902432144]\n",
            "[gazette] and [papyrus] sim scores is [0.22188228368759155]\n",
            "[gazette] and [manila] sim scores is [0.2529136538505554]\n",
            "[gazette] and [newsprint] sim scores is [0.20029675960540771]\n",
            "[gazette] and [parchment] sim scores is [0.18534383177757263]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "doesnt_match = \"tissue\"\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "# reload_grader()\n",
        "grader.grade(test_case_id = 'test_q13_does_not_match', answer = doesnt_match) # we only check partial data"
      ],
      "metadata": {
        "id": "eB3WOnVKbv_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5eea0c-c0cf-4847-af3f-4b3acdf4fc36"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " -  **Problem 1.4:** Solve the following analogy: `leg` is to `jump` as X is to `throw` [1 point]"
      ],
      "metadata": {
        "id": "cjol744PbwIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors.most_similar(positive = [\"leg\", \"throw\"], negative = [\"jump\"])"
      ],
      "metadata": {
        "id": "bbpDYTjgExZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f28ed0c-adb9-45d7-ff0b-561284c9eed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('forearm', np.float32(0.48294652)),\n",
              " ('shin', np.float32(0.47376162)),\n",
              " ('elbow', np.float32(0.4679689)),\n",
              " ('metacarpal_bone', np.float32(0.46781474)),\n",
              " ('metacarpal_bones', np.float32(0.46605822)),\n",
              " ('ankle', np.float32(0.46434426)),\n",
              " ('shoulder', np.float32(0.46183354)),\n",
              " ('thigh', np.float32(0.45393682)),\n",
              " ('knee', np.float32(0.4455708)),\n",
              " ('ulna_bone', np.float32(0.4423491))]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "analogy = \"forearm\"\n",
        "\n",
        "# PennGrader - DO NOT CHANGE\n",
        "# reload_grader()\n",
        "grader.grade(test_case_id = 'test_q14_analogy', answer = analogy)"
      ],
      "metadata": {
        "id": "guWIM6eLb2jA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76bfe686-28e7-467e-9ae1-15178feb23df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: SimLex-999 Dataset Revisited [10 points + 5 Bonus]\n",
        "Let us revisit [SimLex-999](https://fh295.github.io/simlex.html) dataset from Extra Credit in HW4. We will use `SimLex-999.txt`.\n",
        "\n",
        "We provided you a script below that:\n",
        "\n",
        "1. Takes `word1`, `word2`, and `SimLex` columns from the `SimLex-999.txt` dataset,\n",
        "2. Computes the similarity between `word1` and `word2` using `GoogleNews-vectors-negative300.magnitude` from Part 1\n",
        "3. Displays correlation for human judgments of similarity to the vector similarities using [Kendall’s Tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient)."
      ],
      "metadata": {
        "id": "xfGRGgLCc38j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference Code - DO NOT CHANGE\n",
        "vectors = Magnitude(\"/content/GoogleNews-vectors-negative300.magnitude\")\n",
        "df = pd.read_csv('/content/SimLex-999.txt', sep='\\t')[['word1', 'word2', 'SimLex999']]\n",
        "human_scores = []\n",
        "vector_scores = []\n",
        "\n",
        "counter = 0\n",
        "for word1, word2, score in df.values.tolist():\n",
        "    human_scores.append(score)\n",
        "    similarity_score = vectors.similarity(word1, word2)\n",
        "    vector_scores.append(similarity_score)\n",
        "    if counter < 5: # only print the first five\n",
        "        print(f'{word1},{word2},{score},{similarity_score:.4f}')\n",
        "        counter += 1\n",
        "\n",
        "print()\n",
        "correlation, p_value = stats.kendalltau(human_scores, vector_scores)\n",
        "print(f'Correlation = {correlation}, P Value = {p_value}')"
      ],
      "metadata": {
        "id": "HC0Rc41Hdikw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a94b88d-4bf9-4e64-b881-9a0042a74b65"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "old,new,1.58,0.2228\n",
            "smart,intelligent,9.2,0.6495\n",
            "hard,difficult,8.77,0.6026\n",
            "happy,cheerful,9.55,0.3838\n",
            "hard,easy,0.95,0.4710\n",
            "\n",
            "Correlation = 0.30913428432001067, P Value = 2.6592796177776212e-48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of the assignment we would like for you to explore how the Kendall’s Tau correlation changes based on the similarity. You may use the script we provided or create your own script.\n",
        "\n",
        "**Please respond to the following questions in your report\n",
        "\n",
        "Note: **5 Extra points** will be awarded for creativity and a more thorough qualitative analysis.)"
      ],
      "metadata": {
        "id": "CJ2LrGtYc9AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Answer 2.1:** What is the least similar 2 pairs of words based on human judgement scores and vector similarity? Do the pairs match? [3 points]"
      ],
      "metadata": {
        "id": "fwDFA5rffQ5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: [Least similar pairs] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "UXA1QqjME53h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_pairs = []\n",
        "sim_scores = []\n",
        "human_sim_scores = []\n",
        "for word1, word2, score in df.values.tolist():\n",
        "    word_pairs.append((word1, word2))\n",
        "    sim_score = vectors.similarity(word1, word2)\n",
        "    sim_scores.append(sim_score)\n",
        "    human_sim_scores.append(score)\n",
        "\n",
        "sim_scores_array = np.array(sim_scores)\n",
        "idx = np.argmin(sim_scores_array)\n",
        "print(\"least pair in vector similarity {}\".format(word_pairs[idx]))\n",
        "\n",
        "sorted_indices = np.argsort(sim_scores)\n",
        "# Get the index of the 2nd smallest value\n",
        "second_smallest_index = sorted_indices[1]\n",
        "print(\"2nd least pair in vector similarity {}\".format(word_pairs[second_smallest_index]))\n",
        "\n",
        "human_scores_array = np.array(human_sim_scores)\n",
        "idx = np.argmin(human_scores_array)\n",
        "print(\"least pair in human similarity {}\".format(word_pairs[idx]))\n",
        "\n",
        "\n",
        "sorted_indices = np.argsort(human_sim_scores)\n",
        "# Get the index of the 2nd smallest value\n",
        "second_smallest_index = sorted_indices[1]\n",
        "print(\"2nd least pair in human similarity {}\".format(word_pairs[second_smallest_index]))"
      ],
      "metadata": {
        "id": "2TuNcVOuE6D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071f499f-5105-4916-ad9a-6214fe7a00c2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "least pair in vector similarity ('house', 'key')\n",
            "2nd least pair in vector similarity ('flower', 'endurance')\n",
            "least pair in human similarity ('new', 'ancient')\n",
            "2nd least pair in human similarity ('shrink', 'grow')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Answer 2.2:** What is the most similar 2 pairs of words based on human judgement scores and vector similarity? Do the pairs match? [3 points]"
      ],
      "metadata": {
        "id": "QlymLVazfVJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: [Most similar pairs] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "eEjH6A1dE-ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argmax(sim_scores_array)\n",
        "print(\"most similar pair in vector similarity {}\".format(word_pairs[idx]))\n",
        "\n",
        "sorted_indices = np.argsort(sim_scores_array)\n",
        "second_biggest_index = sorted_indices[-2]\n",
        "print(\"most similar pair in vector similarity {}\".format(word_pairs[second_biggest_index]))\n",
        "\n",
        "human_scores_array = np.array(human_sim_scores)\n",
        "idx = np.argmax(human_scores_array)\n",
        "print(\"most similar pair in human similarity {}\".format(word_pairs[idx]))\n",
        "\n",
        "sorted_indices = np.argsort(human_sim_scores)\n",
        "second_biggest_index = sorted_indices[-2]\n",
        "print(\"most similar pair in vector similarity {}\".format(word_pairs[second_biggest_index]))"
      ],
      "metadata": {
        "id": "MKwpBnM6E-vi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97dd86ea-d869-46d8-ee0d-e22fb0625895"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "most similar pair in vector similarity ('south', 'north')\n",
            "most similar pair in vector similarity ('north', 'west')\n",
            "most similar pair in human similarity ('vanish', 'disappear')\n",
            "most similar pair in vector similarity ('quick', 'rapid')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Answer 2.3:** Provide correlation scores and p values for the following models:\n",
        "   - (Stanford - GloVe Wikipedia 2014 + Gigaword 5 6B Medium 50D) `glove.6B.50d.magnitude`\n",
        "   - (Stanford - GloVe Wikipedia 2014 + Gigaword 5 6B Medium 100D)`glove.6B.100d.magnitude`\n",
        "   - (Stanford - GloVe Wikipedia 2014 + Gigaword 5 6B Medium 200D) `glove.6B.200d.magnitude`\n",
        "   - (Stanford - GloVe Wikipedia 2014 + Gigaword 5 6B Medium 300D) `glove.6B.300d.magnitude`\n",
        "   - (Stanford - GloVe Common Crawl Medium 300D) `love.840B.300d.magnitude`\n",
        "\n",
        "  **How do those correlation value compare to each other?** [4 points]"
      ],
      "metadata": {
        "id": "3irwnpa9fcLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: [Discussion] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "H9smACWQFC7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget http://magnitude.plasticity.ai/glove/medium/glove.6B.50d.magnitude\n",
        "!wget http://magnitude.plasticity.ai/glove/medium/glove.6B.100d.magnitude\n",
        "!wget http://magnitude.plasticity.ai/glove/medium/glove.6B.200d.magnitude\n",
        "!wget http://magnitude.plasticity.ai/glove/medium/glove.6B.300d.magnitude\n",
        "!wget http://magnitude.plasticity.ai/glove/medium/glove.840B.300d.magnitude"
      ],
      "metadata": {
        "id": "V0ABIVIXs09b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if the above links do not work, please uncomment the below lines and run them"
      ],
      "metadata": {
        "id": "9l4NugVudP45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1r0ebRDG-_4ALl3PJ7Vko0DkLcMdLPIoL # glove.6B.50d.magnitude\n",
        "!gdown 1TQ5W7mma_fYKqVL-Dm7_ogwIftyJpXAT # glove.6B.100d.magnitude\n",
        "!gdown 1LiKprfuwD434FGC-bf8OARMIKCtNIL4Z # glove.6B.200d.magnitude\n",
        "!gdown 1_p-9y15JvbobeJ37L5v4kXnWMXsfHsD4 # glove.6B.300d.magnitude\n",
        "!gdown 1zs0Z-m7YbbVbKvqkq-HEIxNYp3e75-7e # glove.840B.300d.magnitude"
      ],
      "metadata": {
        "id": "v_0w8DO5dYRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0794f3c-c154-455f-e0d2-7aaa23161b32",
        "collapsed": true
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1r0ebRDG-_4ALl3PJ7Vko0DkLcMdLPIoL\n",
            "From (redirected): https://drive.google.com/uc?id=1r0ebRDG-_4ALl3PJ7Vko0DkLcMdLPIoL&confirm=t&uuid=335077c9-4974-4034-a7fd-19d91a092b95\n",
            "To: /content/glove.6B.50d.magnitude\n",
            "100% 211M/211M [00:01<00:00, 139MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TQ5W7mma_fYKqVL-Dm7_ogwIftyJpXAT\n",
            "From (redirected): https://drive.google.com/uc?id=1TQ5W7mma_fYKqVL-Dm7_ogwIftyJpXAT&confirm=t&uuid=c333dc86-f291-4af7-b6b5-a57fca3ad5d8\n",
            "To: /content/glove.6B.100d.magnitude\n",
            "100% 302M/302M [00:01<00:00, 156MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1LiKprfuwD434FGC-bf8OARMIKCtNIL4Z\n",
            "From (redirected): https://drive.google.com/uc?id=1LiKprfuwD434FGC-bf8OARMIKCtNIL4Z&confirm=t&uuid=04cddd91-0026-4122-92d9-ac2732ea8afc\n",
            "To: /content/glove.6B.200d.magnitude\n",
            "100% 507M/507M [00:04<00:00, 111MB/s] \n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_p-9y15JvbobeJ37L5v4kXnWMXsfHsD4\n",
            "From (redirected): https://drive.google.com/uc?id=1_p-9y15JvbobeJ37L5v4kXnWMXsfHsD4&confirm=t&uuid=7cd7d85a-5814-4061-8923-e106c1df162b\n",
            "To: /content/glove.6B.300d.magnitude\n",
            "100% 667M/667M [00:10<00:00, 65.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zs0Z-m7YbbVbKvqkq-HEIxNYp3e75-7e\n",
            "From (redirected): https://drive.google.com/uc?id=1zs0Z-m7YbbVbKvqkq-HEIxNYp3e75-7e&confirm=t&uuid=eb58af31-fde7-4335-807e-dbe5be296976\n",
            "To: /content/glove.840B.300d.magnitude\n",
            "100% 3.65G/3.65G [00:40<00:00, 90.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"./\")"
      ],
      "metadata": {
        "id": "ocOrOTPGdc_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7865e67e-c784-4633-c70e-c405caf5a85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'GoogleNews-vectors-negative300.magnitude',\n",
              " 'notebook-config.yaml',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correlation(file_path):\n",
        "    vectors = Magnitude(file_path)\n",
        "    df = pd.read_csv('/content/SimLex-999.txt', sep='\\t')[['word1', 'word2', 'SimLex999']]\n",
        "    human_scores = []\n",
        "    vector_scores = []\n",
        "\n",
        "    for word1, word2, score in df.values.tolist():\n",
        "        human_scores.append(score)\n",
        "        similarity_score = vectors.similarity(word1, word2)\n",
        "        vector_scores.append(similarity_score)\n",
        "\n",
        "    correlation, p_value = stats.kendalltau(human_scores, vector_scores)\n",
        "    print(f'file_path = {file_path} Correlation = {correlation}, P Value = {p_value}')\n",
        "    return correlation, p_value"
      ],
      "metadata": {
        "id": "fXP91lawIOBU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE ##\n",
        "# you can re-use the code from the Reference Code\n",
        "magnitude_files = [\n",
        "    \"/content/glove.6B.50d.magnitude\",\n",
        "    \"/content/glove.6B.100d.magnitude\",\n",
        "    \"/content/glove.6B.200d.magnitude\",\n",
        "    \"/content/glove.6B.300d.magnitude\",\n",
        "    \"/content/glove.840B.300d.magnitude\",\n",
        "]\n",
        "\n",
        "for file_path in magnitude_files:\n",
        "    get_correlation(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "hVfFpp44zECt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103b9827-c075-4741-c823-16044bc025f8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_path = /content/glove.6B.50d.magnitude Correlation = 0.18100126067449063, P Value = 1.2242211264976856e-17\n",
            "file_path = /content/glove.6B.100d.magnitude Correlation = 0.20506409092608713, P Value = 3.41228663395174e-22\n",
            "file_path = /content/glove.6B.200d.magnitude Correlation = 0.23670323199262908, P Value = 4.9936324557833286e-29\n",
            "file_path = /content/glove.6B.300d.magnitude Correlation = 0.25894302181101986, P Value = 2.080389068003349e-34\n",
            "file_path = /content/glove.840B.300d.magnitude Correlation = 0.2860664813618063, P Value = 1.293335613361039e-41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CoU6zfxps06_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Creating Word Sense Clusters [96 points]\n",
        "**Background:** Many Natural Language Processing (NLP) tasks require knowing the sense of polysemous words, which are words with multiple meanings. For example, the word bug can mean:\n",
        "\n",
        "1. A creepy crawly thing\n",
        "2. An error in your computer code\n",
        "3. A virus or bacteria that makes you sick\n",
        "4. A listening device planted by the FBI\n",
        "\n",
        "In past research my PhD students and I have looked into automatically deriving the different meaning of polysemous words like bug by clustering their paraphrases. We have developed a resource called [the paraphrase database (PPDB)](http://paraphrase.org/) that contains of paraphrases for tens of millions words and phrases. For the target word bug, we have an unordered list of paraphrases including: insect, glitch, beetle, error, microbe, wire, cockroach, malfunction, microphone, mosquito, virus, tracker, pest, informer, snitch, parasite, bacterium, fault, mistake, failure and many others. We used automatic clustering group those into sets like:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1-YbbvZ0qwRKPiHZ1ZWOm62dfCkfu4tLJ'>\n",
        "\n",
        "The clusters in the image above approximate the different word senses of bug, where the 4 circles are the 4 senses of bug. The input to this problem is all the paraphrases in a single list, and the task is to separate them correctly. As humans, this is pretty intuitive, but computers are not that smart. You will explore the main idea underlying our word sense clustering method: which measure the similarity between each pair of paraphrases for a target word and then group together the paraphrases that are most similar to each other. This affinity matrix gives an example of one of the methods for measuring similarity that we tried in our [paper](https://www.cis.upenn.edu/~ccb/publications/clustering-paraphrases-by-word-sense.pdf):\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1v1dBzwoSM3S3Y1wDUwqcVBEZ7GxxKKJ4'>\n",
        "\n",
        "Here the darkness values give an indication of how similar paraphrases are to each other. For instance in this example similarity between *insect* and *pest* is greater than the similarity between insect and error. You can read more about this task in [these](https://www.cis.upenn.edu/~ccb/publications/clustering-paraphrases-by-word-sense.pdf) [papers](https://cs.uwaterloo.ca/~cdimarco/pdf/cs886/Pantel+Lin02.pdf).\n",
        "\n",
        "In this assignment, we will use vector representations in order to measure their similarities of pairs of paraphrases. You will play with different vector space representations of words to create clusters of word senses. We expect that you have read Jurafsky and Martin [Chapter 6](https://web.stanford.edu/~jurafsky/slp3/6.pdf). Word vectors, also known as word embeddings, can be thought of simply as points in some high-dimensional space. Remember in geometry class when you learned about the Euclidean plane, and 2-dimensional points in that plane? It’s not hard to understand distance between those points – you can even measure it with a ruler. Then you learned about 3-dimensional points, and how to calculate the distance between these. These 3-dimensional points can be thought of as positions in physical space.\n",
        "\n",
        "Now, do your best to stop thinking about physical space, and generalize this idea in your mind: you can calculate a distance between 2-dimensional and 3-dimensional points, now imagine a point with `N` dimensions. The dimensions don’t necessarily have meaning in the same way as the X,Y, and Z dimensions in physical space, but we can calculate distances all the same.\n",
        "\n",
        "This is how we will use word vectors in this assignment: as points in some high-dimensional space, where distances between points are meaningful. The interpretation of distance between word vectors depends entirely on how they were made, but for our purposes, we will consider distance to measure semantic similarity. Word vectors that are close together should have meanings that are similar.\n",
        "\n",
        "With this framework, we can see how to solve our paraphrase clustering problem."
      ],
      "metadata": {
        "id": "chcQNwdxfye3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Data:**\n",
        "The input data to be used for this assignment consists of sets of paraphrases corresponding to one of polysemous target words, e.g.\n",
        "\n",
        "Target\t  | Paraphrase set\n",
        "----------|------------------\n",
        "note.v    | comment mark tell observe state notice say remark mention\n",
        "hot.a     | raging spicy blistering red-hot live\n",
        "\n",
        "(Here the `.v` following the target `note` indicates the part of speech)\n",
        "\n",
        "Your objective is to automatically cluster each paraphrase set such that each cluster contains words pertaining to a single sense, or meaning, of the target word. Note that a single word from the paraphrase set might belong to one or more clusters.\n",
        "\n",
        "**Development Data:** The development data consists of two files:\n",
        "\n",
        "1. words file (input)\n",
        "2. clusters file (output)\n",
        "\n",
        "The words file `dev_input.txt` is formatted such that each line contains one target, its paraphrase set, and the number of ground truth clusters `k`, separated by a `::` symbol. You can use `k` as input to your clustering algorithm.\n",
        "\n",
        "`target.pos :: k :: paraphrase1 paraphrase2 paraphrase3 ...`\n",
        "\n",
        "The clusters file `dev_output.txt` contains the ground truth clusters for each target word’s paraphrase set, split over k lines:\n",
        "\n",
        "```\n",
        "target.pos :: 1 :: paraphrase2 paraphrase6\n",
        "target.pos :: 2 :: paraphrase3 paraphrase4 paraphrase5\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "target.pos :: k :: paraphrase1 paraphrase9\n",
        "```\n",
        "\n",
        "**Test data:** For testing Tasks 3.1 – 3.3, you will receive only words file `test_input.txt` containing the test target words, number of ground truth clusters and their paraphrase sets. For testing Task 3.4, you will receive only words file `test_nok_input.txt` containing the test target words and their paraphrases sets. Neither order of senses, nor order of words in a cluster matter.\n",
        "\n",
        "**Evaluation:** There are many possible ways to evaluate clustering solutions. For this homework we will rely on the paired F-score, which you can read more about in [this paper](https://www.cs.york.ac.uk/semeval2010_WSI/paper/semevaltask14.pdf).\n",
        "\n",
        "The general idea behind paired F-score is to treat clustering prediction like a classification problem; given a target word and its paraphrase set, we call a *positive instance* any pair of paraphrases that appear together in a ground-truth cluster. Once we predict a clustering solution for the paraphrase set, we similarly generate the set of word pairs such that both words in the pair appear in the same predicted cluster. We can then evaluate our set of predicted pairs against the ground truth pairs using precision, recall, and F-score.\n",
        "\n",
        "V-Measure is another metric that is used to evaluate clustering solutions, however we will not be using it in this Assignment."
      ],
      "metadata": {
        "id": "8Qkv_eW5luSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks:**\n",
        "Your task is to fill in 4 functions: `cluster_random`, `cluster_with_sparse_representation`, `cluster_with_dense_representation`, `cluster_with_no_k`.\n",
        "\n",
        "We provided 5 utility functions for you to use:\n",
        "\n",
        "1. `load_input_file(file_path)` that converts the input data (the words file) into 2 dictionaries. The first dictionary is a mapping between a target word and a list of paraphrases. The second dictionary is a mapping between a target word and a number of clusters for a given target word.\n",
        "\n",
        "2. `load_output_file(file_path)` that converts the output data (the clusters file) into a dictionary, where a key is a target word and a value is it’s list of list of paraphrases. Each list of paraphrases is a cluster. Remember that Neither order of senses, nor order of words in a cluster matter.\n",
        "\n",
        "3. `get_paired_f_score(gold_clustering, predicted_clustering)` that calculates paired F-score given a gold and predicted clustering for a target word.\n",
        "\n",
        "4. `evaluate_clusterings(gold_clusterings, predicted_clusterings)` that calculates paired F-score for all target words present in the data and prints the final F-Score weighted by the number of senses that a target word has.\n",
        "\n",
        "5. `write_to_output_file(file_path, clusterings)` that writes the result of the clustering for each target word into the output file (clusters file)\n",
        "Full points will be awarded for each of the tasks if your implementation gets above a certain threshold on the test dataset. Please submit to autograder to see thresholds. Note that thresholds are based on the scores from the previous year and might be lowered depending on the average performance."
      ],
      "metadata": {
        "id": "Q3io9PlcoYqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions, DO NOT MODIFY\n",
        "def load_input_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads the input file to two dictionaries\n",
        "    :param file_path: path to an input file\n",
        "    :return: 2 dictionaries:\n",
        "    1. Dictionary, where key is a target word and value is a list of paraphrases\n",
        "    2. Dictionary, where key is a target word and value is a number of clusters\n",
        "    \"\"\"\n",
        "    word_to_paraphrases_dict = {}\n",
        "    word_to_k_dict = {}\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for line in fin:\n",
        "            target_word, k, paraphrases = line.split(' :: ')\n",
        "            word_to_k_dict[target_word] = int(k)\n",
        "            word_to_paraphrases_dict[target_word] = paraphrases.split()\n",
        "\n",
        "    return word_to_paraphrases_dict, word_to_k_dict\n",
        "\n",
        "    #Example for word note, one dictionary value is list of paraphrase [currency, comment, mark, tell], 2nd dictionary has k value of 4 as value\n",
        "    #{'note': [currency, comment]}, {'note': 5}\n",
        "\n",
        "def load_output_file(file_path):\n",
        "    \"\"\"\n",
        "    :param file_path: path to an output file\n",
        "    :return: A dictionary, where key is a target word and value is a list of list of paraphrases\n",
        "    \"\"\"\n",
        "    clusterings = {}\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for line in fin:\n",
        "            target_word, _, paraphrases_in_cluster = line.strip().split(' :: ')\n",
        "            paraphrases_list = paraphrases_in_cluster.strip().split()\n",
        "            if target_word not in clusterings:\n",
        "                clusterings[target_word] = []\n",
        "            clusterings[target_word].append(paraphrases_list)\n",
        "\n",
        "    return clusterings\n",
        "\n",
        "        #{ #key is target word\n",
        "    #    'note': [['comment', 'remark'], ['mark', 'observe', 'state'], ['tell', 'say', 'mention']] each list is predicted cluster\n",
        "    #}\n",
        "\n",
        "\n",
        "def get_paired_f_score(gold_clustering, predicted_clustering):\n",
        "    \"\"\"\n",
        "    :param gold_clustering: gold list of list of paraphrases\n",
        "    :param predicted_clustering: predicted list of list of paraphrases\n",
        "    :return: Paired F-Score\n",
        "    \"\"\"\n",
        "    gold_pairs = set()\n",
        "    for gold_cluster in gold_clustering:\n",
        "        for pair in combinations(gold_cluster, 2):\n",
        "            gold_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "    predicted_pairs = set()\n",
        "    for predicted_cluster in predicted_clustering:\n",
        "        for pair in combinations(predicted_cluster, 2):\n",
        "            predicted_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "    overlapping_pairs = gold_pairs & predicted_pairs\n",
        "\n",
        "    precision = 1. if len(predicted_pairs) == 0 else float(len(overlapping_pairs)) / len(predicted_pairs)\n",
        "    recall = 1. if len(gold_pairs) == 0 else float(len(overlapping_pairs)) / len(gold_pairs)\n",
        "    paired_f_score = 0. if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return paired_f_score\n",
        "\n",
        "\n",
        "    #example call below\n",
        "    #gold_clustering = [['comment', 'remark'], ['mark', 'observe', 'state'], ['tell', 'say', 'mention']]\n",
        "    #predicted_clustering = [['comment', 'remark', 'mark'], ['observe', 'state'], ['tell', 'say', 'mention']]\n",
        "    #f_score = get_paired_f_score(gold_clustering, predicted_clustering)\n",
        "    #print(f_score)  # Output: 0.6\n",
        "\n",
        "def evaluate_clusterings(gold_clusterings, predicted_clusterings):\n",
        "    \"\"\"\n",
        "    Displays evaluation scores between gold and predicted clusterings\n",
        "    :param gold_clusterings: dictionary where key is a target word and value is a list of list of paraphrases\n",
        "    :param predicted_clusterings: dictionary where key is a target word and value is a list of list of paraphrases\n",
        "    :return: N/A\n",
        "    \"\"\"\n",
        "    target_words = set(gold_clusterings.keys()) & set(predicted_clusterings.keys())\n",
        "\n",
        "    if len(target_words) == 0:\n",
        "        print('No overlapping target words in ground-truth and predicted files')\n",
        "        return None\n",
        "\n",
        "    paired_f_scores = np.zeros((len(target_words)))\n",
        "    ks = np.zeros((len(target_words)))\n",
        "\n",
        "    table = PrettyTable(['Target', 'k', 'Paired F-Score'])\n",
        "    for i, target_word in enumerate(target_words):\n",
        "        paired_f_score = get_paired_f_score(gold_clusterings[target_word], predicted_clusterings[target_word])\n",
        "        k = len(gold_clusterings[target_word])\n",
        "        paired_f_scores[i] = paired_f_score\n",
        "        ks[i] = k\n",
        "        table.add_row([target_word, k, f'{paired_f_score:0.4f}'])\n",
        "\n",
        "    average_f_score = np.average(paired_f_scores, weights=ks)\n",
        "    print(table)\n",
        "    print(f'=> Average Paired F-Score:  {average_f_score:.4f}')\n",
        "\n",
        "    #example call below -> averages paired f score also weighted by no of senses that a target word has\n",
        "    #gold_clusterings = {'note.v': [['comment', 'remark'], ['mark', 'observe', 'state'], ['tell', 'say', 'mention']]}\n",
        "    #predicted_clusterings = {'note.v': [['comment', 'remark', 'mark'], ['observe', 'state'], ['tell', 'say', 'mention']]}\n",
        "    #evaluate_clusterings(gold_clusterings, predicted_clusterings)\n",
        "\n",
        "def write_to_output_file(file_path, clusterings):\n",
        "    \"\"\"\n",
        "    Writes the result of clusterings into an output file\n",
        "    :param file_path: path to an output file\n",
        "    :param clusterings:  A dictionary, where key is a target word and value is a list of list of paraphrases\n",
        "    :return: N/A\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w') as fout:\n",
        "        for target_word, clustering in clusterings.items():\n",
        "            for i, cluster in enumerate(clustering):\n",
        "                fout.write(f'{target_word} :: {i + 1} :: {\" \".join(cluster)}\\n')\n",
        "        fout.close()"
      ],
      "metadata": {
        "id": "VACDGSp3VQ1f"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Cluster Randomly [11 points]\n",
        "Write a function `cluster_random(word_to_paraphrases_dict, word_to_k_dict)` that accepts 2 dictionaries:\n",
        "\n",
        "1. word_to_paraphrases_dict = a mapping between a target word and a list of paraphrases\n",
        "\n",
        "2. word_to_k_dict = a mapping between a target word and a number of clusters for a given target\n",
        "\n",
        "The function outputs a dictionary, where the key is a target word and a value is a list of list of paraphrases, where a list of paraphrases represents a distinct sense of a target word.\n",
        "\n",
        "For this task put paraphrases into distinct senses at random. That is, assign to pick a random word for each cluster, as opposed to picking a random cluster for each word. This will ensure that all clusters have at lease one word in them. We recommend using random packages. Please use 123 as a random seed. Your output should look similar to this on the development dataset:\n",
        "\n",
        "```\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file('dev_input.txt')\n",
        "gold_clusterings = load_output_file('dev_output.txt')\n",
        "predicted_clusterings = cluster_random(word_to_paraphrases_dict, word_to_k_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)\n",
        "```\n",
        "```\n",
        "+----------------+----+----------------+\n",
        "|     Target     | k  | Paired F-Score |\n",
        "+----------------+----+----------------+\n",
        "|    paper.n     | 7  |     0.2978     |\n",
        "|     play.v     | 34 |     0.0896     |\n",
        "|     miss.v     | 8  |     0.2376     |\n",
        "|   produce.v    | 7  |     0.2335     |\n",
        "|    party.n     | 5  |     0.2480     |\n",
        "|     note.v     | 3  |     0.6667     |\n",
        "|     bank.n     | 9  |     0.1515     |\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "|     eat.v      | 6  |     0.2908     |\n",
        "|    climb.v     | 6  |     0.2427     |\n",
        "|    degree.n    | 7  |     0.2891     |\n",
        "|   interest.n   | 5  |     0.2093     |\n",
        "+----------------+----+----------------+\n",
        "=> Average Paired F-Score:  0.2318\n",
        "```"
      ],
      "metadata": {
        "id": "n4sau65Vo47N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 3.1:** Implement `cluster_random` function. **The augograder for 3.2, 3.3, 3.4 will grade your implementation based on the test-set `f_score` achieved by the clustering.**  [10 points]\n"
      ],
      "metadata": {
        "id": "N4XX7i3VLqnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recitation\n",
        "\n",
        "\n",
        "        word_to_paraphrases_dict = {\n",
        "        'note.v': ['comment', 'mark', 'tell', 'observe', 'state', 'notice', 'say', 'remark', 'mention'],\n",
        "        'hot.a': ['raging', 'spicy', 'blistering', 'red-hot', 'live']\n",
        "                                    }\n",
        "\n",
        "          word_to_k_dict = {\n",
        "          'note.v': 3,\n",
        "          'hot.a': 2\n",
        "                           }\n",
        "\n",
        "        Expected output :\n",
        "        {\n",
        "            'note.v': [['remark', 'mention'], ['comment', 'state', 'tell', 'mark'], ['observe', 'say', 'notice']],\n",
        "            'hot.a': [['blistering', 'red-hot'], ['live', 'spicy', 'raging']]\n",
        "        }\n",
        "\n",
        "      shuffled = ['tell', 'remark', 'say', 'comment', 'mention', 'observe', 'state', 'mark', 'notice'] ->order reshuffled\n"
      ],
      "metadata": {
        "id": "c4LIRiNryqY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(123)\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "def cluster_random(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases randomly\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        k = word_to_k_dict[target_word]\n",
        "        # TODO: Implement beg\n",
        "        cluster = []\n",
        "        for i in range(k):\n",
        "            cluster.append([])\n",
        "        if k == 1 or k == 0:\n",
        "            cluster[0] = paraphrase_list[:]\n",
        "        else:\n",
        "            numbers = [i for i in range(0, len(paraphrase_list))]\n",
        "            random.shuffle(numbers)\n",
        "\n",
        "            for i in range(len(paraphrase_list)):\n",
        "                bucket = i % k\n",
        "                cluster[bucket].append(paraphrase_list[numbers[i]])\n",
        "\n",
        "            # for word in paraphrase_list:\n",
        "            #     random_number = random.randint(0, k-1)\n",
        "            #     cluster[random_number].append(word)\n",
        "\n",
        "        clusterings[target_word] = cluster\n",
        "        # TODO: Implement end\n",
        "\n",
        "    return clusterings"
      ],
      "metadata": {
        "id": "O4q_QnuaXZp-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Answer 3.1:** Run clustering on `dev` data, report the `f_scores` from the `dev` data [1 point]"
      ],
      "metadata": {
        "id": "QD3t0b5nkdVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: [Report f_scores from the dev data] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "_L7TdWwc_yb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Reference Code ###\n",
        "###### You can use the following code to test your clustering on dev data ######\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_random(word_to_paraphrases_dict_dev, word_to_k_dict_dev)\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "id": "WAqsnipaCAwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c13938-81db-4c14-9de7-c8d0209d566c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|     eat.v      | 6  |     0.2186     |\n",
            "|    climb.v     | 6  |     0.1963     |\n",
            "|    expect.v    | 6  |     0.2105     |\n",
            "|     note.v     | 3  |     0.3684     |\n",
            "|    simple.a    | 5  |     0.2000     |\n",
            "|   produce.v    | 7  |     0.1968     |\n",
            "|   shelter.n    | 5  |     0.2500     |\n",
            "|     wash.v     | 13 |     0.0849     |\n",
            "|    write.v     | 9  |     0.1507     |\n",
            "|     rule.v     | 7  |     0.1267     |\n",
            "|     miss.v     | 8  |     0.1972     |\n",
            "|     mean.v     | 6  |     0.1772     |\n",
            "|     win.v      | 4  |     0.2756     |\n",
            "|    treat.v     | 8  |     0.1610     |\n",
            "|     bank.n     | 9  |     0.0741     |\n",
            "|   judgment.n   | 7  |     0.1480     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|    begin.v     | 8  |     0.0897     |\n",
            "|     talk.v     | 6  |     0.2428     |\n",
            "| performance.n  | 5  |     0.2347     |\n",
            "|   provide.v    | 7  |     0.2204     |\n",
            "|    image.n     | 9  |     0.1204     |\n",
            "|    source.n    | 9  |     0.1260     |\n",
            "|    degree.n    | 7  |     0.1985     |\n",
            "|     play.v     | 34 |     0.0308     |\n",
            "|    paper.n     | 7  |     0.2044     |\n",
            "|   interest.n   | 5  |     0.1704     |\n",
            "|  difference.n  | 5  |     0.2564     |\n",
            "|    watch.v     | 5  |     0.2041     |\n",
            "| organization.n | 7  |     0.1734     |\n",
            "|   suspend.v    | 6  |     0.1304     |\n",
            "|  atmosphere.n  | 6  |     0.1812     |\n",
            "|   operate.v    | 7  |     0.1237     |\n",
            "|    smell.v     | 4  |     0.2857     |\n",
            "|   express.v    | 7  |     0.1562     |\n",
            "|     hear.v     | 5  |     0.1835     |\n",
            "|     plan.n     | 3  |     0.3693     |\n",
            "|     use.v      | 6  |     0.2407     |\n",
            "|   receive.v    | 13 |     0.0903     |\n",
            "|    party.n     | 5  |     0.2017     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.1592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following command to generate the output file for the predicted clusterings for the test dataset."
      ],
      "metadata": {
        "id": "DIrLRhCCANCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file('test_input.txt')\n",
        "predicted_clusterings_random = cluster_random(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('test_output_random.txt', predicted_clusterings_random)"
      ],
      "metadata": {
        "id": "wvWFRKjVpjsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "# reload_grader()\n",
        "grader.grade(test_case_id = 'test_q3_clusters_random', answer = (predicted_clusterings_random, 'random'))"
      ],
      "metadata": {
        "id": "LdCjRoPu8VrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae25bc6-7164-4ee4-c056-9b8c6714a7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 10/10 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Cluster with Sparse Representations [26 points]\n",
        "\n",
        "Write a function `cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict)`. The input and output remains the same as in Task 1, however the clustering of paraphrases will no longer be random and is based on sparse vector representation.\n",
        "\n",
        "We will feature-based (not dense) vector space representation. In this type of VSM, each dimension of the vector space corresponds to a specific feature, such as a context word (see, for example, the term-context matrix described in [Chapter 6.1.2 of Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/6.pdf)). You will calculate cooccurrence vectors on the Reuters RCV1 corpus. It can take a long time to build cooccurrence vectors, so we have pre-built set called `coocvec-500mostfreq-window-3.filter.magnitude`. To save on space, these include only the words used in the given files. This representation of words uses a term-context matrix `M` of size `|V| x D`, where `|V|` is the size of the vocabulary and D=500. Each feature corresponds to one of the top 500 most-frequent words in the corpus. The value of matrix entry `M[i][j]` gives the number of times the context word represented by column `j` appeared within W=3 words to the left or right of the word represented by row `i` in the corpus.\n",
        "\n",
        "Use one of the clustering algorithms, for instance K-means clustering in `cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict)`. Here is an example of the K-means clustering code:"
      ],
      "metadata": {
        "id": "eXbO7_Rapj8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recitation\n",
        "This function is tasked with clustering paraphrases based on sparse vector representations.\n",
        "We use concept of cooccurence matrix and sparse vector representations\n",
        "\n",
        "1. Cooccurrence Vectors: Cooccurrence vectors represent how often each word in the vocabulary co-occurs with every other word in a given context window.\n",
        "Suppose we have the following sentences in our corpus:\n",
        "\n",
        "* \"I like eating apples.\"\n",
        "* \"Apples are delicious.\"\n",
        "* \"She bought some apples from the market.\"\n",
        "* \"The teacher gave us apples as a snack.\"\n",
        "\n",
        "Our vocabulary consists of the following words: \"I\", \"like\", \"eating\", \"apples\", \"are\", \"delicious\", \"she\", \"bought\", \"some\", \"from\", \"the\", \"market\", \"teacher\", \"gave\", \"us\", \"as\", \"a\", \"snack\".\n",
        "\n",
        "The cooccurrence matrix would look something like this (simplified):\n",
        "\n",
        "|        | I | like | eating | apples | are | delicious | she | bought | some | from | the | market | teacher | gave | us | as | a | snack |\n",
        "|--------|---|------|--------|--------|-----|-----------|-----|--------|------|------|-----|--------|---------|------|----|----|---|-------|\n",
        "| I      | 0 | 1    | 0      | 0      | 0   | 0         | 0   | 0      | 0    | 0    | 0   | 0      | 0       | 0    | 0  | 0  | 0 | 0     |\n",
        "| like   | 1 | 0    | 1      | 0      | 0   | 0         | 0   | 0      | 0    | 0    | 0   | 0      | 0       | 0    | 0  | 0  | 0 | 0     |\n",
        "| eating | 0 | 1    | 0      | 1      | 0   | 0         | 0   | 0      | 0    | 0    | 0   | 0      | 0       | 0    | 0  | 0  | 0 | 0     |\n",
        "| apples | 0 | 0    | 1      | 0      | 1   | 1         | 1   | 1      | 1    | 1    | 1   | 1      | 0       | 0    | 0  | 0  | 0 | 0     |\n",
        "\n",
        "\n",
        "The value at position (i, j) in the matrix represents the number of times word i co-occurs with word j within the context window.\n",
        "\n",
        "This is an example of sparse vector representation because most of the matrix above is sparse. We are using sparse vector representation from Magnitude:\n",
        "\n",
        "**coocvec-500mostfreq-window-3.filter.magnitude**: Derived from a co-occurrence matrix based on a corpus like Reuters RCV1.Each dimension represents the count of the word's co-occurrence with one of the top 500 most frequent context words within a window of 3 words.\n"
      ],
      "metadata": {
        "id": "5Rx6g29gxlrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=k).fit(X)\n",
        "print(kmeans.labels_)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gYTPLYmliqed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_filter = Magnitude(\"coocvec-500mostfreq-window-3.filter.magnitude\")\n",
        "vectors_filter.dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "efvByMrmn38B",
        "outputId": "b29eb874-2ea5-4dc8-9118-a824b10924aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectors_filter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RH_2v8jgX1Z",
        "outputId": "cc53f391-0b84-4839-ca12-9f8860af2b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2178"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_filter.query(\"wage\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S2WwqEWMsNbw",
        "outputId": "8bbc5f1e-ffd3-4c98-c752-400f8079073f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.596719e-01, 3.361610e-01, 3.025449e-01, 2.642704e-01,\n",
              "       2.494139e-01, 2.509247e-01, 3.181569e-01, 3.522766e-01,\n",
              "       1.480619e-01, 4.935400e-02, 8.561400e-02, 6.924700e-03,\n",
              "       6.093710e-02, 2.366980e-02, 1.913730e-02, 1.578824e-01,\n",
              "       9.039840e-02, 3.802270e-02, 2.832820e-02, 6.219610e-02,\n",
              "       8.397730e-02, 5.514550e-02, 4.910220e-02, 5.136840e-02,\n",
              "       2.845410e-02, 7.931890e-02, 1.611560e-02, 0.000000e+00,\n",
              "       3.600830e-02, 3.361610e-02, 4.532500e-03, 2.304030e-02,\n",
              "       3.424560e-02, 2.278840e-02, 1.573790e-02, 2.027040e-02,\n",
              "       2.631370e-02, 4.041490e-02, 3.009080e-02, 4.343650e-02,\n",
              "       3.273480e-02, 8.309600e-03, 2.933540e-02, 3.764500e-02,\n",
              "       2.052220e-02, 1.888500e-03, 1.070180e-02, 2.304030e-02,\n",
              "       5.162000e-03, 1.334570e-02, 2.505470e-02, 2.769900e-03,\n",
              "       0.000000e+00, 1.548610e-02, 1.775230e-02, 4.784300e-03,\n",
              "       1.095360e-02, 1.485660e-02, 4.532500e-03, 1.548610e-02,\n",
              "       1.158310e-02, 1.057590e-02, 2.404750e-02, 1.183490e-02,\n",
              "       4.280700e-03, 2.644000e-03, 5.791500e-03, 1.107950e-02,\n",
              "       1.170900e-02, 2.215890e-02, 3.651200e-03, 3.349020e-02,\n",
              "       2.858000e-02, 7.050600e-03, 6.547000e-03, 2.140400e-03,\n",
              "       2.518000e-04, 3.550460e-02, 4.910200e-03, 1.624150e-02,\n",
              "       3.525300e-03, 5.413800e-03, 6.798800e-03, 5.413800e-03,\n",
              "       5.287900e-03, 5.036100e-03, 6.295000e-04, 4.658400e-03,\n",
              "       3.147600e-03, 3.777100e-03, 1.321980e-02, 8.183700e-03,\n",
              "       5.036000e-04, 2.001860e-02, 4.406600e-03, 1.246440e-02,\n",
              "       1.007200e-03, 3.021700e-03, 1.674510e-02, 7.050600e-03,\n",
              "       0.000000e+00, 7.931900e-03, 8.813200e-03, 1.196080e-02,\n",
              "       8.813200e-03, 5.036100e-03, 1.070180e-02, 2.140400e-03,\n",
              "       8.561400e-03, 5.539700e-03, 1.737460e-02, 3.021700e-03,\n",
              "       0.000000e+00, 5.036000e-04, 7.176500e-03, 1.145720e-02,\n",
              "       6.295200e-03, 4.028900e-03, 1.649330e-02, 1.259000e-04,\n",
              "       9.568600e-03, 1.007220e-02, 7.554000e-04, 0.000000e+00,\n",
              "       7.554000e-04, 2.518000e-04, 3.147600e-03, 2.266300e-03,\n",
              "       2.014400e-03, 0.000000e+00, 6.421100e-03, 3.525300e-03,\n",
              "       2.014400e-03, 2.568420e-02, 1.284210e-02, 1.259000e-04,\n",
              "       2.392200e-03, 1.082770e-02, 1.133100e-03, 7.554000e-04,\n",
              "       2.014400e-03, 5.036000e-04, 6.295200e-03, 2.266300e-03,\n",
              "       0.000000e+00, 1.334570e-02, 7.554000e-04, 1.938910e-02,\n",
              "       8.813000e-04, 2.392200e-03, 4.784300e-03, 0.000000e+00,\n",
              "       8.813000e-04, 8.435500e-03, 4.154800e-03, 3.147600e-03,\n",
              "       6.043300e-03, 8.813000e-04, 0.000000e+00, 4.280700e-03,\n",
              "       4.028900e-03, 2.316620e-02, 5.036100e-03, 7.554000e-04,\n",
              "       1.120540e-02, 5.036100e-03, 1.347160e-02, 7.302400e-03,\n",
              "       7.554000e-04, 3.777000e-04, 3.777000e-04, 2.518000e-04,\n",
              "       0.000000e+00, 8.435500e-03, 6.043300e-03, 8.813000e-04,\n",
              "       7.302400e-03, 7.037980e-02, 3.777000e-04, 7.554000e-04,\n",
              "       7.680100e-03, 2.014400e-03, 1.259000e-04, 4.658400e-03,\n",
              "       5.036100e-03, 1.460470e-02, 2.014400e-03, 4.154800e-03,\n",
              "       6.798800e-03, 8.813000e-04, 4.406600e-03, 6.295000e-04,\n",
              "       1.007200e-03, 1.057590e-02, 1.133100e-03, 1.259000e-03,\n",
              "       7.680100e-03, 2.895800e-03, 4.280700e-03, 0.000000e+00,\n",
              "       5.036000e-04, 1.510800e-03, 1.384900e-03, 5.791500e-03,\n",
              "       2.769900e-03, 2.518000e-04, 0.000000e+00, 3.273500e-03,\n",
              "       1.259000e-04, 1.133100e-03, 7.050600e-03, 1.636700e-03,\n",
              "       7.554000e-04, 0.000000e+00, 3.777000e-04, 4.658400e-03,\n",
              "       8.813000e-04, 1.636700e-03, 1.259000e-04, 2.518000e-04,\n",
              "       1.208670e-02, 6.295000e-04, 6.295200e-03, 1.510800e-03,\n",
              "       2.895800e-03, 4.104440e-02, 4.532500e-03, 1.007200e-03,\n",
              "       2.518100e-03, 2.266300e-03, 3.147600e-03, 1.636700e-03,\n",
              "       1.259000e-04, 8.183700e-03, 8.813000e-04, 4.028900e-03,\n",
              "       1.762600e-03, 3.777000e-04, 5.791500e-03, 6.924700e-03,\n",
              "       1.384900e-03, 9.190900e-03, 1.762600e-03, 1.007200e-03,\n",
              "       2.895800e-03, 4.784300e-03, 2.417340e-02, 4.280700e-03,\n",
              "       3.021700e-03, 6.295000e-04, 2.518100e-03, 1.888500e-03,\n",
              "       3.651200e-03, 4.406600e-03, 6.672900e-03, 2.895800e-03,\n",
              "       1.259000e-03, 5.036000e-04, 3.777100e-03, 2.769900e-03,\n",
              "       0.000000e+00, 2.518000e-04, 1.523430e-02, 3.399400e-03,\n",
              "       0.000000e+00, 8.435500e-02, 4.784300e-03, 1.636700e-03,\n",
              "       1.259000e-04, 2.518000e-04, 2.266300e-03, 3.147600e-03,\n",
              "       1.384900e-03, 4.028900e-03, 1.762600e-03, 7.957070e-02,\n",
              "       3.210530e-02, 2.266300e-03, 1.384900e-03, 5.036000e-04,\n",
              "       4.028900e-03, 4.658400e-03, 4.028900e-03, 5.287900e-03,\n",
              "       3.777000e-04, 1.875950e-02, 7.302400e-03, 6.798800e-03,\n",
              "       1.510800e-03, 3.903000e-03, 1.510800e-03, 2.769900e-03,\n",
              "       3.777000e-04, 1.271620e-02, 3.865220e-02, 3.273500e-03,\n",
              "       2.518000e-04, 3.777000e-04, 2.644000e-03, 1.259000e-03,\n",
              "       7.554000e-04, 1.636700e-03, 3.273500e-03, 4.784300e-03,\n",
              "       2.518000e-04, 7.554000e-04, 5.036000e-04, 2.266300e-03,\n",
              "       1.259000e-04, 3.399400e-03, 1.384900e-03, 2.140400e-03,\n",
              "       2.769900e-03, 1.510800e-03, 2.518000e-04, 1.259000e-03,\n",
              "       1.133100e-03, 0.000000e+00, 1.888500e-03, 2.644000e-03,\n",
              "       8.309600e-03, 0.000000e+00, 2.518000e-04, 2.266300e-03,\n",
              "       2.518060e-02, 2.644000e-03, 2.140400e-03, 1.762640e-02,\n",
              "       1.762600e-03, 1.888500e-03, 4.532500e-03, 6.547000e-03,\n",
              "       1.259000e-04, 3.777000e-04, 0.000000e+00, 1.259000e-04,\n",
              "       1.259000e-03, 3.399400e-03, 1.259000e-04, 2.518000e-04,\n",
              "       1.007200e-03, 3.273500e-03, 1.259000e-04, 1.259000e-03,\n",
              "       4.532500e-03, 3.537870e-02, 4.154800e-03, 2.392200e-03,\n",
              "       1.510800e-03, 1.007220e-02, 2.140400e-03, 5.036000e-04,\n",
              "       5.036000e-04, 3.525300e-03, 0.000000e+00, 8.813000e-04,\n",
              "       5.539700e-03, 2.140400e-03, 2.518100e-03, 0.000000e+00,\n",
              "       2.644000e-03, 3.273500e-03, 2.769900e-03, 2.014400e-03,\n",
              "       8.813000e-04, 0.000000e+00, 0.000000e+00, 3.525300e-03,\n",
              "       2.644000e-03, 2.518100e-03, 7.680100e-03, 5.036000e-04,\n",
              "       0.000000e+00, 3.777000e-04, 0.000000e+00, 2.518100e-03,\n",
              "       0.000000e+00, 1.724870e-02, 1.259000e-04, 3.777100e-03,\n",
              "       6.698040e-02, 7.554000e-04, 1.259000e-04, 2.518000e-04,\n",
              "       1.259000e-03, 7.050600e-03, 8.813000e-04, 1.636700e-03,\n",
              "       1.762600e-03, 1.259000e-04, 4.406600e-03, 1.259000e-04,\n",
              "       1.259000e-03, 1.636700e-03, 2.518100e-03, 1.510800e-03,\n",
              "       1.510800e-03, 5.036000e-04, 1.007200e-03, 1.133100e-03,\n",
              "       2.392200e-03, 8.813000e-04, 1.007200e-03, 3.147600e-03,\n",
              "       7.554000e-04, 2.518000e-04, 5.036000e-04, 1.636700e-03,\n",
              "       3.903000e-03, 2.518100e-03, 1.007200e-03, 2.266300e-03,\n",
              "       2.518000e-04, 2.518100e-03, 4.154800e-03, 5.036000e-04,\n",
              "       1.636700e-03, 1.259000e-03, 0.000000e+00, 3.399400e-03,\n",
              "       9.190900e-03, 1.296800e-02, 2.518000e-04, 1.133100e-03,\n",
              "       8.813000e-04, 4.028900e-03, 1.259000e-03, 2.518000e-04,\n",
              "       7.554000e-04, 2.392200e-03, 1.510800e-03, 1.636700e-03,\n",
              "       7.554000e-04, 4.910200e-03, 1.259000e-04, 0.000000e+00,\n",
              "       3.777000e-04, 1.259000e-04, 1.259000e-03, 0.000000e+00,\n",
              "       0.000000e+00, 1.259000e-04, 0.000000e+00, 3.777100e-03,\n",
              "       4.532500e-03, 5.036000e-04, 2.769900e-03, 1.510800e-03,\n",
              "       1.510800e-03, 2.266300e-03, 2.014400e-03, 5.036100e-03,\n",
              "       1.510800e-03, 2.518000e-04, 2.518000e-04, 1.259000e-04,\n",
              "       3.273500e-03, 1.259000e-04, 3.777000e-04, 7.554000e-04,\n",
              "       6.295000e-04, 1.246440e-02, 1.510800e-03, 5.036100e-03,\n",
              "       2.140400e-03, 7.554000e-04, 1.259000e-04, 1.888500e-03,\n",
              "       3.273500e-03, 0.000000e+00, 1.347160e-02, 1.636700e-03,\n",
              "       1.762600e-03, 7.554000e-04, 0.000000e+00, 5.615270e-02,\n",
              "       1.636700e-03, 0.000000e+00, 2.769900e-03, 2.266300e-03,\n",
              "       3.651200e-03, 1.259000e-03, 1.636700e-03, 1.259000e-04,\n",
              "       2.014400e-03, 2.140400e-03, 1.133100e-03, 0.000000e+00,\n",
              "       2.769900e-03, 1.384900e-03, 2.392200e-03, 1.259000e-04,\n",
              "       3.777000e-04, 2.392200e-03, 1.259000e-04, 1.259000e-04,\n",
              "       0.000000e+00, 1.133100e-03, 0.000000e+00, 2.518000e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_filter.most_similar(\"provide\", topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7-NCNyhpkgU",
        "outputId": "b39fa209-8228-4aec-e2c5-fee9cbf56d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pay', np.float32(0.96469593)),\n",
              " ('make', np.float32(0.9622039)),\n",
              " ('initiate', np.float32(0.95320475)),\n",
              " ('deliver', np.float32(0.95313513)),\n",
              " ('produce', np.float32(0.95056176)),\n",
              " ('develop', np.float32(0.950259)),\n",
              " ('give', np.float32(0.94986254)),\n",
              " ('add', np.float32(0.9494212)),\n",
              " ('invite', np.float32(0.9436469)),\n",
              " ('help', np.float32(0.9426739))]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random pick a vector\n",
        "random_idx = random.randint(0, len(vectors_filter)-1)\n",
        "vectors_filter.index(random_idx)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_S2Zw2NtqF6E",
        "outputId": "3f4de88a-351b-4914-a7bd-77e9fa3380cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0. , 0.5, 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
              "       0. , 0. , 0. , 0. , 0. , 0. ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectors_filter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiYxhqXEs7Te",
        "outputId": "d27da867-57cf-4aa6-d3fb-f2c1c602bfd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2178"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"bewhisker\" in vectors_filter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8U8OsqKrDJ-",
        "outputId": "4d454d3e-b4ea-43c2-eb92-971de71d8abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing purpose\n",
        "candidates = [\"ramp\", \"wage\", \"computerise\", \"yield\", \"charge\", \"articulate\", \"nourish\", \"upholster\", \"fix\", \"match\", \"glut\", \"rail\", \"furnish\", \"edge\", \"fire\", \"canal\", \"engage\", \"grate\", \"cater\", \"dish\",  \"indulge\", \"arm\", \"sustain\", \"capitalise\",  \"provision\", \"headquarter\", \"fulfil\", \"feed\", \"fill\"]\n",
        "# \"transistorise\",  \"crenel\", \"bewhisker\", \"interleave\", \"reflectorise\", \"alphabetize\",\"transistorize\", \"subtitle\",\n",
        "for cand in candidates:\n",
        "    sim_score = vectors_filter.similarity(\"provide\", cand)\n",
        "    print(f\"{cand}: {sim_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FSY9hYgpv6e",
        "outputId": "c96e5bdf-ffc5-4e7c-8c5f-95086c81798e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ramp: 0.768477201461792\n",
            "wage: 0.6933643817901611\n",
            "computerise: 0.8486056923866272\n",
            "yield: 0.6685950756072998\n",
            "charge: 0.564055323600769\n",
            "articulate: 0.7511733174324036\n",
            "nourish: 0.8423349261283875\n",
            "upholster: 0.4503878951072693\n",
            "fix: 0.7347373962402344\n",
            "match: 0.633495032787323\n",
            "glut: 0.5237604379653931\n",
            "rail: 0.6514250040054321\n",
            "furnish: 0.7317972183227539\n",
            "edge: 0.6045271158218384\n",
            "fire: 0.6418688893318176\n",
            "canal: 0.6681944727897644\n",
            "engage: 0.6400899887084961\n",
            "grate: 0.6223409175872803\n",
            "cater: 0.9186713099479675\n",
            "dish: 0.6929568648338318\n",
            "indulge: 0.5828347206115723\n",
            "arm: 0.4609805643558502\n",
            "sustain: 0.9171574115753174\n",
            "capitalise: 0.6914789080619812\n",
            "provision: 0.5384178161621094\n",
            "headquarter: 0.4618830978870392\n",
            "fulfil: 0.9044535756111145\n",
            "feed: 0.7542145252227783\n",
            "fill: 0.9162811636924744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"./\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhhZ3ojMnmp7",
        "outputId": "2b122644-c66a-4614-9739-a7442f9daf22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'dev_output.txt',\n",
              " 'glove.840B.300d.magnitude',\n",
              " 'SimLex-999.txt',\n",
              " 'glove.6B.200d.magnitude',\n",
              " 'coocvec-500mostfreq-window-3.filter.magnitude',\n",
              " 'glove.6B.100d.magnitude',\n",
              " 'GoogleNews-vectors-negative300.magnitude',\n",
              " 'glove.6B.50d.magnitude',\n",
              " 'glove.6B.300d.magnitude',\n",
              " 'test_output_dense.txt',\n",
              " 'test_nok_input.txt',\n",
              " 'notebook-config.yaml',\n",
              " 'dev_input.txt',\n",
              " 'test_input.txt',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors2 = Magnitude(\"coocvec-500mostfreq-window-3.filter.magnitude\")\n",
        "len(vectors2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0JpKWU5gcx2",
        "outputId": "cfb4f178-3ed5-4ff5-e996-8bf691fb634a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2178"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 3.2:** Implement `cluster_with_sparse_representation` function [20 points]"
      ],
      "metadata": {
        "id": "d8NhxobeLVJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using sparse vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"coocvec-500mostfreq-window-3.filter.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        k = word_to_k_dict[target_word]\n",
        "        cluster = []\n",
        "        for i in range(k):\n",
        "            cluster.append([])\n",
        "        # TODO: Implement\n",
        "        X = np.zeros((len(paraphrase_list), vectors.dim))\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            paraphrase = paraphrase_list[i]\n",
        "            if paraphrase in vectors:\n",
        "                X[i] = vectors.query(paraphrase)\n",
        "            else:\n",
        "                random_idx = random.randint(0, len(vectors)-1)\n",
        "                vectors.index(random_idx)[1]\n",
        "                X[i] = vectors.index(random_idx)[1].copy()\n",
        "                # random_idx = random.randint(0, len(vectors_filter)-1)\n",
        "                # vectors_filter.index(random_idx)[1]\n",
        "                # X[i] = vectors_filter.index(random_idx)[1].copy()\n",
        "        kmeans.fit(X)\n",
        "        labels = kmeans.labels_\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            cluster[labels[i]].append(paraphrase_list[i])\n",
        "        # TODO: end\n",
        "        clusterings[target_word] = cluster\n",
        "\n",
        "    return clusterings"
      ],
      "metadata": {
        "id": "32YASRIJZ4p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recitation\n",
        "Example:\n",
        "\n",
        "        word_to_paraphrases_dict = {\n",
        "        'note.v': ['comment', 'mark', 'tell', 'observe', 'state', 'notice', 'say', 'remark', 'mention'],\n",
        "        'hot.a': ['raging', 'spicy', 'blistering', 'red-hot', 'live']\n",
        "                                    }\n",
        "\n",
        "          word_to_k_dict = {\n",
        "          'note.v': 3,\n",
        "          'hot.a': 2\n",
        "                           }\n",
        "        Expected output :\n",
        "        {\n",
        "            'note.v': [['remark', 'mention'], ['comment', 'state', 'tell', 'mark'], ['observe', 'say', 'notice']],\n",
        "            'hot.a': [['blistering', 'red-hot'], ['live', 'spicy', 'raging']]\n",
        "        }\n",
        "          x (matrix of features):         Represent each word in 500 dimensions\n",
        "          Word      dim1 dim2 dim3....dim500\n",
        "          comment   ........................\n",
        "          mark      ........................\n",
        "          .\n",
        "          .\n",
        "          .\n",
        "          mention   ,,,,,,,,,,,,,,,,,,,,,,,,\n",
        "          if dimensions of the word not present in magnitude, randomly draw a vector from magnitude\n",
        "        \n",
        "        Once you have your X, run k means on that.\n",
        "        #Kmeans must have assigned a cluster no to each of the 3 paraphrase\n",
        "            #comment 0\n",
        "            #tell 1\n",
        "            #state 1\n",
        "            #say 0\n",
        "\n",
        "        Based on above (paraphrase,cluster no) pair, make your list of lists and hence your output dictionary\n",
        "        "
      ],
      "metadata": {
        "id": "XqnAm7ohxuzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 3.2.1:** Run clustering on `dev` data, report the `f_scores` from the `dev` data [1 point]"
      ],
      "metadata": {
        "id": "6q1d4gGaL3jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: [Report f_scores from the dev data] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "TqZdlNrXAyUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (you can re-use reference code from 3.1)\n",
        "###### You can use the following code to test your clustering on dev data ######\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_sparse_representation(word_to_paraphrases_dict_dev, word_to_k_dict_dev)\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "id": "fI6ViokIL4Pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f59999e0-3c36-40fe-9102-82c3fda3bc37",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.3273     |\n",
            "|  atmosphere.n  | 6  |     0.3058     |\n",
            "|     play.v     | 34 |     0.0845     |\n",
            "|    smell.v     | 4  |     0.2947     |\n",
            "|     talk.v     | 6  |     0.3223     |\n",
            "|     plan.n     | 3  |     0.5772     |\n",
            "|    party.n     | 5  |     0.2322     |\n",
            "|   express.v    | 7  |     0.2340     |\n",
            "|     win.v      | 4  |     0.4051     |\n",
            "|    paper.n     | 7  |     0.4899     |\n",
            "|     rule.v     | 7  |     0.2174     |\n",
            "|   produce.v    | 7  |     0.2159     |\n",
            "|     note.v     | 3  |     0.5333     |\n",
            "| performance.n  | 5  |     0.2684     |\n",
            "|     bank.n     | 9  |     0.3373     |\n",
            "|     miss.v     | 8  |     0.2182     |\n",
            "|   operate.v    | 7  |     0.2283     |\n",
            "|    write.v     | 9  |     0.1660     |\n",
            "|    source.n    | 9  |     0.2337     |\n",
            "|     mean.v     | 6  |     0.3804     |\n",
            "|    expect.v    | 6  |     0.3661     |\n",
            "|  difference.n  | 5  |     0.3835     |\n",
            "|    image.n     | 9  |     0.1634     |\n",
            "|   shelter.n    | 5  |     0.2919     |\n",
            "|     use.v      | 6  |     0.3369     |\n",
            "|     wash.v     | 13 |     0.1467     |\n",
            "|   interest.n   | 5  |     0.2333     |\n",
            "| organization.n | 7  |     0.2422     |\n",
            "|    treat.v     | 8  |     0.2145     |\n",
            "|    begin.v     | 8  |     0.2775     |\n",
            "|    climb.v     | 6  |     0.3608     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.3276     |\n",
            "|   suspend.v    | 6  |     0.2222     |\n",
            "|    simple.a    | 5  |     0.1429     |\n",
            "|     hear.v     | 5  |     0.3368     |\n",
            "|     eat.v      | 6  |     0.3096     |\n",
            "|   judgment.n   | 7  |     0.2086     |\n",
            "|   receive.v    | 13 |     0.2013     |\n",
            "|    watch.v     | 5  |     0.2857     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.2526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following command to generate the output file for the predicted clusterings for the test dataset."
      ],
      "metadata": {
        "id": "kitStHe3AuPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file('test_input.txt')\n",
        "predicted_clusterings_sparse = cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('test_output_sparse.txt', predicted_clusterings_sparse)"
      ],
      "metadata": {
        "id": "wUWjirVALhAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "# reload_grader()\n",
        "grader.grade(test_case_id = 'test_q3_clusters_sparse', answer = (predicted_clusterings_sparse, 'sparse'))"
      ],
      "metadata": {
        "id": "_J_OYL3YAfsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19aa867f-e9f9-4cfb-9d10-0f5cb2b83052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 20/20 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 3.2.2:** Provide a brief description of your method in the report, making sure to describe the vector space model you chose, the clustering algorithm you used, and the results of any preliminary experiments you might have run on the dev set.  [5 points]"
      ],
      "metadata": {
        "id": "ku8Ymx_djo0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suggestions to improve the performance of your model:\n",
        "\n",
        " - ~~What if you reduce or increase `D` in the baseline implementation?  **-> Increase or decrease Dimensions from 500, use such vector representation**~~\n",
        "\n",
        " - ~~Does it help to change the window `W` used to extract contexts? **-> The file coocvec-500mostfreq-window-3.filter.magnitude contains vectors that were generated with a window size W=3.This means that when the co-occurrence matrix was built, the context for each word was determined by considering up to 3 words to the left and 3 words to the right.Try changing that. use such vector representation**~~\n",
        "\n",
        " - Play around with the feature weighting – instead of raw counts, would it help to use PPMI? -**> Convert your co-occurrence matrix to a PPMI matrix and use these weighted vectors for clustering.**\n",
        "\n",
        " - Try a different clustering algorithm that’s included with the [scikit-learn clustering package](http://scikit-learn.org/stable/modules/clustering.html), or implement your own. **-> Agglomerative Clustering, DBSCAN**\n",
        "\n",
        " - What if you include additional types of features, like paraphrases in the [Paraphrase Database](http://www.paraphrase.org/) or the part-of-speech of context words? **-> Enrich your vectors with additional features and observe the impact on clustering performance.**\n",
        "\n",
        "The only feature types that are off-limits are WordNet features.\n",
        "\n",
        "Provide a brief description of your method in the Report, making sure to describe the vector space model you chose, the clustering algorithm you used, and the results of any preliminary experiments you might have run on the dev set."
      ],
      "metadata": {
        "id": "41LGnqdmLiVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AgglomerativeCLustering\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def cluster_with_sparse_representation_agglo(word_to_paraphrases_dict, word_to_k_dict, linkage_opt):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using sparse vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"coocvec-500mostfreq-window-3.filter.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        k = word_to_k_dict[target_word]\n",
        "        cluster = []\n",
        "        for i in range(k):\n",
        "            cluster.append([])\n",
        "        # TODO: Implement\n",
        "        X = np.zeros((len(paraphrase_list), vectors.dim))\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            paraphrase = paraphrase_list[i]\n",
        "            if paraphrase in vectors:\n",
        "                X[i] = vectors.query(paraphrase)\n",
        "            else:\n",
        "                random_idx = random.randint(0, len(vectors)-1)\n",
        "                vectors.index(random_idx)[1]\n",
        "                X[i] = vectors.index(random_idx)[1].copy()\n",
        "        agglo_model = AgglomerativeClustering(n_clusters=k, linkage=linkage_opt)\n",
        "        labels = agglo_model.fit_predict(X)\n",
        "\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            cluster[labels[i]].append(paraphrase_list[i])\n",
        "        # TODO: end\n",
        "        clusterings[target_word] = cluster\n",
        "\n",
        "    return clusterings"
      ],
      "metadata": {
        "id": "fItTu9SrniRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ward\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_sparse_representation_agglo(word_to_paraphrases_dict_dev, word_to_k_dict_dev, \"ward\")\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "id": "Oe9pNyo9YXM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b29a6a-c399-405a-d09d-266b16e81d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.2674     |\n",
            "|  atmosphere.n  | 6  |     0.2597     |\n",
            "|     play.v     | 34 |     0.0761     |\n",
            "|    smell.v     | 4  |     0.2955     |\n",
            "|     talk.v     | 6  |     0.3026     |\n",
            "|     plan.n     | 3  |     0.5239     |\n",
            "|    party.n     | 5  |     0.2497     |\n",
            "|   express.v    | 7  |     0.2630     |\n",
            "|     win.v      | 4  |     0.3873     |\n",
            "|    paper.n     | 7  |     0.3442     |\n",
            "|     rule.v     | 7  |     0.2484     |\n",
            "|   produce.v    | 7  |     0.2740     |\n",
            "|     note.v     | 3  |     0.4762     |\n",
            "| performance.n  | 5  |     0.2428     |\n",
            "|     bank.n     | 9  |     0.2299     |\n",
            "|     miss.v     | 8  |     0.1895     |\n",
            "|   operate.v    | 7  |     0.2174     |\n",
            "|    write.v     | 9  |     0.2181     |\n",
            "|    source.n    | 9  |     0.2067     |\n",
            "|     mean.v     | 6  |     0.3505     |\n",
            "|    expect.v    | 6  |     0.2533     |\n",
            "|  difference.n  | 5  |     0.3862     |\n",
            "|    image.n     | 9  |     0.1478     |\n",
            "|   shelter.n    | 5  |     0.3573     |\n",
            "|     use.v      | 6  |     0.3646     |\n",
            "|     wash.v     | 13 |     0.1008     |\n",
            "|   interest.n   | 5  |     0.2134     |\n",
            "| organization.n | 7  |     0.2769     |\n",
            "|    treat.v     | 8  |     0.1970     |\n",
            "|    begin.v     | 8  |     0.3304     |\n",
            "|    climb.v     | 6  |     0.3232     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.3069     |\n",
            "|   suspend.v    | 6  |     0.1892     |\n",
            "|    simple.a    | 5  |     0.2222     |\n",
            "|     hear.v     | 5  |     0.3593     |\n",
            "|     eat.v      | 6  |     0.2429     |\n",
            "|   judgment.n   | 7  |     0.2584     |\n",
            "|   receive.v    | 13 |     0.1538     |\n",
            "|    watch.v     | 5  |     0.3361     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.2389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# average\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_sparse_representation_agglo(word_to_paraphrases_dict_dev, word_to_k_dict_dev, \"average\")\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PzNLl1AxFgJC",
        "outputId": "f2b7ce1d-07cf-443d-d2e8-b4e297abedb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.4654     |\n",
            "|  atmosphere.n  | 6  |     0.3924     |\n",
            "|     play.v     | 34 |     0.1538     |\n",
            "|    smell.v     | 4  |     0.3462     |\n",
            "|     talk.v     | 6  |     0.5996     |\n",
            "|     plan.n     | 3  |     0.6387     |\n",
            "|    party.n     | 5  |     0.3567     |\n",
            "|   express.v    | 7  |     0.4131     |\n",
            "|     win.v      | 4  |     0.5247     |\n",
            "|    paper.n     | 7  |     0.5109     |\n",
            "|     rule.v     | 7  |     0.2911     |\n",
            "|   produce.v    | 7  |     0.4287     |\n",
            "|     note.v     | 3  |     0.6400     |\n",
            "| performance.n  | 5  |     0.4931     |\n",
            "|     bank.n     | 9  |     0.2429     |\n",
            "|     miss.v     | 8  |     0.2667     |\n",
            "|   operate.v    | 7  |     0.2844     |\n",
            "|    write.v     | 9  |     0.2740     |\n",
            "|    source.n    | 9  |     0.3248     |\n",
            "|     mean.v     | 6  |     0.4436     |\n",
            "|    expect.v    | 6  |     0.3302     |\n",
            "|  difference.n  | 5  |     0.4703     |\n",
            "|    image.n     | 9  |     0.2864     |\n",
            "|   shelter.n    | 5  |     0.4752     |\n",
            "|     use.v      | 6  |     0.6223     |\n",
            "|     wash.v     | 13 |     0.1253     |\n",
            "|   interest.n   | 5  |     0.3206     |\n",
            "| organization.n | 7  |     0.3944     |\n",
            "|    treat.v     | 8  |     0.3838     |\n",
            "|    begin.v     | 8  |     0.4000     |\n",
            "|    climb.v     | 6  |     0.3064     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.7004     |\n",
            "|   suspend.v    | 6  |     0.2254     |\n",
            "|    simple.a    | 5  |     0.2564     |\n",
            "|     hear.v     | 5  |     0.3220     |\n",
            "|     eat.v      | 6  |     0.4426     |\n",
            "|   judgment.n   | 7  |     0.3214     |\n",
            "|   receive.v    | 13 |     0.1761     |\n",
            "|    watch.v     | 5  |     0.3529     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# complete\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_sparse_representation_agglo(word_to_paraphrases_dict_dev, word_to_k_dict_dev, \"complete\")\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uHYzc4zFFnbD",
        "outputId": "f5acffa5-39b8-4d76-d443-42e65c6fe226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.3727     |\n",
            "|  atmosphere.n  | 6  |     0.3003     |\n",
            "|     play.v     | 34 |     0.0985     |\n",
            "|    smell.v     | 4  |     0.2892     |\n",
            "|     talk.v     | 6  |     0.3480     |\n",
            "|     plan.n     | 3  |     0.5809     |\n",
            "|    party.n     | 5  |     0.3024     |\n",
            "|   express.v    | 7  |     0.2855     |\n",
            "|     win.v      | 4  |     0.3776     |\n",
            "|    paper.n     | 7  |     0.3645     |\n",
            "|     rule.v     | 7  |     0.2584     |\n",
            "|   produce.v    | 7  |     0.3088     |\n",
            "|     note.v     | 3  |     0.5333     |\n",
            "| performance.n  | 5  |     0.3973     |\n",
            "|     bank.n     | 9  |     0.2597     |\n",
            "|     miss.v     | 8  |     0.1609     |\n",
            "|   operate.v    | 7  |     0.2385     |\n",
            "|    write.v     | 9  |     0.2881     |\n",
            "|    source.n    | 9  |     0.2305     |\n",
            "|     mean.v     | 6  |     0.4094     |\n",
            "|    expect.v    | 6  |     0.3564     |\n",
            "|  difference.n  | 5  |     0.4345     |\n",
            "|    image.n     | 9  |     0.2498     |\n",
            "|   shelter.n    | 5  |     0.3539     |\n",
            "|     use.v      | 6  |     0.5333     |\n",
            "|     wash.v     | 13 |     0.1329     |\n",
            "|   interest.n   | 5  |     0.2500     |\n",
            "| organization.n | 7  |     0.3738     |\n",
            "|    treat.v     | 8  |     0.2580     |\n",
            "|    begin.v     | 8  |     0.3203     |\n",
            "|    climb.v     | 6  |     0.3131     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.6403     |\n",
            "|   suspend.v    | 6  |     0.2532     |\n",
            "|    simple.a    | 5  |     0.1379     |\n",
            "|     hear.v     | 5  |     0.3109     |\n",
            "|     eat.v      | 6  |     0.2994     |\n",
            "|   judgment.n   | 7  |     0.3315     |\n",
            "|   receive.v    | 13 |     0.1490     |\n",
            "|    watch.v     | 5  |     0.3529     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.2832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_sparse_representation_agglo(word_to_paraphrases_dict_dev, word_to_k_dict_dev, \"single\")\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL7FoKvzGkV1",
        "outputId": "c3bd9e79-58e8-4a1e-a0e6-9f516e9a9bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.4666     |\n",
            "|  atmosphere.n  | 6  |     0.3595     |\n",
            "|     play.v     | 34 |     0.1712     |\n",
            "|    smell.v     | 4  |     0.4839     |\n",
            "|     talk.v     | 6  |     0.6142     |\n",
            "|     plan.n     | 3  |     0.6387     |\n",
            "|    party.n     | 5  |     0.3572     |\n",
            "|   express.v    | 7  |     0.4345     |\n",
            "|     win.v      | 4  |     0.4813     |\n",
            "|    paper.n     | 7  |     0.4570     |\n",
            "|     rule.v     | 7  |     0.2880     |\n",
            "|   produce.v    | 7  |     0.4385     |\n",
            "|     note.v     | 3  |     0.6400     |\n",
            "| performance.n  | 5  |     0.4427     |\n",
            "|     bank.n     | 9  |     0.2429     |\n",
            "|     miss.v     | 8  |     0.3511     |\n",
            "|   operate.v    | 7  |     0.2844     |\n",
            "|    write.v     | 9  |     0.3516     |\n",
            "|    source.n    | 9  |     0.3378     |\n",
            "|     mean.v     | 6  |     0.4361     |\n",
            "|    expect.v    | 6  |     0.4205     |\n",
            "|  difference.n  | 5  |     0.4778     |\n",
            "|    image.n     | 9  |     0.2885     |\n",
            "|   shelter.n    | 5  |     0.4244     |\n",
            "|     use.v      | 6  |     0.6223     |\n",
            "|     wash.v     | 13 |     0.1963     |\n",
            "|   interest.n   | 5  |     0.3121     |\n",
            "| organization.n | 7  |     0.3958     |\n",
            "|    treat.v     | 8  |     0.4033     |\n",
            "|    begin.v     | 8  |     0.3688     |\n",
            "|    climb.v     | 6  |     0.3375     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.7063     |\n",
            "|   suspend.v    | 6  |     0.2830     |\n",
            "|    simple.a    | 5  |     0.2564     |\n",
            "|     hear.v     | 5  |     0.3234     |\n",
            "|     eat.v      | 6  |     0.4495     |\n",
            "|   judgment.n   | 7  |     0.3309     |\n",
            "|   receive.v    | 13 |     0.2289     |\n",
            "|    watch.v     | 5  |     0.4400     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Description of your method **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "btx89xeVA3lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Cluster with Dense Representations [28 points]\n",
        "\n",
        "Write a function `cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict)`. The input and output remains the same as in Task 1 and 2, however the clustering of paraphrases is based on dense vector representation.\n",
        "\n",
        "We would like to see if dense word embeddings are better for clustering the words in our test set. Run the word clustering task again, but this time use a dense word representation.\n",
        "\n",
        "For this task, we have also included a file called `GoogleNews-vectors-negative300.filter.magnitude`, which is filtered to contain only the words in the dev/test splits.\n",
        "\n",
        "As before, use the provided word vectors to represent words and perform one of the clusterings. Here are some suggestions to improve the performance of your model:\n",
        "\n",
        " - Try downloading a different dense vector space model from the web, like [Paragram](http://www.cs.cmu.edu/~jwieting/) or [fastText](https://fasttext.cc/docs/en/english-vectors.html).\n",
        " - Train your own word vectors, either on the provided corpus or something you find online. You can try the skip-gram, CBOW models, or [GLOVE](https://nlp.stanford.edu/projects/glove/). Try experimenting with the dimensionality.\n",
        " - [Retrofitting](https://www.cs.cmu.edu/~hovy/papers/15HLT-retrofitting-word-vectors.pdf) is a simple way to add additional semantic knowledge to pre-trained vectors. The retrofitting code is available here. Experiment with different lexicons, or even try [counter-fitting](http://www.aclweb.org/anthology/N16-1018)."
      ],
      "metadata": {
        "id": "r1gDMVIeL7Bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 3.3:** Implement `cluster_with_dense_representation` function [20 points]"
      ],
      "metadata": {
        "id": "AFuNXJyQMD5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recitation\n",
        "**GoogleNews-vectors-negative300.magnitude:**\n",
        "\n",
        "\n",
        "\n",
        "*   Dense vector representation.\n",
        "*   Trained on a large corpus of Google News articles.\n",
        "*   300 dimensions\n",
        "*   Please keep the `np.random.seed(5)` it is important for the autograder\n",
        "\n",
        "300 dimensions\n"
      ],
      "metadata": {
        "id": "yB8Jro8ex5sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using dense vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        k = word_to_k_dict[target_word]\n",
        "        np.random.seed(5)\n",
        "\n",
        "        # TODO: Implement\n",
        "        cluster = [] # init an empty array for clustering\n",
        "        for i in range(k):\n",
        "            cluster.append([])\n",
        "\n",
        "        X = np.zeros((len(paraphrase_list), vectors.dim))\n",
        "        kmeans = KMeans(n_clusters=k, random_state=33) # old value: 42, new value: 123\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            paraphrase = paraphrase_list[i]\n",
        "            if paraphrase in vectors:\n",
        "                X[i] = vectors.query(paraphrase)\n",
        "            else:\n",
        "                # random_idx = random.randint(0, len(vectors_filter)-1)\n",
        "                random_idx = np.random.randint(0, len(vectors))\n",
        "                vectors.index(random_idx)[1]\n",
        "                X[i] = vectors.index(random_idx)[1].copy()\n",
        "        kmeans.fit(X)\n",
        "        labels = kmeans.labels_\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            cluster[labels[i]].append(paraphrase_list[i])\n",
        "        # TODO: end\n",
        "        clusterings[target_word] = cluster\n",
        "\n",
        "    return clusterings"
      ],
      "metadata": {
        "id": "gK2D-aKsaOVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4536eb22-17a8-4e9f-fb7f-6ec4a9209063"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 3.3.1:** Run clustering on `dev` data, report the `f_scores` from the `dev` data [1 point]"
      ],
      "metadata": {
        "id": "rtfulbcTBM7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: [Report f_scores from the dev data] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "I7HJWGCLMAC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (you can re-use reference code from 3.1)\n",
        "\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_dense_representation(word_to_paraphrases_dict_dev, word_to_k_dict_dev)\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "id": "-JBHo0u5MAC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3a2c1d-76e8-4551-ee9a-38793c81d5a1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|     eat.v      | 6  |     0.2863     |\n",
            "|    climb.v     | 6  |     0.2811     |\n",
            "|    expect.v    | 6  |     0.3501     |\n",
            "|     note.v     | 3  |     0.8400     |\n",
            "|    simple.a    | 5  |     0.1739     |\n",
            "|   produce.v    | 7  |     0.2149     |\n",
            "|   shelter.n    | 5  |     0.2653     |\n",
            "|     wash.v     | 13 |     0.2166     |\n",
            "|    write.v     | 9  |     0.1750     |\n",
            "|     rule.v     | 7  |     0.3908     |\n",
            "|     miss.v     | 8  |     0.2933     |\n",
            "|     mean.v     | 6  |     0.3203     |\n",
            "|     win.v      | 4  |     0.4030     |\n",
            "|    treat.v     | 8  |     0.2286     |\n",
            "|     bank.n     | 9  |     0.5937     |\n",
            "|   judgment.n   | 7  |     0.3299     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|    begin.v     | 8  |     0.2767     |\n",
            "|     talk.v     | 6  |     0.3891     |\n",
            "| performance.n  | 5  |     0.3496     |\n",
            "|   provide.v    | 7  |     0.2374     |\n",
            "|    image.n     | 9  |     0.2423     |\n",
            "|    source.n    | 9  |     0.2843     |\n",
            "|    degree.n    | 7  |     0.2909     |\n",
            "|     play.v     | 34 |     0.1170     |\n",
            "|    paper.n     | 7  |     0.5180     |\n",
            "|   interest.n   | 5  |     0.4400     |\n",
            "|  difference.n  | 5  |     0.4992     |\n",
            "|    watch.v     | 5  |     0.3364     |\n",
            "| organization.n | 7  |     0.2467     |\n",
            "|   suspend.v    | 6  |     0.4151     |\n",
            "|  atmosphere.n  | 6  |     0.2950     |\n",
            "|   operate.v    | 7  |     0.2162     |\n",
            "|    smell.v     | 4  |     0.3117     |\n",
            "|   express.v    | 7  |     0.2735     |\n",
            "|     hear.v     | 5  |     0.3636     |\n",
            "|     plan.n     | 3  |     0.3844     |\n",
            "|     use.v      | 6  |     0.2812     |\n",
            "|   receive.v    | 13 |     0.1717     |\n",
            "|    party.n     | 5  |     0.3497     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.2910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bD-amBKwL_4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, run the following command to generate the output file for the predicted clusterings for the test dataset."
      ],
      "metadata": {
        "id": "-q_THdDDaOep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file('test_input.txt')\n",
        "predicted_clusterings_dense = cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('test_output_dense.txt', predicted_clusterings_dense)"
      ],
      "metadata": {
        "id": "DOErpQxTMiiJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "# reload_grader()\n",
        "grader.grade(test_case_id = 'test_q3_clusters_dense', answer = (predicted_clusterings_dense, 'dense'))"
      ],
      "metadata": {
        "id": "hMigGstY8npi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023bcf04-ba4a-4b35-8cc6-f344889c2c0d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_with_dense_representation_with_random_state(word_to_paraphrases_dict, word_to_k_dict, random_state):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using dense vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        k = word_to_k_dict[target_word]\n",
        "        np.random.seed(5)\n",
        "\n",
        "        # TODO: Implement\n",
        "        cluster = [] # init an empty array for clustering\n",
        "        for i in range(k):\n",
        "            cluster.append([])\n",
        "\n",
        "        X = np.zeros((len(paraphrase_list), vectors.dim))\n",
        "        kmeans = KMeans(n_clusters=k, random_state=random_state) # old value: 42, new value: 123\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            paraphrase = paraphrase_list[i]\n",
        "            if paraphrase in vectors:\n",
        "                X[i] = vectors.query(paraphrase)\n",
        "            else:\n",
        "                # random_idx = random.randint(0, len(vectors_filter)-1)\n",
        "                random_idx = np.random.randint(0, len(vectors))\n",
        "                vectors.index(random_idx)[1]\n",
        "                X[i] = vectors.index(random_idx)[1].copy()\n",
        "        kmeans.fit(X)\n",
        "        labels = kmeans.labels_\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            cluster[labels[i]].append(paraphrase_list[i])\n",
        "        # TODO: end\n",
        "        clusterings[target_word] = cluster\n",
        "\n",
        "    return clusterings\n",
        "\n",
        "# for random_state in range(16,43): # all is 17\n",
        "for random_state in range(44,60):\n",
        "    print(\"random_state {}\".format(random_state))\n",
        "    word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "    predicted_clusterings_random_dev = cluster_with_dense_representation_with_random_state(word_to_paraphrases_dict_dev, word_to_k_dict_dev, random_state)\n",
        "    grader.grade(test_case_id = 'test_q3_clusters_dense', answer = (predicted_clusterings_dense, 'dense'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFj5c36tpql7",
        "outputId": "dacf2050-654c-4eec-d862-c52f8376cc84"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_state 44\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 45\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 46\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 47\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 48\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 49\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 50\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 51\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 52\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 53\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 54\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 55\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 56\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 57\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 58\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n",
            "random_state 59\n",
            "You earned 17/20 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 3.3.2:** Provide a brief description of your method in the report that includes the vectors you used, and any experimental results you have from running your model on the dev set.  [5 points]"
      ],
      "metadata": {
        "id": "a_vDHhZHBjN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: [Describe your method] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "fGUaOkJfB4As"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 3.3.3:** In addition, for Task 3.2 and 3.3, do an analysis of different errors made by each system – i.e. look at instances that the word-context matrix representation gets wrong and dense gets right, and vice versa, and see if there are any interesting patterns. There is no right answer for this. [2 points]"
      ],
      "metadata": {
        "id": "7vHlYWaxMkUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AgglomerativeCLustering\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def cluster_with_dense_representation_agglo(word_to_paraphrases_dict, word_to_k_dict, linkage_opt):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using sparse vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        k = word_to_k_dict[target_word]\n",
        "        cluster = []\n",
        "        for i in range(k):\n",
        "            cluster.append([])\n",
        "        # TODO: Implement\n",
        "        X = np.zeros((len(paraphrase_list), vectors.dim))\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            paraphrase = paraphrase_list[i]\n",
        "            if paraphrase in vectors:\n",
        "                X[i] = vectors.query(paraphrase)\n",
        "            else:\n",
        "                random_idx = random.randint(0, len(vectors)-1)\n",
        "                vectors.index(random_idx)[1]\n",
        "                X[i] = vectors.index(random_idx)[1].copy()\n",
        "        agglo_model = AgglomerativeClustering(n_clusters=k, linkage=linkage_opt)\n",
        "        labels = agglo_model.fit_predict(X)\n",
        "\n",
        "        for i in range(len(paraphrase_list)):\n",
        "            cluster[labels[i]].append(paraphrase_list[i])\n",
        "        # TODO: end\n",
        "        clusterings[target_word] = cluster\n",
        "\n",
        "    return clusterings"
      ],
      "metadata": {
        "id": "MVSQi9O0eLZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_dense_representation_agglo(word_to_paraphrases_dict_dev, word_to_k_dict_dev, \"ward\")\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi3nSFx3eTG-",
        "outputId": "c67e01b0-2876-4537-bb53-c3de8a560114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.3283     |\n",
            "|  atmosphere.n  | 6  |     0.2236     |\n",
            "|     play.v     | 34 |     0.1390     |\n",
            "|    smell.v     | 4  |     0.2857     |\n",
            "|     talk.v     | 6  |     0.2830     |\n",
            "|     plan.n     | 3  |     0.4488     |\n",
            "|    party.n     | 5  |     0.3990     |\n",
            "|   express.v    | 7  |     0.2657     |\n",
            "|     win.v      | 4  |     0.3699     |\n",
            "|    paper.n     | 7  |     0.3660     |\n",
            "|     rule.v     | 7  |     0.3360     |\n",
            "|   produce.v    | 7  |     0.2832     |\n",
            "|     note.v     | 3  |     0.5714     |\n",
            "| performance.n  | 5  |     0.3806     |\n",
            "|     bank.n     | 9  |     0.3103     |\n",
            "|     miss.v     | 8  |     0.2778     |\n",
            "|   operate.v    | 7  |     0.2461     |\n",
            "|    write.v     | 9  |     0.1729     |\n",
            "|    source.n    | 9  |     0.2488     |\n",
            "|     mean.v     | 6  |     0.3478     |\n",
            "|    expect.v    | 6  |     0.4048     |\n",
            "|  difference.n  | 5  |     0.4278     |\n",
            "|    image.n     | 9  |     0.3229     |\n",
            "|   shelter.n    | 5  |     0.3926     |\n",
            "|     use.v      | 6  |     0.3688     |\n",
            "|     wash.v     | 13 |     0.2383     |\n",
            "|   interest.n   | 5  |     0.4014     |\n",
            "| organization.n | 7  |     0.2298     |\n",
            "|    treat.v     | 8  |     0.2464     |\n",
            "|    begin.v     | 8  |     0.5000     |\n",
            "|    climb.v     | 6  |     0.2312     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.2524     |\n",
            "|   suspend.v    | 6  |     0.5200     |\n",
            "|    simple.a    | 5  |     0.4000     |\n",
            "|     hear.v     | 5  |     0.4583     |\n",
            "|     eat.v      | 6  |     0.3107     |\n",
            "|   judgment.n   | 7  |     0.3083     |\n",
            "|   receive.v    | 13 |     0.1325     |\n",
            "|    watch.v     | 5  |     0.3269     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.2948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_dense_representation_agglo(word_to_paraphrases_dict_dev, word_to_k_dict_dev, \"complete\")\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ILXwWYGeZeP",
        "outputId": "4ac9d58d-0524-489b-b6e9-6c04cfb03a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.3319     |\n",
            "|  atmosphere.n  | 6  |     0.2931     |\n",
            "|     play.v     | 34 |     0.1204     |\n",
            "|    smell.v     | 4  |     0.3500     |\n",
            "|     talk.v     | 6  |     0.4376     |\n",
            "|     plan.n     | 3  |     0.5688     |\n",
            "|    party.n     | 5  |     0.3825     |\n",
            "|   express.v    | 7  |     0.3270     |\n",
            "|     win.v      | 4  |     0.3404     |\n",
            "|    paper.n     | 7  |     0.3838     |\n",
            "|     rule.v     | 7  |     0.2937     |\n",
            "|   produce.v    | 7  |     0.2707     |\n",
            "|     note.v     | 3  |     0.5238     |\n",
            "| performance.n  | 5  |     0.3339     |\n",
            "|     bank.n     | 9  |     0.4167     |\n",
            "|     miss.v     | 8  |     0.2821     |\n",
            "|   operate.v    | 7  |     0.2187     |\n",
            "|    write.v     | 9  |     0.1714     |\n",
            "|    source.n    | 9  |     0.2390     |\n",
            "|     mean.v     | 6  |     0.2993     |\n",
            "|    expect.v    | 6  |     0.6139     |\n",
            "|  difference.n  | 5  |     0.5000     |\n",
            "|    image.n     | 9  |     0.2478     |\n",
            "|   shelter.n    | 5  |     0.4724     |\n",
            "|     use.v      | 6  |     0.3068     |\n",
            "|     wash.v     | 13 |     0.2327     |\n",
            "|   interest.n   | 5  |     0.5567     |\n",
            "| organization.n | 7  |     0.2328     |\n",
            "|    treat.v     | 8  |     0.2195     |\n",
            "|    begin.v     | 8  |     0.4130     |\n",
            "|    climb.v     | 6  |     0.2990     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.3099     |\n",
            "|   suspend.v    | 6  |     0.5185     |\n",
            "|    simple.a    | 5  |     0.2727     |\n",
            "|     hear.v     | 5  |     0.3239     |\n",
            "|     eat.v      | 6  |     0.3544     |\n",
            "|   judgment.n   | 7  |     0.3035     |\n",
            "|   receive.v    | 13 |     0.1319     |\n",
            "|    watch.v     | 5  |     0.4833     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_dense_representation_agglo(word_to_paraphrases_dict_dev, word_to_k_dict_dev, \"average\")\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i69PkMYeZx8",
        "outputId": "07110517-c042-44a8-c419-b2a12e4d539c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.4062     |\n",
            "|  atmosphere.n  | 6  |     0.4215     |\n",
            "|     play.v     | 34 |     0.1794     |\n",
            "|    smell.v     | 4  |     0.4348     |\n",
            "|     talk.v     | 6  |     0.5852     |\n",
            "|     plan.n     | 3  |     0.6679     |\n",
            "|    party.n     | 5  |     0.6483     |\n",
            "|   express.v    | 7  |     0.4139     |\n",
            "|     win.v      | 4  |     0.4157     |\n",
            "|    paper.n     | 7  |     0.5536     |\n",
            "|     rule.v     | 7  |     0.2963     |\n",
            "|   produce.v    | 7  |     0.3726     |\n",
            "|     note.v     | 3  |     0.8400     |\n",
            "| performance.n  | 5  |     0.4231     |\n",
            "|     bank.n     | 9  |     0.6154     |\n",
            "|     miss.v     | 8  |     0.3218     |\n",
            "|   operate.v    | 7  |     0.2791     |\n",
            "|    write.v     | 9  |     0.2697     |\n",
            "|    source.n    | 9  |     0.2428     |\n",
            "|     mean.v     | 6  |     0.3689     |\n",
            "|    expect.v    | 6  |     0.4428     |\n",
            "|  difference.n  | 5  |     0.4928     |\n",
            "|    image.n     | 9  |     0.3126     |\n",
            "|   shelter.n    | 5  |     0.6029     |\n",
            "|     use.v      | 6  |     0.5785     |\n",
            "|     wash.v     | 13 |     0.3427     |\n",
            "|   interest.n   | 5  |     0.4101     |\n",
            "| organization.n | 7  |     0.3468     |\n",
            "|    treat.v     | 8  |     0.2983     |\n",
            "|    begin.v     | 8  |     0.4291     |\n",
            "|    climb.v     | 6  |     0.3214     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.3644     |\n",
            "|   suspend.v    | 6  |     0.3023     |\n",
            "|    simple.a    | 5  |     0.1818     |\n",
            "|     hear.v     | 5  |     0.3196     |\n",
            "|     eat.v      | 6  |     0.4087     |\n",
            "|   judgment.n   | 7  |     0.2974     |\n",
            "|   receive.v    | 13 |     0.1967     |\n",
            "|    watch.v     | 5  |     0.4541     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# single\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_dense_representation_agglo(word_to_paraphrases_dict_dev, word_to_k_dict_dev, \"single\")\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xOT4_cjeaFS",
        "outputId": "d873fcbf-48e4-47be-e084-0d94a73f7cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.4232     |\n",
            "|  atmosphere.n  | 6  |     0.4354     |\n",
            "|     play.v     | 34 |     0.1533     |\n",
            "|    smell.v     | 4  |     0.4677     |\n",
            "|     talk.v     | 6  |     0.6511     |\n",
            "|     plan.n     | 3  |     0.6471     |\n",
            "|    party.n     | 5  |     0.4323     |\n",
            "|   express.v    | 7  |     0.3982     |\n",
            "|     win.v      | 4  |     0.4892     |\n",
            "|    paper.n     | 7  |     0.5633     |\n",
            "|     rule.v     | 7  |     0.2975     |\n",
            "|   produce.v    | 7  |     0.4542     |\n",
            "|     note.v     | 3  |     0.8400     |\n",
            "| performance.n  | 5  |     0.4327     |\n",
            "|     bank.n     | 9  |     0.6154     |\n",
            "|     miss.v     | 8  |     0.3217     |\n",
            "|   operate.v    | 7  |     0.3113     |\n",
            "|    write.v     | 9  |     0.3581     |\n",
            "|    source.n    | 9  |     0.2733     |\n",
            "|     mean.v     | 6  |     0.3646     |\n",
            "|    expect.v    | 6  |     0.4675     |\n",
            "|  difference.n  | 5  |     0.4789     |\n",
            "|    image.n     | 9  |     0.3430     |\n",
            "|   shelter.n    | 5  |     0.5545     |\n",
            "|     use.v      | 6  |     0.6252     |\n",
            "|     wash.v     | 13 |     0.2362     |\n",
            "|   interest.n   | 5  |     0.3461     |\n",
            "| organization.n | 7  |     0.3833     |\n",
            "|    treat.v     | 8  |     0.3906     |\n",
            "|    begin.v     | 8  |     0.3532     |\n",
            "|    climb.v     | 6  |     0.3412     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.7261     |\n",
            "|   suspend.v    | 6  |     0.3023     |\n",
            "|    simple.a    | 5  |     0.2963     |\n",
            "|     hear.v     | 5  |     0.3241     |\n",
            "|     eat.v      | 6  |     0.4566     |\n",
            "|   judgment.n   | 7  |     0.2477     |\n",
            "|   receive.v    | 13 |     0.2016     |\n",
            "|    watch.v     | 5  |     0.4700     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict_test, word_to_k_dict_test = load_input_file('test_input.txt')\n",
        "predicted_clusterings_random_test = cluster_with_dense_representation_agglo(word_to_paraphrases_dict_test, word_to_k_dict_test, \"single\")\n",
        "for key in predicted_clusterings_random_test.keys():\n",
        "    clusters = predicted_clusterings_random_test[key]\n",
        "    for i in range(len(clusters)):\n",
        "        cluster = clusters[i]\n",
        "        s = \" \".join(cluster)\n",
        "        s.strip()\n",
        "        print(f\"{key} :: {i+1} :: {s}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVrRdqEym5XN",
        "outputId": "d46d6fa2-137d-4d70-ae26-d2c169a64fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "important.a :: 1 :: authoritative\n",
            "important.a :: 2 :: significant\n",
            "important.a :: 3 :: crucial\n",
            "argument.n :: 1 :: logomachy disceptation argy-bargy argle-bargle\n",
            "argument.n :: 2 :: value\n",
            "argument.n :: 3 :: variable casuistry conflict word clincher summary arguing statement contestation polemic fight contention adducing address proof policy argumentation give-and-take sum-up sparring disputation difference determiner dispute case firestorm parameter reasoning discussion reference evidence controversy counterargument debate\n",
            "argument.n :: 4 :: line\n",
            "argument.n :: 5 :: pro\n",
            "argument.n :: 6 :: con\n",
            "argument.n :: 7 :: tilt\n",
            "encounter.v :: 1 :: find see chance be play happen meet replay face receive confront have\n",
            "encounter.v :: 2 :: bump\n",
            "encounter.v :: 3 :: experience\n",
            "encounter.v :: 4 :: cross\n",
            "encounter.v :: 5 :: intersect\n",
            "activate.v :: 1 :: initiate modify spark trigger change reactivate alter\n",
            "activate.v :: 2 :: aerate\n",
            "activate.v :: 3 :: pioneer\n",
            "activate.v :: 4 :: trip\n",
            "activate.v :: 5 :: actuate\n",
            "decide.v :: 1 :: adjudicate cause mold settle have get make will govern seal terminate stimulate adjust determine order orient try resolve regularize orientate take regulate mensurate purpose rule induce select choose shape measure decree judge\n",
            "decide.v :: 2 :: regularise\n",
            "decide.v :: 3 :: influence\n",
            "decide.v :: 4 :: end\n",
            "solid.a :: 1 :: hearty strong substantial satisfying upstanding unanimous\n",
            "solid.a :: 2 :: self-coloured\n",
            "solid.a :: 3 :: square\n",
            "solid.a :: 4 :: self-colored\n",
            "solid.a :: 5 :: firm\n",
            "solid.a :: 6 :: whole\n",
            "difficulty.n :: 1 :: troublesomeness rigourousness\n",
            "difficulty.n :: 2 :: pinch job formidability wrinkle grimness effortfulness condition tsuris hardness facer rigorousness strain hinderance hardship pisser situation toughness difficultness snorter bitch mire quandary status hindrance ruggedness burdensomeness muddle mess subtlety rigour severeness wall sweat severity asperity predicament killer impediment kink worriment deterrent travail pickle quality plight check rigor fix baulk heaviness oppressiveness inconvenience trouble stress problem rattrap pitfall balk jam exertion handicap niceness hole\n",
            "difficulty.n :: 3 :: effort\n",
            "difficulty.n :: 4 :: onerousness\n",
            "audience.n :: 1 :: gallery conference grandstand multitude gathering hearing masses people viewers readership mass house assemblage\n",
            "audience.n :: 2 :: chance opportunity\n",
            "audience.n :: 3 :: consultation\n",
            "audience.n :: 4 :: interview\n",
            "remain.v :: 1 :: persist stick linger continue bide keep stay rest\n",
            "remain.v :: 2 :: be\n",
            "remain.v :: 3 :: abide\n",
            "remain.v :: 4 :: stand\n",
            "disc.n :: 1 :: audio deadeye platter floppy dot diskette planchet 78 circle point plate puck disk L-P discus diaphragm saucer record Frisbee token\n",
            "disc.n :: 2 :: seventy-eight\n",
            "disc.n :: 3 :: LP\n",
            "disc.n :: 4 :: round\n",
            "lose.v :: 1 :: regress worsen retrograde recede retrogress suffer\n",
            "lose.v :: 2 :: decline drop\n",
            "lose.v :: 3 :: mislay put forget misplace overlook lay leave set miss\n",
            "lose.v :: 4 :: place\n",
            "lose.v :: 5 :: pose\n",
            "lose.v :: 6 :: whiteout\n",
            "lose.v :: 7 :: position\n",
            "lose.v :: 8 :: white-out\n",
            "add.v :: 1 :: numerate enumerate milk impart transfuse concatenate intercalate calculate increase adjoin button inject qualify alter instill summate compute append supply modify tally butylate form constitute cipher count enrich sum insert tell number change supplement factor make figure mix reckon cypher bring bestow string say contribute combine punctuate include state lend total fortify mark\n",
            "add.v :: 2 :: foot welt\n",
            "add.v :: 3 :: stud\n",
            "add.v :: 4 :: tot\n",
            "add.v :: 5 :: tinsel\n",
            "add.v :: 6 :: compound\n",
            "sort.n :: 1 :: make genus genre individual similarity stripe person flavor style antitype kind category like manner brand somebody someone model description form soul species flavour ilk color mortal type variety\n",
            "sort.n :: 2 :: sorting\n",
            "sort.n :: 3 :: operation\n",
            "sort.n :: 4 :: colour\n",
            "ask.v :: 1 :: articulate interrogate require involve demand postulate inquire query request address expect question need intercommunicate claim solicit govern cost consult formulate necessitate call take compel bespeak communicate\n",
            "ask.v :: 2 :: phrase word\n",
            "ask.v :: 3 :: pry\n",
            "ask.v :: 4 :: quest\n",
            "ask.v :: 5 :: enquire\n",
            "ask.v :: 6 :: draw\n",
            "ask.v :: 7 :: exact\n",
            "hot.a :: 1 :: live\n",
            "hot.a :: 2 :: red-hot\n",
            "hot.a :: 3 :: blistering\n",
            "hot.a :: 4 :: spicy\n",
            "hot.a :: 5 :: raging\n",
            "arm.n :: 1 :: branch division\n",
            "arm.n :: 2 :: weapon shirtsleeve flamethrower hatchet blade lance knuckles instrument tomahawk knucks sleeve bow gun missile slasher sling steel shaft spear limb armrest projectile WMD sword knife pike\n",
            "arm.n :: 3 :: projection\n",
            "arm.n :: 4 :: subdivision\n",
            "arm.n :: 5 :: brand\n",
            "arm.n :: 6 :: W.M.D.\n",
            "appear.v :: 1 :: pop flash glow glint show cut be materialize glisten loom reappear radiate peep fulminate rise happen gleam make manifest occur glitter lift jump feel erupt execute beam shine seem break perform sound emerge do look\n",
            "appear.v :: 2 :: basset\n",
            "appear.v :: 3 :: surface outcrop\n",
            "appear.v :: 4 :: materialise\n",
            "appear.v :: 5 :: rear\n",
            "appear.v :: 6 :: re-emerge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recitation\n",
        "To compare, make a dataframe\n",
        "**Target word | F score_ sparse | F score_dense | Diff sparse - dense | Diff dense - sparse**\n",
        "\n",
        "Now sort based on diff_sparse and diff_dense to identify words where one works better than other"
      ],
      "metadata": {
        "id": "jOjWndvWyQVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: [Error analysis] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "3cIodXdnB6P7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Cluster without K [31 points]\n",
        "\n",
        "So far we made the clustering problem deliberately easier by providing you with `k`, the number of clusters, as an input. But in most clustering situations the best `k` is not given. To take this assignment one step further, see if you can come up with a way to automatically choose `k`.\n",
        "\n",
        "Write a function `cluster_with_no_k(word_to_paraphrases_dict)` that accepts only the first dictionary as an input and produces clusterings for given target words.\n",
        "\n",
        "We have provided an additional test set `test_nok_input.txt`, where the `k` field has been zeroed out. See if you can come up with a method that clusters words by sense, and chooses the best `k` on its own. You can start by assigning `k=5` for all target words as a baseline model.\n",
        "\n",
        "You might want to try and use the development data to analyze how got is your model in determining `k`.\n",
        "\n",
        "One of the ways to approach this challenge is to try and select best `k` for a target word and a list of paraphrases is to use try out a range of `k`'s and judge the performance of the clusterings based on some metric, for instance a [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).\n",
        "\n",
        "Be sure to describe your method in the Report."
      ],
      "metadata": {
        "id": "T6ELKOARMnBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 3.4:** Implement `cluster_with_no_k` function [25 points]\n"
      ],
      "metadata": {
        "id": "qHwYHJlPMQJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recitation\n",
        "**Till now we had been giving K and you used that in K means to split your paraphrase list into list of lists (clusters). Now in this section we dont give you K. For this, Keep logic similar where you were preparing matrix of features. Next step : Try K means using diff number of clusters. Calculate silhouette score for each number and finally chose the optimal no of clusters for that target word**"
      ],
      "metadata": {
        "id": "-4G-4nBVyJdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score # Hint: this could be useful if you want to use silhouette_score as distance metric"
      ],
      "metadata": {
        "id": "naUpFNx49AN8"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fscore(cluster_func):\n",
        "    word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "    predicted_clusterings_random_dev = cluster_func(word_to_paraphrases_dict_dev, word_to_k_dict_dev)\n",
        "    gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "    f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "    return f_score"
      ],
      "metadata": {
        "id": "7kdYRXhZbxKt"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cluster_with_k(vectors, paraphrase_list, k):\n",
        "    cluster = [] # init an empty array for clustering\n",
        "    for i in range(k):\n",
        "        cluster.append([])\n",
        "\n",
        "    X = np.zeros((len(paraphrase_list), vectors.dim))\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    for i in range(len(paraphrase_list)):\n",
        "        paraphrase = paraphrase_list[i]\n",
        "        if paraphrase in vectors:\n",
        "            X[i] = vectors.query(paraphrase)\n",
        "        else:\n",
        "            # random_idx = random.randint(0, len(vectors_filter)-1)\n",
        "            random_idx = np.random.randint(0, len(vectors))\n",
        "            vectors.index(random_idx)[1]\n",
        "            X[i] = vectors.index(random_idx)[1].copy()\n",
        "    kmeans.fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    for i in range(len(paraphrase_list)):\n",
        "        cluster[labels[i]].append(paraphrase_list[i])\n",
        "    s_score = silhouette_score(X, labels)\n",
        "    return s_score, cluster\n"
      ],
      "metadata": {
        "id": "6zqp5nQwcZhO"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n",
        "w = \"different.a\"\n",
        "\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "p_list = word_to_paraphrases_dict_dev[w]\n",
        "print(p_list)\n",
        "\n",
        "clusters = [] # init an empty array for clustering\n",
        "s_scores = []\n",
        "potential_k = min(11, len(p_list))\n",
        "for i in range(2, potential_k):\n",
        "    s_score, cluster = get_cluster_with_k(v, p_list, i)\n",
        "    print(\"score is {}\".format(s_score))\n",
        "    print(f\"s_score: {s_score} cluster: {cluster}\")\n",
        "    clusters.append(cluster)\n",
        "    s_scores.append(s_score)\n",
        "s_score_array = np.asarray(s_scores)\n",
        "s_scores\n",
        "# best_k = np.argmax(s_score_array)\n",
        "\n",
        "# print(f\"Best k for {w}: {best_k} tested clustering {potential_k}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_KY9NC5fWb1",
        "outputId": "8922765c-c05d-4aef-b88b-6624e6316b05"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dissimilar', 'unlike']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_with_no_k(word_to_paraphrases_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using any vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        # print(f\"Processing {target_word}\")\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        # TODO: Implement\n",
        "        # hint: first find the best k value, you can define a seperate function, if needed.\n",
        "        # hint: then fit a KMeans model on the data with k clusters\n",
        "        clusters = [] # init an empty array for clustering\n",
        "        s_scores = []\n",
        "        potential_k = min(11, len(paraphrase_list))\n",
        "        if potential_k <= 2:\n",
        "            clusterings[target_word] = [paraphrase_list]\n",
        "            continue\n",
        "        for i in range(2, potential_k):\n",
        "            s_score, cluster = get_cluster_with_k(vectors, paraphrase_list, i)\n",
        "            clusters.append(cluster)\n",
        "            s_scores.append(s_score)\n",
        "        s_score_array = np.asarray(s_scores)\n",
        "        best_k = np.argmax(s_score_array)\n",
        "        # print(f\"Best k for {target_word}: {best_k} tested clustering {potential_k}\")\n",
        "        clusterings[target_word] = clusters[best_k]\n",
        "\n",
        "    return clusterings"
      ],
      "metadata": {
        "id": "BWeQwrJmakL2"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n",
        "w = \"different.a\"\n",
        "\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "p_list = word_to_paraphrases_dict_dev[w]\n",
        "print(p_list)\n",
        "\n",
        "clusters = [] # init an empty array for clustering\n",
        "s_scores = []\n",
        "potential_k = min(11, len(p_list))\n",
        "for i in range(2, potential_k):\n",
        "    s_score, cluster = get_cluster_with_k(p_list, i)\n",
        "    print(\"score is {}\".format(s_score))\n",
        "    print(f\"s_score: {s_score} cluster: {cluster}\")\n",
        "    clusters.append(cluster)\n",
        "    s_scores.append(s_score)\n",
        "s_score_array = np.asarray(s_scores)\n",
        "s_scores"
      ],
      "metadata": {
        "id": "PvKV6iVLjXDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce482097-cf64-4c29-86dd-98c7e9ae7a5c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dissimilar', 'unlike']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 3.4.1:** Run clustering on `dev` data, report the `f_scores` from the `dev` data [1 point]"
      ],
      "metadata": {
        "id": "SSquG6HBCNXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** [Report f_score on dev data] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "ilQ8iSrECYZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE (you can re-use reference code from 3.1)\n",
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_no_k(word_to_paraphrases_dict_dev)\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "id": "vlLfP-jQCZMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005ef155-6e6f-4838-d8a6-b85b47402ea2"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|     eat.v      | 6  |     0.2213     |\n",
            "|    climb.v     | 6  |     0.2400     |\n",
            "|    expect.v    | 6  |     0.4405     |\n",
            "|     note.v     | 3  |     0.1875     |\n",
            "|    simple.a    | 5  |     0.2857     |\n",
            "|   produce.v    | 7  |     0.3875     |\n",
            "|   shelter.n    | 5  |     0.3114     |\n",
            "|     wash.v     | 13 |     0.3333     |\n",
            "|    write.v     | 9  |     0.2215     |\n",
            "|     rule.v     | 7  |     0.2958     |\n",
            "|     miss.v     | 8  |     0.2609     |\n",
            "|     mean.v     | 6  |     0.3274     |\n",
            "|     win.v      | 4  |     0.4398     |\n",
            "|    treat.v     | 8  |     0.3955     |\n",
            "|     bank.n     | 9  |     0.4231     |\n",
            "|   judgment.n   | 7  |     0.3834     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|    begin.v     | 8  |     0.3377     |\n",
            "|     talk.v     | 6  |     0.5370     |\n",
            "| performance.n  | 5  |     0.2568     |\n",
            "|   provide.v    | 7  |     0.4630     |\n",
            "|    image.n     | 9  |     0.2868     |\n",
            "|    source.n    | 9  |     0.2887     |\n",
            "|    degree.n    | 7  |     0.2685     |\n",
            "|     play.v     | 34 |     0.1928     |\n",
            "|    paper.n     | 7  |     0.5413     |\n",
            "|   interest.n   | 5  |     0.2775     |\n",
            "|  difference.n  | 5  |     0.5372     |\n",
            "|    watch.v     | 5  |     0.3077     |\n",
            "| organization.n | 7  |     0.3357     |\n",
            "|   suspend.v    | 6  |     0.4878     |\n",
            "|  atmosphere.n  | 6  |     0.2526     |\n",
            "|   operate.v    | 7  |     0.3189     |\n",
            "|    smell.v     | 4  |     0.4340     |\n",
            "|   express.v    | 7  |     0.4380     |\n",
            "|     hear.v     | 5  |     0.2697     |\n",
            "|     plan.n     | 3  |     0.1562     |\n",
            "|     use.v      | 6  |     0.6495     |\n",
            "|   receive.v    | 13 |     0.1765     |\n",
            "|    party.n     | 5  |     0.4735     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, run the following command to generate the output file for the predicted clusterings for the test dataset."
      ],
      "metadata": {
        "id": "bJq0-6pnakUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict, _ = load_input_file('/content/test_nok_input.txt')\n",
        "predicted_clusterings_nok = cluster_with_no_k(word_to_paraphrases_dict)\n",
        "# write_to_output_file('test_output_nok.txt', predicted_clusterings_nok)"
      ],
      "metadata": {
        "id": "riIjx9FaNEwy"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "# reload_grader()\n",
        "grader.grade(test_case_id = 'test_q3_clusters_no_k', answer = (predicted_clusterings_nok, 'no k'))"
      ],
      "metadata": {
        "id": "7eugg3gH842c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b257ba69-5788-403a-d771-f443fa340895"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 25/25 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Agglomerative cluster\n",
        "def cluster_with_no_k_agglo(word_to_paraphrases_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using any vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        # print(f\"Processing {target_word}\")\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        # TODO: Implement\n",
        "        # hint: first find the best k value, you can define a seperate function, if needed.\n",
        "        # hint: then fit a KMeans model on the data with k clusters\n",
        "        clusters = [] # init an empty array for clustering\n",
        "        s_scores = []\n",
        "        potential_k = min(11, len(paraphrase_list))\n",
        "        if potential_k <= 2:\n",
        "            clusterings[target_word] = [paraphrase_list]\n",
        "            continue\n",
        "        for i in range(2, potential_k):\n",
        "            s_score, cluster = get_cluster_with_k(paraphrase_list, i)\n",
        "            clusters.append(cluster)\n",
        "            s_scores.append(s_score)\n",
        "        s_score_array = np.asarray(s_scores)\n",
        "        best_k = np.argmax(s_score_array)\n",
        "        # print(f\"Best k for {target_word}: {best_k} tested clustering {potential_k}\")\n",
        "        clusterings[target_word] = clusters[best_k]\n",
        "\n",
        "    return clusterings"
      ],
      "metadata": {
        "id": "QIODabEujdBx"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 3.4.2:** Provide a brief description of your method in the report that includes the vectors you used, and any experimental results you have from running your model on the dev set.  [5 points]"
      ],
      "metadata": {
        "id": "LYj7zEKwDiF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: [Describe your method] **[writeup.pdf]**"
      ],
      "metadata": {
        "id": "HUkMZBtZDnyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agglomerative clustering\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_cluster_with_k_agglomerative (paraphrase_list, k, linkage_opt=\"single\"):\n",
        "    cluster = [] # init an empty array for clustering\n",
        "    for i in range(k):\n",
        "        cluster.append([])\n",
        "\n",
        "    X = np.zeros((len(paraphrase_list), vectors.dim))\n",
        "    agglo_model = AgglomerativeClustering(n_clusters=k, linkage=linkage_opt)\n",
        "    for i in range(len(paraphrase_list)):\n",
        "        paraphrase = paraphrase_list[i]\n",
        "        if paraphrase in vectors:\n",
        "            X[i] = vectors.query(paraphrase)\n",
        "        else:\n",
        "            # random_idx = random.randint(0, len(vectors_filter)-1)\n",
        "            random_idx = np.random.randint(0, len(vectors))\n",
        "            vectors.index(random_idx)[1]\n",
        "            X[i] = vectors.index(random_idx)[1].copy()\n",
        "    agglo_model.fit(X)\n",
        "    labels = agglo_model.labels_\n",
        "    for i in range(len(paraphrase_list)):\n",
        "        cluster[labels[i]].append(paraphrase_list[i])\n",
        "    s_score = silhouette_score(X, labels)\n",
        "    return s_score, cluster"
      ],
      "metadata": {
        "id": "qSWDMGwylVs2"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_with_no_k_agglo(word_to_paraphrases_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using any vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        # print(f\"Processing {target_word}\")\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        # TODO: Implement\n",
        "        # hint: first find the best k value, you can define a seperate function, if needed.\n",
        "        # hint: then fit a KMeans model on the data with k clusters\n",
        "        clusters = [] # init an empty array for clustering\n",
        "        s_scores = []\n",
        "        potential_k = min(11, len(paraphrase_list))\n",
        "        if potential_k <= 2:\n",
        "            clusterings[target_word] = [paraphrase_list]\n",
        "            continue\n",
        "        for i in range(2, potential_k):\n",
        "            s_score, cluster = get_cluster_with_k_agglomerative(paraphrase_list, i)\n",
        "            clusters.append(cluster)\n",
        "            s_scores.append(s_score)\n",
        "        s_score_array = np.asarray(s_scores)\n",
        "        best_k = np.argmax(s_score_array)\n",
        "        # print(f\"Best k for {target_word}: {best_k} tested clustering {potential_k}\")\n",
        "        clusterings[target_word] = clusters[best_k]\n",
        "\n",
        "    return clusterings"
      ],
      "metadata": {
        "id": "jyKhZ0w7lW1C"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('dev_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_no_k_agglo(word_to_paraphrases_dict_dev)\n",
        "gold_clusterings_dev = load_output_file('dev_output.txt')\n",
        "f_score = evaluate_clusterings(gold_clusterings_dev, predicted_clusterings_random_dev)\n",
        "f_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zRevIv1lsUM",
        "outputId": "defb3358-9921-4438-cabf-0a56a3066c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    degree.n    | 7  |     0.4460     |\n",
            "|  atmosphere.n  | 6  |     0.3825     |\n",
            "|     play.v     | 34 |     0.1753     |\n",
            "|    smell.v     | 4  |     0.5033     |\n",
            "|     talk.v     | 6  |     0.6414     |\n",
            "|     plan.n     | 3  |     0.6745     |\n",
            "|    party.n     | 5  |     0.3739     |\n",
            "|   express.v    | 7  |     0.4256     |\n",
            "|     win.v      | 4  |     0.5445     |\n",
            "|    paper.n     | 7  |     0.6022     |\n",
            "|     rule.v     | 7  |     0.3317     |\n",
            "|   produce.v    | 7  |     0.4968     |\n",
            "|     note.v     | 3  |     0.7719     |\n",
            "| performance.n  | 5  |     0.4761     |\n",
            "|     bank.n     | 9  |     0.5688     |\n",
            "|     miss.v     | 8  |     0.3058     |\n",
            "|   operate.v    | 7  |     0.3060     |\n",
            "|    write.v     | 9  |     0.3567     |\n",
            "|    source.n    | 9  |     0.3109     |\n",
            "|     mean.v     | 6  |     0.4011     |\n",
            "|    expect.v    | 6  |     0.4642     |\n",
            "|  difference.n  | 5  |     0.5078     |\n",
            "|    image.n     | 9  |     0.3086     |\n",
            "|   shelter.n    | 5  |     0.5194     |\n",
            "|     use.v      | 6  |     0.6658     |\n",
            "|     wash.v     | 13 |     0.2579     |\n",
            "|   interest.n   | 5  |     0.3383     |\n",
            "| organization.n | 7  |     0.3955     |\n",
            "|    treat.v     | 8  |     0.4046     |\n",
            "|    begin.v     | 8  |     0.3389     |\n",
            "|    climb.v     | 6  |     0.3646     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   provide.v    | 7  |     0.7470     |\n",
            "|   suspend.v    | 6  |     0.4878     |\n",
            "|    simple.a    | 5  |     0.3889     |\n",
            "|     hear.v     | 5  |     0.3351     |\n",
            "|     eat.v      | 6  |     0.4729     |\n",
            "|   judgment.n   | 7  |     0.3298     |\n",
            "|   receive.v    | 13 |     0.2256     |\n",
            "|    watch.v     | 5  |     0.4646     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_paraphrases_dict_dev, word_to_k_dict_dev = load_input_file('test_nok_input.txt')\n",
        "predicted_clusterings_random_dev = cluster_with_no_k_agglo(word_to_paraphrases_dict_dev)\n",
        "for key in predicted_clusterings_random_dev.keys():\n",
        "    clusters = predicted_clusterings_random_dev[key]\n",
        "    for i in range(len(clusters)):\n",
        "        cluster = clusters[i]\n",
        "        s = \" \".join(cluster)\n",
        "        s.strip()\n",
        "        print(f\"{key} :: {i+1} :: {s}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3ZCwIikmEQl",
        "outputId": "6a29ffc2-0ae0-42fb-893c-5f1666ecf03c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "terrible.a :: 1 :: horrendous painful dreaded fearful fearsome awful atrocious dire frightful tremendous severe frightening abominable unspeakable dread wicked dreadful horrific\n",
            "terrible.a :: 2 :: direful\n",
            "closely.r :: 1 :: tight close intimately\n",
            "closely.r :: 2 :: nearly\n",
            "around.r :: 1 :: roughly approximately\n",
            "around.r :: 2 :: round\n",
            "around.r :: 3 :: some\n",
            "around.r :: 4 :: about\n",
            "range.n :: 1 :: spectrum Primus hearing compass image eyeshot motley earshot reach ambit assortment grasp smorgasbord palette formation band contrast view internationalism potpourri chain installation tract internationality miscellanea horizon ken potbelly facility ballpark set capability capableness latitude purview stove extent confines expanse sweep limit salmagundi cookstove scope sight parcel gamut orbit mixture pallet potentiality miscellany variety\n",
            "range.n :: 2 :: earreach\n",
            "stand.v :: 1 :: remain support countenance evaluate_clusterings brook queue place ramp lay continue bear judge endure digest position let set accept withstand put pose resist stay suffer permit swallow abide allow serve pay wash tolerate align measure fend stomach fight rest service be oppose defend\n",
            "stand.v :: 2 :: array\n",
            "strain.n :: 1 :: substance fanfare variety pedigree var. nervousness cradlesong nisus effort meaning stress drift sweat variant breed tenseness injury harm signature theme voice overstrain roulade carol music striving jehad nerves purport tune travail attempt part leitmotiv tension try idea leitmotif tenor endeavor flourish exertion glissando difficulty air straining line song jihad endeavour stock taxon pains form deformation hurt lullaby bloodstock melody trauma\n",
            "strain.n :: 2 :: tucket\n",
            "worthy.a :: 1 :: suitable desirable\n",
            "full.a :: 1 :: entire broad total good wide\n",
            "full.a :: 2 :: wide-cut\n",
            "full.a :: 3 :: replete\n",
            "heavy.a :: 1 :: wakeless sonorous overweight enceinte operose expectant large backbreaking gruelling clayey hard sound ponderous impenetrable leaden gravid lumbering intemperate lowering toilsome sullen punishing great arduous grievous fleshy grave big threatening labored laboured dense profound weighty grueling laborious\n",
            "heavy.a :: 2 :: cloggy\n",
            "soil.n :: 1 :: gumbo tillage mud filth topsoil undersoil subsoil sand ploughland earth permafrost uncleanness podzol bottom grime sward dirtiness mould podsol farmland silt bole tilth overburden dirt badlands wetland loess caliche humus polder territory grunge object plowland bottomland greensward till laterite marl scablands mold loam regosol turf ground grease clay hardpan wiesenboden coastland rangeland land stain sod\n",
            "soil.n :: 2 :: clunch\n",
            "blow.n :: 1 :: pound knockdown occurrent whip whiplash whiff reversal reverse hammering hammer KO C jolt sandblast insufflation cocaine backhander smacking bang snow happening boot blip wind belt coke kick smash thrust bash jounce thwack whack poke cocain swat kayo rap shock kicking bluster slug setback exhalation clout puff expiration occurrence bump sideswipe smack impact stinger lash swing smacker uppercut box clip wallop surprise biff stroke slap blast knockout strike pounding gust counterblow lick tap jar shot punch concussion buffeting knock stab whammy thump\n",
            "blow.n :: 2 :: whang\n",
            "well.r :: 1 :: easily comfortably advantageously intimately\n",
            "well.r :: 2 :: good\n",
            "well.r :: 3 :: considerably substantially\n",
            "way.n :: 1 :: category stairway artefact escape pick selection clearance course headroom journey position property lane salvation tool idiom condition hadith mode lifestyle houseroom heading distance status journeying effectuation signature aim seats touch life-style watercourse voice seating room qibla expedient road direction drape staircase part parking means access tooth wings dint fashion waterway choice Sunnah ambages style Sunna path tendency wise trend route percentage itinerary artifact portion instrument setup approach bearing passage share headway response lebensraum warpath form manner implementation fit\n",
            "way.n :: 2 :: agency\n",
            "true.a :: 1 :: truthful honest\n",
            "true.a :: 2 :: genuine\n",
            "true.a :: 3 :: dependable reliable\n",
            "true.a :: 4 :: on-key\n",
            "true.a :: 5 :: lawful\n",
            "true.a :: 6 :: straight\n",
            "true.a :: 7 :: unfeigned\n",
            "true.a :: 8 :: rightful\n",
            "acute.a :: 1 :: discriminating piercing sharp needlelike keen knifelike penetrating penetrative intense incisive\n",
            "acute.a :: 2 :: acuate\n",
            "right.r :: 1 :: decent decently\n",
            "right.r :: 2 :: correctly properly\n",
            "right.r :: 3 :: mightily\n",
            "right.r :: 4 :: powerful\n",
            "right.r :: 5 :: justly\n",
            "right.r :: 6 :: aright\n",
            "right.r :: 7 :: mighty\n",
            "right.r :: 8 :: flop\n",
            "cross.a :: 1 :: ill-tempered thwartwise\n",
            "cross.a :: 2 :: transverse transversal\n",
            "cross.a :: 3 :: grumpy grouchy crabby fussy crabbed\n",
            "cross.a :: 4 :: bad-tempered\n",
            "clear.a :: 1 :: clear-cut unmortgaged\n",
            "clear.a :: 2 :: unclouded decipherable readable percipient\n",
            "clear.a :: 3 :: exonerated exculpated vindicated cleared clean absolved\n",
            "clear.a :: 4 :: clean-cut\n",
            "clear.a :: 5 :: open\n",
            "clear.a :: 6 :: well-defined\n",
            "clear.a :: 7 :: light\n",
            "new.a :: 1 :: New Modern\n",
            "new.a :: 2 :: fresh raw\n",
            "new.a :: 3 :: young\n",
            "new.a :: 4 :: novel\n",
            "new.a :: 5 :: unexampled\n",
            "new.a :: 6 :: newfangled\n",
            "saint.n :: 1 :: fakeer nonpareil angel humdinger fakir nonsuch deity model faquir god Buddha ideal nonesuch immortal divinity jimdandy paragon crackerjack apotheosis faqir\n",
            "saint.n :: 2 :: jimhickey\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submissions"
      ],
      "metadata": {
        "id": "2hoP1RcwMqNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Leaderboards [2 points + 3 bonus]\n",
        "In order to stir up some friendly competition, we would also like you to submit the clustering from your best model to a leaderboard. There will be 2 leaderboards to submit to:\n",
        "- **Clusters with no K**: Copy the output file from your best model **(has to come from `3.4`)** to a file called `test_nok_output_leaderboard.txt` and include it with your submission in `HW5: Leaderboard Without K` following the format of the clusters file. [1 point]\n",
        "\n",
        "- **Clusters with K**: Copy the output file from your best model **(has to come from `3.2 or 3.3`)** to a file called `test_output_leaderboard.txt` and include it with your submission in `HW5: Leaderboard With K` following the format of the clusters file. [1 point]\n",
        "\n",
        "The first 10 places in either of the two leaderboards get 3 extra points."
      ],
      "metadata": {
        "id": "fu2saUjcHWPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Free-response Checklist (check if you missed anything!)\n",
        "We will look for the following free-responses in this notebook:\n",
        "- Section 2: Question responses and analysis of correlations\n",
        "- Section 3: For each clustering algorithm, you would need to report the `f_score` from the `dev` set and description of your methods."
      ],
      "metadata": {
        "id": "-DfjnoJXNfCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GradeScope File Submission\n",
        "Here are the deliverables you need to submit to GradeScope:\n",
        "- Write-up:\n",
        "    - Part 2: answers to questions\n",
        "    - Part 3: F-scores for clustering algorithms & discussions about your models\n",
        "- Code:\n",
        "    - This notebook and py file: rename to `homework5.ipynb` and `homework5.py`. You can download the notebook and py file by going to the top-left corner of this webpage, `File -> Download -> Download .ipynb/.py`\n",
        "- Leaderboard Results:\n",
        "  - Leaderboard Without K: `test_nok_output_leaderboard.txt` (Task 3.4 output file)\n",
        "  - Leaderboard With K: `test_output_leaderboard.txt` (Task 3.2 or 3.3 output file)"
      ],
      "metadata": {
        "id": "qx1oLs9-OFjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"./\")"
      ],
      "metadata": {
        "id": "ZncpABPjA29w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fe7e56-2e40-4a70-b60f-9d4308413854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'glove.840B.300d.magnitude',\n",
              " 'wiki-news-300d-1M.vec',\n",
              " 'notebook-config.yaml',\n",
              " 'glove.6B.300d.magnitude',\n",
              " 'test_nok_input.txt',\n",
              " 'glove.6B.200d.magnitude',\n",
              " 'wiki-news-300d-1M.vec.zip',\n",
              " 'dev_output.txt',\n",
              " 'glove.6B.100d.magnitude',\n",
              " 'test_input.txt',\n",
              " 'dev_input.txt',\n",
              " 'glove.6B.50d.magnitude',\n",
              " 'SimLex-999.txt',\n",
              " 'GoogleNews-vectors-negative300.filter.magnitude',\n",
              " 'GoogleNews-vectors-negative300.magnitude',\n",
              " 'coocvec-500mostfreq-window-3.filter.magnitude',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u0_BxouJoDYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}